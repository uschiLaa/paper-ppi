---
title: Using tours to visually investigate properties of new projection pursuit indexes with application to problems in physics

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Ursula Laa
  thanks: The authors gratefully acknowledge the support of the Australian Research Council
  affiliation: Department of Physics, Monash University
  email: \email{ursula.laa@monash.edu}
  
- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
- name: Heike Hofmann
  affiliation: Department of Statistics, Iowa State University
  
- name: Hadley Wickham
  affiliation: RStudio
  
- name: Antony Unwin
  affiliation: Department of Mathematics, Augsburg University
  
- name: Katrin Grimm
  affiliation: ???

keywords:
- scagnostics
- statistical graphics
- data visualisation
- exploratory data analysis
- data science 
- guided tour

abstract: |
  Projection pursuit describes a procedure for searching high-dimensional data for "interesting" low-dimensional projections via the optimization of a criterion function called the projection pursuit index. Most indexes developed focus on finding projections with cluster structure, outliers, or separations between known groups. Here, we are interested in finding projections revealing potentially complex relations between combinations of parameters, by combining the notion of projection pursuit with index functions developed for the detection of interesting bivariate views in the data, including scagnostics and maximum information coefficient.

bibliography: bibliography.bib
output: rticles::asa_article
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{amssymb}
  - \usepackage{xcolor}
---

```{r initial, echo = FALSE, include = FALSE}
library(knitr)
opts_chunk$set(
  warning = FALSE, message = FALSE, echo = FALSE, 
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.show = 'hold', cache = FALSE, external = TRUE, dev = "pdf",
  fig.height = 5, fig.width = 8, out.width = "\\textwidth"
)
opts_knit$set(eval.after = "fig.cap") #so I can use chunck output in the caption
```

# Introduction

<!-- 
Main point of paper:

- Some metrics for exploring high-dimensional data by finding interesting pairs of variables, can be used to find interesting projections, but some have problems. 
- What are the ways to evaluate.
- What adjustments can be done.
- How it can be applied

Organisation of this section:

- Motivation first?
- PP
- Scagnostics
- section needs shortening
-->

The term "projection pursuit" was coined by @FT74 to describe a procedure for searching high (say $p-$)dimensional data for "interesting" low-dimensional projections ($d=1$ or $2$ usually). The procedure, originally suggested by @kr69, involves defining a criterion function, or index, that measures the "interestingness" of each $d$-dimensional projection of $p$-dimensional data. This criterion function is optimized over the space of all $d$-dimensional projections of $p$-space, searching for both global and local maxima. It is hoped that the resulting solutions reveal low-dimensional structure in the data not found by methods such as principal component analysis.

A large number of projection pursuit indices have been developed, to detect departure from multivariate normal, which includes clusters or outliers or separations between known groups (e.g. @f87, @CBC92, @lckl2005, @AHC02, @CEM:CEM2568, @JS87, @5508437).  Less work has been done on indexes to find nonlinear dependence between variables. 

XXX Need to add something here about the guided tour

<!-- check Perisic and Posse (2012) and recent paper by Austrian guys -->


A related topic is variable selection. With high-dimensional data, even plotting all pairs can be daunting, so "scagnostics" [@scag] [@WW08] have been developed to find the most interesting pairs of variables. There are 8 scagnostics, vividly named: "Outlying", "Skewed", "Sparse", "Clumpy", "Striated", "Convex", "Skinny" and "Stringy". Our question is whether these can be extended into projection pursuit indexes that can be used to find two-dimensional projections of high-dimensional data with unusual features. 

Our motivation is based on applications in physics, to aid the interpretation of model fits on experimental results. Consider a physical model with a set of $p$ free parameters, that cannot be measured directly and are determined by fitting a set of $n$ experimental observations, for which predictions can be calculated when fixing all $p$ parameters. Note here, that while we often have analytic expressions for the predictions, this is not always the case and we may have to rely on numerical computation. Moreover, a single prediction can be a complicated function of all free parameters. In addition, we often deal with large number of observations, $n \sim 100-1000$ as well as sizable parameter spaces $p \sim 10$. The results of a fit are generally interpreted using combinations of variables selected by intuition and prior knowledge. But this begs the question, whether important associations are missed because tools to search are not available.

<!--
Suggestion German:
We are motivated by physics applications, specifically to assist the interpretation of models fit to experimental results. In these problems one has a physical model with a set of m free parameters which cannot be measured directly, but that are determined instead by fitting a set of p experimental observations. The model predicts all the observations in terms of the m parameters, often analytically, but occasionally only numerically. Moreover, a single prediction can be a complicated function of all the free parameters requiring intensive numerical resources. 

These problems often deal with a large number of observations and parameters, with p in the hundreds or even thousands and m of order 10. 
The results of a fit have been generally interpreted in terms of combinations of parameters selected by intuition or prior knowledge. Recently, these selections have been also assisted by machine learning techniques including BDT and neural networks.

Our visualisation tools complement these procedures providing a high dimensional picture which assists with the interpretation of the results,  serves as a check for other methods and possibly revealing new associations.
-->

[@Grimm2016] explored the behaviour of scagnostics for selecting variables, and proposed two more, based on smoothing splines and distance correlation, that have some nicer properties. In addition the availability of another two indices based on information criteria, maximal and total information coefficient (MIC and TIC) [@Reshef1518] round out our collection of metrics. \footnote{It is interesting to note that entropy, closely related to the notion of mutual information, has previously been considered as a projection pursuit index when searching for interesting one-dimensional projections, see discussion in \cite{huber85,jones87}.}
These indices are all available in R packages [@rref]: scagnostics [@HWscagR], mbgraphic [@mbgraphic] and minerva [@minerva]. The projection pursuit guided tour is available in the R package, tourr [@tourr].

The paper investigates the behaviour of these newly-defined indexes. Section \ref{sec:construct} discusses index construction, and how they fit into the guided tour. Section \ref{sec:investigate} investigates the behaviour of the indexes, particularly in relation to optimisation. The new guided tour methods are applied to an example where the posterior distributions from physics models (Section \ref{sec:phys}) are explored.

# Constructing a projection pursuit index
\label{sec:construct}

<!--
Explain here how to construct the pp index

- Definition of pp index
- Optimisation
- Illustration of what is already available in tourr
- New index explanations
-->
A projection pursuit index (PPI) is a scalar function $f$ defined on an $d$-dimensional data distribution. Typically the definition is such that larger values of $f$ indicate more interesting distribution, and therefore maximising $f$ over all possible $d$ dimensional projections of a data set with $p>d$ parameters will find the most interesting projections. As most indices characterise deviations from a normal distribution, one would first map the empiric data distribution onto a density for which such deviations can then be defined in different ways, see e.g. @CBC93. When departing from the idea of characterising differences to the normal distribution it may however be preferred to work directly with the empirical data distribution and instead characterise it by measures based on e.g. the minimal spanning tree or the mutual information.

## Standardization
Lower dimensional projections are highly sensitive to standardization performed on the original distribution. We want to avoid e.g.  highlighting directions based only on different scales, or artifically extending the parameter range by including outliers, which may result in seemingly linear behaviour for most points. Methods to consider include rescaling of individual directions to a common interval (e.g. to fall between $[0,1]$), sphering, outlier removal or transformation to a logarithmic scale. The method(s) of choice should be informed by the distributions found in the data set as well as aims of the analysis. \textcolor{red}{particular challenges: noise only directions, identification of outliers?}
\textcolor{red}{for holes/cmass index they are only well defined when data is standardised, how about the other index functions?}
\textcolor{red}{When keeping in mind the concrete example of posterior samples outliers should not be an issue, if extreme outliers are found one should probably investigate origin. Also noise only directions, e.g. nuissance parameters, are also not an issue as they contain no structure, see BBH example.}

\textcolor{red}{\cite{jones87} discusses centering and sphering of the data points. Centering is needed if index is not translational invariant (should not be the case for any index considered here). Arguments for sphering are that scale effects should be extracted, we don't want to reproduce information already obtained by PCA. To my mind rescaling to an overall intervall is more appropriate here. Rotations between parameters should not affect the outcome, but will make interpretation more difficult. Standardising the variance will remove information about the posterior variance. Overall we are interested in strong association only, if these structures were affected by the standardisation procedure, it would most likely be unwelcome effects?}


## Optimization
Given a PPI we are confronted with the task of finding the maximum over all possible $d$ dimensional projections. One challenge is to avoid getting trapped in local maxima that are only a result of sampling fluctuatins or a consequence of noisy index functions, and @f87 suggested a two-step procedure: the first step is using a large step size to find the approximate global maximum while stepping over pseudomaxima. A second step is then starting from the projection corresponding to the approximate maximum and employing a gradient directed optimisation for the identification of the maximum. On the other hand the global maximum may not be the only projection of interest, and one may want to observe the selected views in the context of the full distribution rather than a static view. These issues are addressed when combining projection pursuit with the grand tour [@CBCH94]. In this case the properties of a suitable optimization algorithm include monotonicity of the index value, a variable step-size to avoid overshoothing and to increase the chance of reaching the nearest maximum, and a stopping criterion allowing to move out of a local maximum and into a new search region [@tourr]. A possible alogrithm is inspired by simulated annealing and has been described in @lckl2005, this has been implemented in the \texttt{search\_better} and \texttt{search\_better\_random} search functions in the tourr package. In addition the tourr package provides the \texttt{search\_geodesic} search function, which first selects a promising direction by comparing index values between a selected number of small random steps, and then optimises the function over the line along the geodesic in that direction considering projections up to $\pi/4$ away from the current view.


\textcolor{red}{\cite{posse95b} discusses the optimisation, in particular that for most index functions and optimisers results are too local, largely dependent on starting point. Fiedmans suggestion of hybrid steppind search + steepest-ascent still remains fairly local. \cite{posse90} suggested random search instead, which seems to perform better, but did not find pdf of that article, so not so sure what he actually does.}


## New index functions
\label{sec:indexDef}
Focusing now on $d=2$ dimensional projections, we aim to identify projections that reveal interesting structures or correlations between combinations of variables. We therefore draw on index functions that have been developped for the selection of variable pairs that show interesting features in a scatter plot, but we generalise the notion such that instead of variable pairs we consider any $2$ dimensional projection of the multivariate data distribution. It means that we consider the projection axes as input to the index functions, which will thus score the interestingness of the given projection. What we are optimising over are then the entries of the $p\times2$ dimensional projection matrix, considering that they obey orthonormality conditions.
Concretely here we consider the following types of index functions:\footnote{We note that all descriptions are idealized, while in practice approximations and binning are used to obtain reasonable computing time, we refer to original references and package documentation for details.}

* From the scagnostics family select those best suited to detect shapes, with definitions given below. They are defined to take values between [0,1]. \textcolor{red}{The definition of these should be rotation invariant, however, some small dependence is introduced via the hex binning that is done before calculating the scagnostics indexes.}
    + Convex: The convex measure is defined as $c_{convex}= \frac{area(A)}{area(H)}$ where $A$ is the alpha hull and $H$ the convex hull. This is the only measure where interesting projections will take low values and one may either minimize the index function or maximize $1-c_{convex}$.
    + Skinny: The skinnyness of a polygon can be measured as the ratio of the perimeter to the area. We define $c_{skinny} = 1 -  \frac{\sqrt{4\pi area(A)}}{perimeter(A)}$, where the normalisation is chosen such that $c_{skinny} = 0$ for a full circle, and values close to one or skinny polygons.
    + Stringy: The stringy index is a measure of the branching structure of the minimal spanning tree (MST), $c_{stringy} = \frac{diameter(MST)}{length(MST)}$ where the diameter is the longest connected path, and the length is the total length (sum of all edges), i.e. $c_{stringy}=1$ if the MST contains no branches.

* New index functions available in @mbgraphic, defined to fall between [0,1] \textcolor{red}{and not rotation invariant, as I think Var(X)*Var(Y) introduces rotation invariance even for distance correlation?}:
    + Distance correlation defined in @szekely2007: The definition is based on empiric measures of distance covariance and variance defined on the dataset 
    \begin{equation}
(X,Y) = \{(x_1,y_1), ..., (x_n, y_n) \in \mathbb{R}^p \times \mathbb{R}^q\}
\end{equation}
 as
 \begin{equation}
 Cov^2(X,Y) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}B_{kl},\\
 Var^2(X) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}^2
 \end{equation}
 where $A_{kl}=a_{kl}-\bar{a}_{k.}-\bar{a}_{l.}+\bar{a}_{..}$ with
 \begin{equation}
 a_{kl} = | x_k - x_l|_p,\\
 \bar{a}_{k.} = \frac{1}{n} \sum_{l=1}^n a_{kl},\\
  \bar{a}_{.l} = \frac{1}{n} \sum_{k=1}^n a_{kl},\\
 \bar{a}_{..} = \frac{1}{n^2}\sum_{k,l=1}^n a_{kl}
 \end{equation}
 and reads
 \begin{equation}
 dCor(X,Y) = \begin{cases}
 \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} & Var(X)Var(Y) > 0\\
 0 & Var(X)Var(Y) = 0
 \end{cases}
 \end{equation}
    + Spline based measure: Given a spline model of the data distribution that is obtained via the R implentation in the \texttt{gam} function in the mgcv R package [@w16], we define the index based on the variance of the residuals:
    \begin{equation}
    splines_{2d} = max(1- \frac{Var(res_{X\sim Y})}{Var(X)}, 1-\frac{Var(res_{Y\sim X})}{Var(Y)}),
    \end{equation}
    which takes large values if the distribution is well described by the spline model, indicating functional dependence.
    
* Information based index functions from @Reshef1518 are based on the mutual information $I$ when considering the distribution induced by the data points $D \subset \mathbb{R}^2$ on an $x-$by$y$ grid $G$. With $I^{\ast} (D, x, y) = \max I (D|_G)$ the characteristic matrix $M$ is defined as
\begin{equation}
M(D)_{x,y} = \frac{I^{\ast} (D, x, y)}{\log \min{x,y}},
\end{equation}
where the denominator is chosen such that each entry of $M$ will take values between [0,1].
We can now define the following metrics:
    + Maximum Information Coefficient (MIC):
    \begin{equation}
    MIC(D) = \max_{xy<B(n)}\{M(D)_{x,y}\},
    \label{eq:MIC}
    \end{equation}
    where by default $B(n) = n^{0.6}$
    + Total Information Coefficient (TIC), see @JMLRv1715308:
    \begin{equation}
    TIC(D) = \sum_{xy<B(n)} M(D)_{x,y}
    \end{equation}
  While $MIC$ is normalised to fall between $[0,1]$, the upper bound of $TIC$ depends on the sample size via the bounding function $B(n)$. $TIC$ was designed to be more stable test against null hypothesis of independence, using the full information of the characteristic matrix rather than just the maximum, and as such is expected to be better suited as a ppi.
  
\textcolor{red}{What is upper bound of TIC as function of sample size? It seems from checking by putting in identical parameters as x, y that max(TIC) = 16 for 100 datapoints and max(TIC) = 148 for 1000 datapoints. Don't know how to derive this though, should depend also on c parameter and how the algorithm is approximating the true matrix?.}




```{r load}
library(tourr)
library(tidyverse)
library(reshape2)
library(scagnostics)
library(gridExtra)
library(tictoc) #timer
library(mbgraphic) #Katrins package
library(GGally)
library(geozoo)
library(minerva) #MINE indices
library(kableExtra)
library(forcats)
```


# Investigation of indices
\label{sec:investigate}

A useful projection pursuit index needs to have several properties. This has been discussed in several seminal papers (\cite{diaconis84}, \cite{huber85}, \cite{jones87}, \cite{posse95b}, \cite{hall89}). The PPI should be  minimized by the normal distribution, because this is not interesting from a data exploration perspective. If all projections are normally distributed, good modeling tools already exist. A PPI should be approximately affine invariant, regardless how the projection is rotated the index value should be the same, and the scale of each variable shouldn't affect the index value. Interestingly the original index proposed by \cite{FT74} was not rotationally invariant. A consistent index means that small perturbations to the sample do not dramatically change the index value. This is particularly important to making optimisation feasible, small angles between projections correspond to small perturbations of the sample, and thus shold be small changes to index value. \cite{posse95b} suggests that indexes should be resistant to features in a tail of the distribution, but this is debatable because one departure from normality that is interesting to detect are anomalous observations. Some PPI are designed precisely for these reasons. Lastly, because we need to compute the PPI over many projections, it needs to be fast to compute. These form the basis of the crteria upon which the scagnostic indexes, and the several alternative indexes are examined, as explained below.

- **smoothness**: This is the consistency property mentioned above. The index function values are examined over interpolated tour paths, that is, the value is plotted against time, where time indexes the sequence of projections produced by the tour path. The signature of a good PPI is that the plotted function is relatively smooth. The interpolation path corresponds to small angle changes between projections, so the value should be very similar. 
<!-- 
  should evolve smoothly between nearby projections, i.e. on an interpolated path we expect to observe slow and consistent change in the index function. \textcolor{red}{maybe can relate this to consistency consition developed by P. Hall, but don't understand this at the moment. In addition this relates to questions of optimiser choice and efficiency.} We evaluate smoothness both for transitions between noise variables and when moving from noise variables to an informative view in Section \ref{sec:smooth}.
-->
- **squintability**: \cite{barnett1981interpreting} introduced the idea of squint angle to indicate resolution of structure in the data. Fine structure like the parallel planes in the infamous RANDU data \cite{compaqFortran} (FIXME this is fortran reference, could not find one for the dataset) has a small squint angle because you have to be very close to the optimal projection plane to be able to see the structure. Structures with small squint angle are difficult to find, because the optmisation algorithm needs to get very close to begin hill-climbing to the optimum. The analyst doesn't have control over the data structure, but does have control over the PPI. Squintability is about the shape of the PPI over all projections. It should have smooth low values for noise projections and a clearly larger value, ideally with a big squint angle, for structured projections. The optimiser should be able to clearly see the optimal projections as different from noise. To examine squintability, the PPI values are examined on interpolated tour paths between a noise projection and a distant structured projection. 
<!--
distribution of index values: the distribution of index values when studying data points without structure should have a low variance, and be clearly separated from index values obtained when looking at structured data distributions. \textcolor{red}{I dont really understand this part. I think it can instead be related to the concept of squint angle \cite{barnett1981interpreting}.} We show examples in Section \ref{sec:dist}
-->
- **flexibility**: An analyst can have a toolbox of indices that may cover the range of fine and broad structure, which underlies the scagnostics suite. Early indexes, based on density estimation could be programmed to detect fine or large structure by varying the binwidth. This is examined by using a range of structure in the simulated data examples. 
<!--
ability to find fine and broad structure: \textcolor{red}{there are two different considerations, one  that relates to the concept of squint angle discussed in \cite{posse95b}. On the other hand should also consider detection of "fine" structure vs noise, this would be controlled e.g. by the allowed number of bins for the MINE index functions, additionally this will impact computing times, i.e. trade accuracy for efficiency}. We address these questions in Section \ref{sec:params} with an illustrative example.-->
- **rotation invariance**: The orientation of structure within a projection plane should not change the index value. This is especially important when using the projection pursuit guided tour, because the tour path is defined between planes, along a geodesic path, not bases within planes. If a particular orientation is more optimal, this will get lost as the projection shown pays no attention to orientation. \cite{Buja2004ComputationalMF} describes alternative interpolation paths based on Givens and Householder rotations which progress from basis to basis. It may be possible to ignore rotation invariance with these interpolations but there isn't a current implementation, primarily because the within-plane spin that is generated is distracting from a visualisation perspective. Rotation invariance is checked for the proposed PPIs by rotating the structured projection, within the plane. <!--
in the tour there is an assumed degeneracy between all projections obtained when rotating within the projected space only, for 2-d projections it means that we cannot assing x- and y- axis without ambiguity. A projection pursuit index should therefore be invariant under rotation of the basis in the projection plane. By construction this is not the case for most index functions defined above. We illustrate the impact of non invariance in Section \ref{sec:rot}. \textcolor{red}{consider frame interpolation rather than plane interpolation, which will avoid issues of rotation when interpolating, reamining issue would be optimisation efficiency.}
-->
- **speed**: Being fast to compute allows the index to be used in real-time in a guided tour, where the optimisation can be watched. When the computations are shifted off-line, to watch in replay, computation times matter less. This is checked  by comparing times for benchmark scenarios with varying sample size and dimension. 
    
## Simulation study

### Data construction
\label{sec:dataOv}

Three families of data simulations are used for examining the behaviour of the index functions. Each generates  structure in two variables, with the remaining variables containing various types of noise. This is a very simple construction, because there is no need for projection pursuit to find the structure, one could simply use the PPIs on pairs of variables. However, it serves the purpose to also evaluate the PPIs. The three data families are explained below. In each set, $p$ is used for the number of points, $n$ is the number of dimensions, and $d=2$ is the projection dimension. 

- **pipe**: nuisance directions are generated by sampling independently from a uniform distribution between $[-1,1]$, and the circle is generated by sampling randomly on a 2D circle, and adding a small radial noise. The circle should be easy to see by some indices because it is large structure, but the nonlinearity creates a complication. 
- **sine**:  nuisance directions are generated by sampling independently from a standard normal distribution, and the sine curve is generated by $x_n = \sin(x_{n-1}) + \mathrm{jittering}$. The sine is medium nonlinear structure, which should be visible by multiple indices.
- **spiral**:  nuisance directions are generated by sampling independently from a normal distribution, and the structure directions are sampled from an Archimedean spiral, i.e. $r = a + b \theta$, with $a=b=0.1$ and we sample angles $\theta$ from a normal distribution with mean 0 and variance $2\pi$, giving a spiral with higher densities at lower radii. The absolute value of $\theta$ fixes the direction of the spiral shape. This is fine structure which is only visible close to the optimal projection.

The structured projection is in fifth and sixth variables. Two samples sizes are also used: $n=(100, 1000)$. All variables are standardized to mean 0 and standard deviation 1. 

```{r util}
#defining index functions to be used with the tour
scagIndex <- function(scagType){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(sR[scagType])
  }
}

scagIndexNbin <- function(scagType, n){
  function(mat){
    sR <- scagnostics.default(mat[,1], mat[,2], bins = n)$s
    return(sR[scagType])
  }
}

invConvexIndex <- function(){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(1 - sR["Convex"])
  }
}

splineIndex <- function(){
  function(mat){
    return(splines2d(mat[,1], mat[,2]))
  }
}

dcorIndex <- function(){
  function(mat){
    return(dcor2d(mat[,1], mat[,2]))
  }
}

mineIndex <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2])[[mineIndex]])
  }
}

mineIndexE <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2], est = "mic_e")[[mineIndex]])
  }
}

mineIndexAlpha <- function(mineIndex, alpha){
  function(mat){
    return(mine(mat[,1], mat[,2], alpha=alpha)[[mineIndex]])
  }
}
```


```{r datasetFunctions}
sphereData <- function(n, p){
  dRet <- geozoo::sphere.solid.random(n,p)
  return(as.tibble(dRet$points))
}

pipeData <- function(n, p){
  i <- 1
  dRet <- NULL
  while(i <= p){
    v <- runif(n, -1, 1)
    if(abs(v[n-1]*v[n-1] + v[n]*v[n] - 1) < 0.1){
      dRet <- rbind(dRet, v)
      i <- i+1
    }
  }
  return(as.tibble(dRet))
}

sinData <- function(n, p){
  vName <- paste0("V",n)
  vNameM1 <- paste0("V",n-1)
  expr <- paste0(vName,"=sin(",vNameM1,")") # need string expression if I want to use tibble here
  dRet <- as.tibble(matrix(rnorm((n-1)*p), ncol=(n-1))) #generate normal distributed n-1 dim data
  dRet <- mutate_(dRet, expr) #string evaluation calculates var(n) as tan(var(n-1))
  colnames(dRet)[n] <- vName #correct name of new variable
  dRet[vName] <- jitter(dRet[[vName]]) #adding noise
  return(dRet)
}

spiralData <- function(n, p){
  i <- 1
  a <- 0.1
  b <- 0.1
  dRet <- NULL
  while(i <= p){
    v <- rnorm(n-2)
    theta <- abs(rnorm(1,0,2*pi))
    r <- a + b * theta
    x <- r * cos(theta)
    y <- r * sin(theta)
    v <- c(v, x, y)
    dRet <- rbind(dRet, v)
    i <- i+1
  }
  return(dRet)
}

```

```{r datasets}
# sample(1000:9999, 1)
set.seed(3705)
spiral100 <- spiralData(6, 100) %>% scale() %>% as_tibble()
spiral1000 <- spiralData(6, 1000) %>% scale() %>% as_tibble()
#sphere100 <- sphereData(6, 100) %>% scale() %>% as_tibble()
#sphere1000 <- sphereData(6, 1000) %>% scale() %>% as_tibble()

pipe100 <- pipeData(6, 100) %>% scale() %>% as_tibble()
pipe1000 <- pipeData(6,1000) %>% scale() %>% as_tibble()

sin100 <- sinData(6, 100) %>% scale() %>% as_tibble()
sin1000 <- sinData(6, 1000) %>% scale() %>% as_tibble()
```

Figure \ref{data} shows samples from each of the families, of the nuisance and structured pairs of variables. 

<!--
To get an overview of the dataset we show views of the first two directions showing the independent distribution and the last two directions showing the special view. The enforced relationships will also affect the 1-d densities as can be seen from \ref{fig:dataPlotsDensity}, which means that views including sizable fractions of $x_6$ (and $x_5$ for the Pipe and Spiral distribution) will be distinct from uninformed views of distributions 2 and 3.
-->

```{r data, fig.width=6, fig.height=7, fig.cap="Scatterplots of pairs of variables from samples of each family, showing the nuisance variables and structured variables."}
dsn <- spiral1000 %>% select(V1, V2) %>%
  rename(X1=V1, X2=V2) %>%
  mutate(family="spiral", type="nuisance")
dss <- spiral1000 %>% select(V5, V6) %>%
  rename(X1=V5, X2=V6) %>%
  mutate(family="spiral", type="structured")
dpn <- pipe1000 %>% select(V1, V2) %>%
  rename(X1=V1, X2=V2) %>%
  mutate(family="pipe", type="nuisance")
dps <- pipe1000 %>% select(V5, V6) %>%
  rename(X1=V5, X2=V6) %>%
  mutate(family="pipe", type="structured")
din <- sin1000 %>% select(V1, V2) %>%
  rename(X1=V1, X2=V2) %>%
  mutate(family="sine", type="nuisance")
dis <- sin1000 %>% select(V5, V6) %>%
  rename(X1=V5, X2=V6) %>%
  mutate(family="sine", type="structured")
d <- bind_rows(dsn, dss, dpn, dps, din, dis)
ggplot(d, aes(x=X1, y=X2)) + geom_point(alpha=0.3) + 
  facet_grid(type~family) + theme(aspect.ratio=1) +
  xlab("") + ylab("")
```

### Property assessment

The family of data sets can be used to assess the PPIs on the desirable properties: smoothness, squintability, flexibility, rotation invariance, and speed. The procedures are as follows:

- Compare the index values for structured vs nuisance variables, by simulating many examples from the model families, and recording the 90% confidence intervals. This checks if the PPI can detect the structure, that it has distinct differences between the sets of values. 
- Use the tour to show the index values along an interpolation between pairs of nuisance variables, $x_1 - x_2$ view either to the $x_3 - x_4$. The result is ideally a smooth change in low values. This checks the smoothness property. 
- Use the tour to show the index values along an interpolation between a pair of nuisance variables $x_1 - x_2$ and the structured pair of variables $x_5 - x_6$. This examines the squintability, and smoothness. If the function is smooth and slowly increases towards the structured projection, then the structure is visible from a distance. 
- Ease of optimisation depends on having a smooth function, and this is tested using a guided tour, showing the optimisation path, starting from $x_1 - x_2$.  One index is optimised to show how effectively the maximum is attained, and the values for other PPIs is examined along the same path, which allows an assessment of similarity between PPIs. 
- Rotation invariance is checked by computing PPIs on rotations of the structured projection. 
- Varying the parameters for PPIs is used to examine the effect on value, smoothness, how flexible the index can be to find different structure and whether the index is fiddly to tune. Trace plots between noise and structured projections are used to examine this. 
- Benchmarking on a range of sample sizes is used to assess the computational speed.

<!--
\textcolor{red}{\cite{posse95a} uses simulations with "strongly hidden structure", i.e. truncated Gaussian distributions with approximately identity covariance matrix, such that they don't appear in any of the principal component planes. In particular seems that spiral is hardest to identify based on results from Posse, maybe I can add this to my sphere dataset? or instead of it?}
-->


```{r dataPlotsV1V2, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the first two parameters $x_1$ and $x_2$, i.e. showing a typical uninformed view for each dataset.", eval=FALSE}
pL <- list()
i <- 1
pLabels <- c("Spiral 100", "Spiral 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(spiral100, spiral1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V1, V2)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```


```{r dataPlotsV5V6, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the last two parameters $x_5$ and $x_6$, i.e. showing the special view built into the datasets.", eval=FALSE}
pL <- list()
i <- 1
pLabels <- c("Spiral 100", "Spiral 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(spiral100, spiral1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V5, V6)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

<!--
its possible to do a single figure legend - need to look up code for this
-->

```{r dataPlotsDensity, fig.width=6, fig.height=7, fig.cap="One dimensional density distribution for all parameters and all data sets considered. Apart from statistical fluctuations we see that for the Pipe datasets the parameters $x_5$ and $x_6$ are pushed towards more extreme values by vetoing points away from the circle outline, and for the Sine (and Spiral) dataset the parameter $x_6$ (and $x_5$) follows a very distinct distribution.", eval=FALSE}
pL <- list()
i <- 1
pLabels <- c("Spiral 100", "Spiral 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(spiral100, spiral1000, pipe100, pipe1000, sin100, sin1000)){
  distData <- melt(ds)
  pC <- ggplot(distData, aes(value, color=variable)) +
    geom_density() +
    ggtitle(pLabels[i])
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

Table \ref{tab:indexTable} compares the PPIs for structured projections against those for nuisance variables, based on 100 simulated data sets of each type, using sample size 1000. The lower and upper show the 5th and 95th percentile of values. All indexes, except convex show distinctly higher values for the structured projections. The convex index considers the noise projections more interesting than the structured projections, which might be expected, but also provides an argument to use (1-convex) as the PPI. 

<!--
We next compare the various index values for three views: $x_1$ vs $x_2$, $x_5$ vs $x_6$ and $x_1$ vs $x_6$ (capturing differences from the different distribution in $x_6$). For the Spiral distribution the scagnostics indices skinny and stringy are large and close to their theoretic maximum for the special view. On the other hand while dcor2d is larger is also increased for the special view, the value is far from the theoretic maximum and larger values may be found in other views. MIC and TIC are also found to be sensitive to the spiral distribution. Considering next the Pipe distribution, we see that skinny, stringy and dcor2d are increased for the special view, but in particular MIC and TIC index are much larger and may be used to detect this type of structure in the dataset. Moreover, minimising the convex index (or maximising 1-convex) should also allow us to detect the special view. Finally the Sine distribution shows that several of the indices are maximised in the special view, in particular also the splines2d index designed for the detection of fictional dependence.
-->

```{r indexTable, echo=FALSE}
tableTemp <- function(){
  tibble(
    index=character(),
    data=character(),
    size=numeric(),
    type=character(),
    value5=numeric(),
    value95=numeric()
  )
}

getSample <- function(dataT, size){
  if (dataT == "Pipe"){
    ret <- pipeData(p = size, n = 6) %>% scale() %>% as_tibble()
  }
  if (dataT == "Spiral"){
    ret <- spiralData(p = size, n = 6) %>% scale() %>% as_tibble()
  }
  if (dataT == "Sine"){
    ret <- sinData(p = size, n = 6) %>% scale() %>% as_tibble()
  }
  return(ret)
}

getIdx <- function(dataT, size, type){
  convex <- numeric(length = 100)
  skinny <- numeric(length = 100)
  stringy <- numeric(length = 100)
  micidx <- numeric(length = 100)
  ticidx <- numeric(length = 100)
  dcor2D <- numeric(length = 100)
  splines2D <- numeric(length = 100)
  holesI <- numeric(length = 100)
  for(i in 1:100){
    cSample <- getSample(dataT, size)
    if(type == "noise"){
      x = as.numeric(cSample$V1)
      y = as.numeric(cSample$V2)
    } else{
      x = as.numeric(cSample$V5)
      y = as.numeric(cSample$V6)
    }
    scag <- scagnostics(x=x, y=y)$s
    midx <- mine(x, y, est = "mic_e")
    skinny[i] <- scag["Skinny"]
    stringy[i] <- scag["Stringy"]
    convex[i] <- scag["Convex"]
    micidx[i] <- midx$MIC
    if (size == 100) {ticidx[i] <- midx$TIC / 16}
    if (size == 1000) {ticidx[i] <- midx$TIC / 148}
    dcor2D[i] <- dcor2d(x=x, y=y)
    splines2D[i] <- splines2d(x,y)
    holesI[i] <- holes(matrix(c(x,y), ncol = 2))
    
  }
  #sortedList <- lapply(list(skinny, stringy, convex, micidx, ticidx, dcor2D, splines2D, holesI), sort)
  skinny <- sort(skinny)
  stringy <- sort(stringy)
  convex <- sort(convex)
  micidx <- sort(micidx)
  ticidx <- sort(ticidx)
  dcor2D <- sort(dcor2D)
  splines2D <- sort(splines2D)
  holesI <- sort(holesI)
  res <- tableTemp() %>%
    add_row(index="skinny", data=dataT, size=size,
            type=type,
            value5=skinny[5], value95=skinny[95]) %>%
    add_row(index="stringy", data=dataT, size=size,
            type=type,
            value5=stringy[5], value95=stringy[95]) %>%
    add_row(index="convex", data=dataT, size=size,
            type=type,
            value5=convex[5], value95=convex[95]) %>%
    add_row(index="MICe", data=dataT, size=size,
            type=type,
            value5=micidx[5], value95=micidx[95]) %>%
    add_row(index="TICe", data=dataT, size=size,
            type=type,
            value5=ticidx[5], value95=ticidx[95]) %>%
    add_row(index="dcor2D", data=dataT, size=size,
            type=type,
            value5=dcor2D[5], value95=dcor2D[95]) %>%
    add_row(index="splines2D", data=dataT, size=size,
            type=type,
            value5=splines2D[5], value95=splines2D[95]) %>%
    add_row(index="holes", data=dataT, size=size,
            type=type,
            value5=holesI[5], value95=holesI[95])
}

if(!file.exists("cache/indexTable.rda")){
  set.seed(505)
  res <- tableTemp()
  for(ds in c("Pipe", "Sine", "Spiral")){
    for (size in c(100, 1000)){
      for (type in c("structure", "noise")){
        res <- bind_rows(res,getIdx(ds, size, type))
      }
    }
  }
  res <- res %>% arrange(data, size, index, type) %>% select(data, size, index, type, value5, value95)
  save(res, file = "cache/indexTable.rda")
} else {
  load("cache/indexTable.rda")
}
resp <- res %>% filter(size == 1000, data == "Pipe") %>%
  select(index, type, value5, value95) %>%
  rename(lower=value5, upper=value95)
resi <- res %>% filter(size == 1000, data == "Sine") %>%
  select(index, type, value5, value95) %>%
  rename(lower=value5, upper=value95)
ress <- res %>% filter(size == 1000, data == "Spiral") %>%
  select(index, type, value5, value95) %>%
  rename(lower=value5, upper=value95)
res_all <- bind_cols(resp, resi, ress) %>%
  select(-index1, -type1, -index2, -type2) %>%
  mutate(index = fct_relevel(index, "holes", "convex", "skinny", "stringy", "dcor2D", "splines2D", "MICe", "TICe")) %>%
  arrange(index) %>%
  mutate(index = c("holes", "", "convex", "", "skinny", "", "stringy", "", "dcor2D", "", "splines2D", "", "MICe", "", "TICe", ""))
# Good resource for doing tables
# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf
kable(res_all, format = "latex",  caption = "Comparison of index values between noise projections and structured projections for sample size 1000, using 5th and 95th percentiles from 100 simulated sets. Most indices have much larger values for the structured projections, except for convex.", 
      col.names=c("Index","", "lower", "upper", "lower", "upper", "lower", "upper"),
      digits=2, booktabs = T) %>% 
      add_header_above(c(" ", " ", "pipe" = 2, "sine" = 2, "spiral"=2)) %>%
  group_rows("", 1, 2) %>%
  group_rows("", 3, 4) %>%
  group_rows("", 5, 6) %>%
  group_rows("", 7, 8) %>%
  group_rows("", 9, 10) %>%
  group_rows("", 11, 12) %>%
  group_rows("", 13, 14) %>%
  group_rows("", 15, 16) %>%
  kable_styling(position = "center")
```


## Index traces over tour sequence of interpolated nuisance projections
\label{sec:smooth}

Figure \ref{fig:plotV1V2toV3V4} shows the traces representing the index values calculated across a tour between a pair of nuisance projections. The range of each axis is set to be the limits of the index, as might be expected over many different data sets, 0 to 1. Each projection in the interpolation will also be noise. Two different sample sizes are show, $n=100$ as a dashed line, and $n=1000$ as a solid line. The ideal trace is a smooth function, with relatively low values, and no difference between the sample sizes. A major feature to notice is that the scagnostics produce noisy functions, which is problematic, because small changes in the projection result in big jumps in the value. This will make them difficult to optimize. TIC also is noisy. \textcolor{red}{why is TIC so high? did it get inverted?} On the other hand holes, dcor2d, splines2d and MIC are relatively smooth functions. \textcolor{red}{why is holes so high? it should range between 0-1, maybe needs to be better standardised, too.} Several of the indexes are sensitive to sample size also, the same structured projection with differing numbers of points, produces different values. 

<!--
We first study the smoothness of each measure calculated on a sequence of 2-d projections obtained via an interpolated planned tour path moving between the projection onto $x_1$-$x_2$ to the projection onto $x_3$-$x_4$ and using the default interpolation angle of 0.05. Each time step refers to an interpolation step. Since the value of the TIC index is strongly dependent on the input distribution sample size we have normalised the TIC measure by its maximum for each individual dataset, all other measures are reproduced as reported by the calculation. The results are shown in Figure \ref{fig:plotV1V2toV3V4}. The standard index functions "holes" and "cmass" show the desired behaviour in that the change is smooth over interpolated views. Considering now the indices from the scagnostics family, we first note stronge dependence on the sample size. The convex index is largely sensitive to random fluctuations when considering small samples ($n=100$), while being mostly flat for larger samples, especially in the case of an underlying uniform distribution. The skinny index on the other hand is systematically larger for the smaller samples, while stringy appears not to be sensitive to the sample size, but resulting in large fluctuations and sharp jumps for all datasets. Distance correlation and the spline based index show similar behaviour, i.e. when considering large enough data samples ($n=1000$) the values are consistently close to zero, as expected for a distribution without structure. On the other hand somewhat larger values (but still far from the maximum value of 1) are found when considering small datasets, where distributions are however mostly smooth, with few kinks depending on index function and underlying distribution. Finally the information based metrics also show systematic differences based on the sample size, with sampling fluctuations being significant for small datasets. Note here that fluctuations present in Figure \ref{fig:plotV1V2toV3V4} may be amplified, as maximum values for most index functions are far from the theoretic maximum.
-->

<!-- index values shouldn't be affected by sample size, on average it should be
same value. Maybe some need to be normalised by n. Something needs fixing here.

can we get "theoretical" min/max values, to assess the scale of the index for any particular data set.
-->

```{r getProj}
getProj <- function(df, tPath, nameStr, size){
  sc <- tibble(
    holes=numeric(),
    cmass=numeric(),
    convex=numeric(),
    skinny=numeric(),
    stringy=numeric(),
    dcor2d=numeric(),
    splines2d=numeric(),
    MIC=numeric(),
    TIC1=numeric(),
    t=numeric(),
    name=character(),
    size=numeric())
  n <- length(tPath)
  for (i in 1:n) {
    dprj <- as.matrix(df) %*% tPath[[i]]
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    cmassRes <- cmass(dprj)
    sc <- add_row(sc, holes=holesRes, cmass=cmassRes,
                  convex=scagRes[,"Convex"], skinny=scagRes[,"Skinny"],
                  stringy=scagRes[,"Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC1=mineRes$TIC, t=i, name=nameStr, size=size)
  }
  if (size == 100) {maxTIC <- 16}
  if (size == 1000) {maxTIC <- 148}
  sc <- sc %>%
    mutate(TIC = TIC1/maxTIC) %>%
    select(-TIC1)
  return(sc)
}
```

```{r V1V2toV3V4}
if(!file.exists("cache/V1V2toV3V4.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,1,0,0,0,0,0,0,1,0,0),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t1 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t1full <- as.list(interpolate(t1))
  fullRes <- getProj(sin100, t1full, "sin", 100) %>%
    rbind(getProj(sin1000, t1full, "sin", 1000)) %>%
    rbind(getProj(spiral100, t1full, "spiral", 100)) %>%
    rbind(getProj(spiral1000, t1full, "spiral", 1000)) %>%
    rbind(getProj(pipe100, t1full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t1full, "pipe", 1000))
  save(t1, t1full, fullRes, file = "cache/V1V2toV3V4.rda")
} else {
  load("cache/V1V2toV3V4.rda")
}
```

```{r plotV1V2toV3V4, fig.width=6, fig.height=7, fig.cap="PPIs for projections along an interpolation between two nuisance projections. All projections would be nuisance so the PPI are ideally low and smooth, with little difference between sample sizes (solid lines: $n=1000$; dashed: $n=100$). The scagnostic PPIs are noisy, as is TIC. Some indexes have distinct differences in values between sample sizes."}
fullResMelt <- fullRes %>%
  mutate(convex = 1-convex) %>% 
  gather(PPI, value, -t, -name, -size) %>%
  filter(PPI != "cmass") %>%
  mutate(PPI = fct_relevel(PPI, "holes", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
fullResMelt %>% 
  ggplot(aes(x=t, y=value)) + 
  geom_line(aes(group=size, linetype=factor(size, levels=c("1000", "100")))) + 
  ylim(c(0,1)) +
  facet_grid(PPI~name) + 
  theme(legend.position="none") + xlab("Sequence of projections (t)") + ylab("PPI value")
```

```{r plannedtourpath}
if(!file.exists("cache/plannedtourpath.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,0,0,1,0,0,0,0,0,0,1),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t2 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t2full <- as.list(interpolate(t2))
  fullResPlanned <- getProj(sin100, t2full, "sin", 100) %>%
    rbind(getProj(sin1000, t2full, "sin", 1000)) %>%
    rbind(getProj(spiral100, t2full, "spiral", 100)) %>%
    rbind(getProj(spiral1000, t2full, "spiral", 1000)) %>%
    rbind(getProj(pipe100, t2full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t2full, "pipe", 1000))
  save(t2, t2full, fullResPlanned, file = "cache/plannedtourpath.rda")
} else {
  load("cache/plannedtourpath.rda")
}
```

```{r plannedtour, fig.width=6, fig.height=7, fig.cap="PPIs for projections along an interpolation between a nuisance and a structured projection (solid lines: $n=1000$; dashed: $n=100$). Most of the indexes have peaks at the end of the sequence, indicating that they see the structure. The scagnostics, MIC and TIC see all three structures, so are more flexible for general pattern detection."}
#fullResMeltPlanned <- melt(fullResPlanned, id=c("t", "name", "size"))
fullResMeltPlanned <- fullResPlanned %>%
  mutate(convex = 1-convex) %>% 
  gather(PPI, value, -t, -name, -size) %>%
  filter(PPI != "cmass") %>%
  mutate(PPI = fct_relevel(PPI, "holes", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
fullResMeltPlanned %>% 
  ggplot(aes(x=t, y=value)) + 
  geom_line(aes(group=size, linetype=factor(size, levels=c("1000", "100")))) + 
  ylim(c(0,1)) +
  facet_grid(PPI~name) + 
  theme(legend.position="none") + xlab("Sequence of projections (t)") + ylab("PPI value")
```

## Index traces over tour sequence between nuisance and structured projections
\label{sec:squintability}

Figure \ref{fig:plannedtour} shows the PPIs for a tour sequence between a nuisance and structured projection. Sample size is indicated by linetype: dashed being $n=100$ and solid is $n=1000$. The beginnnig of the sequence is the nuisance projection and the end is the structured projection. The index values for most PPIs increases substantially nearing the structured projection, indicating that they "see" the structure. Some indexes see all three structures: scagnostics, MIC and TIC,  which means that they are flexible indexes capable of detecting a range of structure. [@Grimm2016]'s indexes, dcor2d and splines2d, are excellent for detecting the sine, and they can see it from far away, indicated by the long slow increase in index value. The holes index easily detects the pipe, and can see it from a distance. The scagnostic index, stringy, can see the structure but is myopic, only when it is very close. Interestingly the scagnostic, skinny, sees the spiral from a distance.  

\textcolor{red}{XXX This shouldn't go here. The index needs to have been introduced much earlier in the paper. TIC was suggested in @JMLRv1715308 as an alternative to MIC with better properties for testing against independence, this is because the full characteristic matrix is used, implying that more information is considered. We also see that TIC is smoother when moving into the special view, in terms of the relative differences.}
<!--
We repeat the same exercise, but now we are interested in how the index values change when going from the view in $x_1$ vs $x_2$ to the view in $x_5$ vs $x_6$, which will also allow us to view the evolution between typical low and high values of the index function. Results are shown in Figure \ref{fig:plannedtour}.
We can see a pretty clear and smooth increase in dcor2d and splines2d as well as MIC and TIC index when approaching the special view of the Sine distribution, which should therefore be efficient to optimise. On the other hand while we also see smooth increase in MIC and TIC when approaching the special view of the Pipe distribution, the increase only becomes apparent much closer to the final projection, making this structure more challenging to detect with a guided tour. Finally the Spiral appears to be the most challenging, having an even smaller squint angle than the Pipe distribution. While "cmass" appears to be appropriate to identify this view we may actually find larger values of this index for random distributions, making it unfit to use in a guided tour. The scagnostics indices skinny (or 1-convex) appear to be sensitive to the structure at a larger angle compared to MINE indices, but will suffer from the large fluctuations in random views when used in a guided tour.
Various indices appear less noisy than seen in \ref{fig:plotV1V2toV3V4}, but note that the y-axis scale is different (typically reaching higher index values in this example, whereas smaller fluctuations are more amplified in the previous views).

TIC was suggested in @JMLRv1715308 as an alternative to MIC with better properties for testing against independence, this is because the full characteristic matrix is used, implying that more information is considered. We also see that TIC is smoother when moving into the special view, in terms of the relative differences.

\textcolor{red}{Holes index or even pca could discover pipe as well...}
-->


<!--
I don't think this section adds anything to the paper
## Distributions of index values
\label{sec:dist}
Compare typical index values over the two tour paths. Notation: path 1 / 2 labels tour from $x_1$-$x_2$ to $x_3$-$x_4$ / $x_5$-$x_6$. Typically find broader distributions for small samples because of the large fluctuations in this case. Some indices show preferred behaviour, i.e. sharp peaks around low values for path 1, and very broad distributions leading up to maximum values for path 2. For other indices we can see the strong dependence of the index on sample size, the most extreme example is the skinny index.

\textcolor{red}{Need better representation, in particular normalise TIC to upper limit for given sample size, adapt scales so sharp peaks don't dominate some of the plots.}

```{r indexDist, fig.width=6, fig.height=7, fig.cap="Comparing distributions of index values found in the interpolated tour paths 1 and 2 as described in the text."}
fullResMelt1 <- fullResMelt %>% add_column(path="path1")
fullResMeltPlanned1 <- fullResMeltPlanned %>% add_column(path="path2")
allRes <- rbind(fullResMelt1, fullResMeltPlanned1)
ggplot(allRes, aes(x=value)) +
  geom_density(aes(color=path, linetype = factor(size))) +
  facet_wrap(PPI~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE))
```

\textcolor{red}{testing rotation dependence by turning sine dataset with $\alpha \in [0,2\pi]$, maybe can add second path for spiral?}
Interesting to note that some indexes are symmetric around $\pi$, but scagnostics indexes are very arbitrary, dependence is introduced via binning (and outlier removal?) here. Observe huge drops for some of the indexes! 
-->

## Ease of optimisation
\label{sec:guided}

- remind reader of the options for optimisation in guided tour
- how you use them here
- 

The guided tour combines optimisation with interpolating between pairs of planes. Target planes of the path are chosen by a random search method to find projection planes with high index values, which are then interpolated between. It operates roughly like simulated annealing that as one gets closer to an optimal value the random search range is reduced.  

Given the behaviour of the various index measures under an interpolated tour path we next try to use the tourr implementation of the guided tour together with selected index functions to uncover the special views in datasets 2 and 3 with 1000 datapoints.


### Looking down the pipe

\textcolor{red}{Double challenging: fine structure + (coherent) noise}
\textcolor{red}{simple standard guided tour does not find pipe easily with holes index, did not try brute force approach used with TIC index yet}

As seen before (reference figure) MIC or TIC should be able to find the view down the pipe. The optimisation itself is however expected to be challenging because one has to be fairly close to the optimal projection to observe the increase in index value. We therefore suggest a two step procedure for the uncovering of such "small scale structures". The first step consists in "scouting" projections. Here we use the "search_better" optimisation available with the tourr package, with a standard search window $\alpha = 0.5$ and disabling the cooling (in practice this can be done by fixing the cooling parameter to $1$), together with a large number of tries (setting max.tries to 5000 here). The resulting search will quickly steer away from the most uninteresting projections, followed by a broad scan of the thus identified region. While the lack of cooling means that this will not converge towards the maximum, each step in this first search will increase the index value, thus it can be used to provide an informed starting plane that can be used in a second step where we use the "search_geodesic" optimization to improve the index further towards the maximum.
The described procedure is CPU time intensive, however it is promising that we manage to uncover such small scale structure in the distribution. On the other hand no "guarantee" can be provided for uncovering the special view. Parameters for the scouting phase will need to be adapted to different examples, and with increasing number of dimensions it may be better to consider several runs with different starting projections.
This gives some indication on how to handle difficult datasets. As we will see below most of the time standard "search_geodesic" optimisation will be efficient enough to uncover the interesting views.

The results are shown below, first Fig. \ref{fig:pipeFirstRun} shows how the considered index values evolve in the scouting phase. Note that this is the interpolated path and there are small local maxima and minima along the index value, especially because we keep the step size in the guided tour large. We can see the clear maximum of the TIC index in the final projection, as well as clear minimum of the convex index that carries similar information On the other hand we see that MIC or the dcor2d index are not suitable for uncovering the special view here.

\textcolor{red}{Sometimes index value decreases, because index is not rotationally invariant.}


```{r findpipe, results="hide"}
if(!file.exists("cache/findpipe.rda")){
  set.seed(1984)
  pipeResc <- as.matrix(pipe1000) 
  pipeTour <- save_history(pipeResc,
                          guided_tour(mineIndex("TIC"), search_f = tourr:::search_better,
                                     cooling=1, alpha = 0.5, max.tries = 5000))
  pipeTourFull <- as.list(interpolate(pipeTour))
  save(pipeResc, pipeTour, pipeTourFull, file = "cache/findpipe.rda")
} else {
  load("cache/findpipe.rda")
}
```

```{r pipeFirstRun,fig.width=6, fig.height=7, fig.cap="Index value on tour path when scouting for high TIC index values."}
if(!file.exists("cache/pipeFirstRun.rda")){
  pipeTourRes <- getProj(pipe1000, pipeTourFull, "Pipe", 1000)
  pipeTourResMelt <- melt(pipeTourRes, id=c("t", "name", "size"))
  save(pipeTourRes, pipeTourResMelt, file = "cache/pipeFirstRun.rda")
} else {
  load("cache/pipeFirstRun.rda")
}
newBasisPipeTour <- which(attributes(interpolate(pipeTour))$new_basis)
ggplot(pipeTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")+
  geom_vline(data=as.tibble(newBasisPipeTour), mapping=aes(xintercept=value), color="red")
```

We next show in Fig \ref{fig:testpipe} (left) the final view corresponding to the highest index found in the TIC scouting phase. While this is clearly not the optimal view of the pipe feature, we can already identify the feature in the distribution, and we expect to be able to find the maximum when using this plane as a starting point of the geodesic optimisation. Indeed performing a standard guided tour with the TIC index and the identified starting plane efficiently identifies the optimal view shown in the right plot. In this case the optimisation quickly converges, following a smooth, monotonic increas of the TIC index.


```{r refinepipe, results="hide"}
if(!file.exists("cache/refinepipe.rda")){
  iLast <- length(pipeTourFull)
  fProj <- pipeTourFull[[iLast]]
  pipeTour2 <- save_history(pipeResc, guided_tour(mineIndex("TIC")), start = fProj)
  pipeTourFull2 <- as.list(interpolate(pipeTour2))
  pipeTourRes2 <- getProj(pipe1000, pipeTourFull2, "Pipe", 1000)
  pipeTourResMelt2 <- melt(pipeTourRes2, id=c("t", "name", "size"))
  save(pipeTour2, pipeTourFull2, pipeTourRes2, pipeTourResMelt2, file = "cache/refinepipe.rda")
} else {
  load("cache/refinepipe.rda")
}
```


```{r testpipe,  fig.height=4, fig.cap="Final view discovered by the scouting phase (left) and after second optimisation (right)."}
iLast <- length(pipeTourFull)
fProj <- pipeTourFull[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
p1 <- ggplot(dProj, aes(V1,V2)) + geom_point()
iLast <- length(pipeTourFull2)
fProj <- pipeTourFull2[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
p2 <- ggplot(dProj, aes(V1,V2)) + geom_point()
grid.arrange(p1, p2, ncol=2)
```

### Finding sine waves

We next work with distribution 3 and aim to identify the view showing functional dependence using the splines2d index. Given the results from the planned tour we expect to be able to efficiently identify this view without relying on the two step procedure described above. Indeed the guided tour quickly converges in this case, the evolution of the index functions is shown in Fig \ref{fig:findsine}. Here there are four index functions that appear to be suitable for the optimisation: splines2d, dcor2d, MIC and TIC. On the other hand, while some of the scagnostics indices are reaching their maximum/minimum in the final projection, they do not show a smooth increase/decrease thus preventing efficient optimisation when considering them as index functions. The final view identified by the guided tour is shown in Fig \ref{}


```{r findsine, fig.width=6, fig.height=7, fig.cap="Guided tour optimising the splines2d index for the Sine 1000 dataset.", results="hide"}
set.seed(2018)
if(!file.exists("cache/findsine.rda")){
  sineResc <- as.matrix(sin1000)
  sineTour <- save_history(sineResc,
                          guided_tour(splineIndex()))
  sineTourFull <- as.list(interpolate(sineTour))
  sineTourRes <- getProj(sin1000, sineTourFull, "Sine", 1000)
  sineTourResMelt <- melt(sineTourRes, id=c("t", "name", "size"))
  save(sineResc, sineTour, sineTourFull, sineTourRes, sineTourResMelt, file = "cache/findsine.rda")
} else {
  load("cache/findsine.rda")
}
ggplot(sineTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")
```

```{r testsine, out.width=".5\\textwidth", fig.cap="Final view identified by the optimisation."}
iLast <- length(sineTourFull)
fProj <- sineTourFull[[iLast]]
dProj <- as.tibble(sineResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```

### Spiral detection

\textcolor{red}{can use skinny to find sprial in 3 dimensions, but not higher, scouting method does not work because of large rotation dependence, maybe make it study of guided tour success vs number of parameters entering? e.g. given n starting planes how many times was view in plane V5-V6 identified as function of number of dimensions and index used?}

\textcolor{red}{what about role of optimisation here? search\_better not working because rotation in plane makes index drop randomly, geodesic search does not work because (my guess) it picks directions based on being most promising on small step size tests, maybe need to modify code to pass step size as argument, search directions based on larger step size may result in improved scouting run? (but may also give same issues of rotation dependence) ... tried  but didnt really work...}

```{r findspiral, fig.width=6, fig.height=7, fig.cap="Guided tour optimising the skinny index for the Sprial 1000 dataset.", results="hide"}
#getResWithSeed <- function(s){
#  set.seed(s)
#  startM <- orthonormalise(matrix(runif(12),ncol = 2))
#  tourRes <- save_history(spiralResc, guided_tour(scagIndex("Skinny")), max_bases = 10000)
#}
#set.seed(202020)
#if(!file.exists("cache/findsine.rda")){
#  spiralResc <- as.matrix(spiral1000)
#spiralTour <- save_history(spiralResc, guided_tour(dcorIndex()))
#spiralTour <- save_history(spiralResc, guided_tour(scagIndex("Skinny"), max.tries = 100, n=10, stepS=0.5), max_bases = 10000)
  #spiralTour <- save_history(spiralResc,guided_tour(scagIndex("Skinny"), search_f = tourr:::search_better,
  #                                   alpha = 0.5, max.tries = 500), max_bases = 10000)
  #spiralTourFull <- as.list(interpolate(spiralTour))
  #spiralTourRes <- getProj(spiral1000, spiralTourFull, "Spiral", 1000)
  #spiralTourResMelt <- melt(spiralTourRes, id=c("t", "name", "size"))
#  save(spiralResc, spiralTour, spiralTourFull, spiralTourRes, spiralTourResMelt,
#       file = "cache/findspiral.rda")
#} else {
#  load("cache/findsine.rda")
#}
#ggplot(spiralTourResMelt, aes(x=t, y=value)) +
#  geom_line() + 
#  facet_wrap(~variable, ncol=1, scales = "free_y")
#iL <- length(as.list(spiralTour))
#dProj <- spiralResc %*% as.list(spiralTour)[[iL]]
#ggplot(as.tibble(dProj), aes(V1, V2)) + geom_point()
```


## Rotation dependence
\label{sec:rot}
```{r rotationDep, fig.width=6, fig.height=7, fig.cap="PPI for rotations of the sine 1000 data, to examine rotation invariance. Most are close to rotation invariant, except for dcor2d, splines2d and TIC."}
if(!file.exists("cache/rotationDep.rda")){
  sineM <- as.matrix(select(sin1000, V5, V6))
  sc <- tibble(
      holes=numeric(),
      cmass=numeric(),
      convex=numeric(),
      skinny=numeric(),
      stringy=numeric(),
      dcor2d=numeric(),
      splines2d=numeric(),
      MIC=numeric(),
      TIC=numeric(),
      alpha=numeric()
  )
  for (a in seq(0,2*pi, pi/100)){
    rotM <- matrix(c(cos(a), sin(a), -sin(a), cos(a)), ncol = 2)
    dprj <- sineM %*% rotM
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    cmassRes <- cmass(dprj)
    sc <- add_row(sc, holes=holesRes, cmass=cmassRes,
                  convex=scagRes[,"Convex"], skinny=scagRes[,"Skinny"],
                  stringy=scagRes[,"Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC=mineRes$TIC/148, alpha=a)
  }
  save(sc, file = "cache/rotationDep.rda")
} else {
  load("cache/rotationDep.rda")
}

scMelt <- sc %>% 
  mutate(angle=alpha*360/(2*pi), TIC = TIC) %>%
  select(-alpha) %>%
  gather(PPI, value, -angle) %>%
  mutate(PPI = fct_relevel(PPI, "holes", "cmass", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
ggplot(scMelt, aes(x=angle, y=value)) +
  geom_line() + 
  ylim(c(-1, 1.1)) +
  facet_wrap(~PPI, ncol = 3) +
  #geom_blank(aes(y = 0)) +
  #geom_blank(aes(y = 1)) +
  coord_polar() +
  xlab("") + ylab("")
```



## Note on calculation time
\label{sec:timer}
Computing time differs for each index, and depends strongly on the number of data points (as well as probably some dependence on distribution type), here we use as an example samples of points in a 6-d solid sphere (obtained via the geozoo package [@geozoo]) and calculate the computing time for different indices over 100 interpolated grand tour projections as a function of the sample size. \textcolor{red}{Since projection is fast compared to index evaluation, the number of original dimensions is not very important here. On the other hand different distributions may impact the computing time.}
We see that the evaluation of the spline index shows weak dependence on the sample size, making it the least efficient for very small data sets, but one of the fastest when considering large data sets. The situation is similar for calculating the scagnositics measures, which are however faster to compute on datasets of around 10 points. This is because the implementation is first binning the data points, which is fast compared to the evaluation that requires triangulation of the binned points. The distance correlation based measure exhibits strong increase with sample size, we note however that the implementation also allowes for evaluation on binned data points which we have not considered. Finally for the MINE family the computing time increases exponentially as can be expected from the dependence on the sample size, where we see that the improved $MIC_e$ algorithm is somewhat faster than the original implementation, see also @JMLRv1715308.

\textcolor{red}{The plot is not perfect. I use alpha to fade out points, but cant find value that gives nice picture here. In addition the timer isnt precise enough to resolve timing for some indexes on small datasets, they are zeroes, appear on bottom of the log10 y axis.}

```{r getTimer}

mineE <- function(x,y){
  return(mine(x=x, y=y, est = "mic_e"))
}


timeThis <- function(d, t, idx, pmax, n, idxName){ #d=data matrix, t=interpolated tour path, idx=index function, pmax=max number of projections, n=sample size, idxName=str name of idx funciton
    i <- 1
    dfTimer = data.frame( t= numeric(), i=numeric(), n = numeric(), name=character())
    for(pMatrix in t){
      if(i>pmax) break
      tic.clearlog()
      tic() #start timer
      dProj <- d %*% pMatrix
      sgnst <- idx(dProj[,1],dProj[,2])
      toc(log=TRUE,quiet=TRUE)
      scTd <- unlist(tic.log(format=FALSE))["toc.elapsed"]-unlist(tic.log(format=FALSE))["tic.elapsed"]
      dfTimer <- add_row(dfTimer, t=scTd, i=i, n= n, name=idxName)
      i = i+1
    }
    return(dfTimer)
}

```


```{r timer}
if(!file.exists("cache/timer.rda")){
  set.seed(2018)
  grandTour100 <- save_history(sphereData(6,100), grand_tour(2), max=4) %>%
    interpolate() %>%
    as.list()

  sizeL <- c(10, 100, 500, 1000, 5000, 10000)
  t <- 1
  dfTimer <- NULL
  for(sampleSize in sizeL){
    set.seed(sampleSize + 11) # new seed for each sample size
    if(sampleSize == 0) sampleSize = 10 # smallest considered sample has 10 points
    thisMatrix <-  sphereData(6, sampleSize) %>% rescale()
    scagT <- timeThis(thisMatrix, grandTour100, scagnostics.default, 100, sampleSize, "scagnostics")
    dcorT <- timeThis(thisMatrix, grandTour100, dcor2d, 100, sampleSize, "dcor2D")
    splineT <- timeThis(thisMatrix, grandTour100, splines2d, 100, sampleSize, "splines2D")
    mineT <- timeThis(thisMatrix, grandTour100, mine, 100, sampleSize, "MINE")
    mineeT <- timeThis(thisMatrix, grandTour100, mineE, 100, sampleSize, "MINE E")
    dfTimerC <- list(scagT, dcorT, splineT, mineT,mineeT) %>%
      reduce(rbind)
    dfTimer <- rbind(dfTimer, dfTimerC)
    t <-  t+1
  }
  save(dfTimer, file = "cache/timer.rda")
} else {
  load("cache/timer.rda")
}

timerMeans <- dfTimer %>%
  group_by(name, n) %>%
  summarise(t=mean(t))

ggplot(dfTimer, aes(x = n, y = t)) + 
  geom_point(alpha=0.1, aes(color=name, shape=name)) +
  geom_point(data=timerMeans, aes(color=name, shape=name)) +
  geom_line(data=timerMeans, aes(linetype=name, color=name)) +
  scale_x_log10(limits=c(90,10100)) +
#  scale_y_log10(limits=c(0.00001,1)) + 
  labs(x = "Sample Size", y = "Time [s]", color = "Index family", 
       linetype = "Index family", shape = "Index family") + 
  scale_colour_brewer(palette="Dark2")
```


## Effect of parameter choice in nndex value
\label{sec:params}
Some parameters must be fixed when using the index functions considered here, controlling trade-offs between noise and fine structure detection, and between computing time and precision.

- Binning:
    - Scagnostics: the number of bins can be controlled by the user, note however that internally the implementation will reduce the number of bins if too many non-empty bins are found (more than 250).
    - MINE: the maximum number of bins considered is fixed by the user as a function of the number of data points, see Equation \ref{eq:MIC}. The default is chosen such as a trade-off between resolution and noise dependence, but it may be tuned based on requirements dictated by specific datasets. Apart from sensitivity to noise computing time may also be a consideration here.
- Spline knots: for the splines2D measure we need to fix the number of knots. By default it is fixed to be 10 (or lower if appropriate based on the data values). In our examples we find the number to be appropriate to identify functional dependence while rejecting noise, but some distributions may require tuning of this parameter.

### Scagnostics binning
The implementation of scagnostics will always perform a binning step first, before evaluating the measures. The binning will introduce random fluctuations on interpolated tour paths, but we may also consider using it to reduce fluctuations from the discreteness inherent to the index definitions, see in particular the definition of the stringy index. This needs some care however as different values may be preferred for different structures and depending on which index is being considered. Moreover, choosing a binning that is too rough may result in loss of discrimination power as structures can no longer be resolved. As an example we compare in Figure \ref{fig:sprialScagBinning} the scagnostics index traces for the Spiral 1000 dataset when moving from $x_1$-$x_2$ to $x_5$-$x_6$, for three values of the bins parameter entering the hexagonal binning procedure. We find indeed that different values may be preferred depending on the index considered, with the default bins=50 being the better choice for the convex and skinny index, while bins=20 may work better for the stringy index. Note that neither choice results in smooth curves for any of the indexes considered here.

```{r spiralScagBinning, fig.cap="Comparing the traces of the three scagnostics indices when changing the binning via the bins parameter set to 10, 20 and 50 in this example. For too rough binning (bins=10) we observe random fluctuations from the binning, with structure detedction being obscured. With bins=20 we see somewhat smoother behaviour for the stringy index, especially when moving into the maximum, while the skinny index shows a dip in the final view, and larger bins=50 is preferred. For the convex index we also find bins=50 to be the better choice."}
getScagComp <- function(df, tPath){
  sc <- tibble(
    convex10=numeric(),
    skinny10=numeric(),
    stringy10=numeric(),
    convex20=numeric(),
    skinny20=numeric(),
    stringy20=numeric(),
    convex50=numeric(),
    skinny50=numeric(),
    stringy50=numeric(),
    t=numeric())
  n <- length(tPath)
  for (i in 1:n) {
    dprj <- as.matrix(df) %*% tPath[[i]]
    scagRes10 <- scagnostics.default(dprj[,1], dprj[,2], bins=10)$s
    scagRes20 <- scagnostics.default(dprj[,1], dprj[,2], bins=20)$s
    scagRes50 <- scagnostics.default(dprj[,1], dprj[,2], bins=50)$s
    sc <- add_row(sc,
                  convex10=scagRes10["Convex"], skinny10=scagRes10["Skinny"],
                  stringy10=scagRes10["Stringy"],
                  convex20=scagRes20["Convex"], skinny20=scagRes20["Skinny"],
                  stringy20=scagRes20["Stringy"],
                  convex50=scagRes50["Convex"], skinny50=scagRes50["Skinny"],
                  stringy50=scagRes50["Stringy"],
                  t=i)
  }
  
  return(sc)
}
if(!file.exists("cache/spiralScagBinning.rda")){
  scagBinningDf <- getScagComp(spiral1000, t2full)
  scagMelt <- melt(scagBinningDf, id=c("t")) %>%
    mutate(nbin = as.numeric(str_sub(variable, start = -2))) %>%
    mutate(idx = str_sub(variable, end = -3))
  save(scagBinningDf, scagMelt, file = "cache/spiralScagBinning.rda")
} else {
  load("cache/spiralScagBinning.rda")
}
ggplot(scagMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(nbin))) + 
  facet_wrap(~idx, ncol=1, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE))

```

### Illustrative example
A nice example can be constructed based on the RANDU dataset available in R. We use the MIC index and compare different parametrisations of the maximum binning by $\alpha$, where $B(n) = n^{\alpha}$. We compare in Figure \ref{fig:randuEx} the final view, index value and computing time obtained when optimising the MIC index with values $\alpha = 0.6, 0.7, 0.8$. Despite the low dimensionality, the small quint angle of the structure means that the identification of the maximum depends strongly on the starting conditions. Empirically we have found that using $\alpha = 0.7$ is most efficient in identifying the structure, while the default value $\alpha = 0.6$ is most likely to miss the structure given equal starting conditions.

\textcolor{red}{Possible interpretation: 0.6 not allowing enough bins to accurately capture the structure, 0.7 good compromise, 0.8 becomes overly sensitive to noise}

```{r randuEx, results="hide", fig.cap="Final view when optimising the MIC index in the RANDU dataset. Each plot lists the selected value of $\\alpha$, the time required by the optimisation in seconds and the MIC index value."}
if(!file.exists("cache/randuEx.rda")){
  pL <- list()
  i <- 1
  for (alpha in c( 0.6, 0.7, 0.8)){
    set.seed(556677)
    tic.clearlog()
    tic() #start timer
    tP <- save_history(randu, guided_tour(mineIndexAlpha("MIC", alpha)),max_bases = 100000)
    toc(log=TRUE,quiet=TRUE)
    optT <- unlist(tic.log(format=FALSE))["toc.elapsed"]-unlist(tic.log(format=FALSE))["tic.elapsed"]
    imax <- length(as.list(tP))
    fV <- as.tibble(as.matrix(randu) %*% as.list(tP)[[imax]])
    colnames(fV) <- c("PP1", "PP2")
    idxV <- mine(fV$PP1, fV$PP2, alpha = alpha)[["MIC"]]
    pL[[i]] <- ggplot(fV, aes(PP1,PP2)) + geom_point() + theme(aspect.ratio=1) + 
      ggtitle(paste(alpha, toString(format(optT, digits=2)), toString(format(idxV, digits=2)), sep = ", "))
    i <- i+1
  }
  save(pL, file = "cache/randuEx.rda")
} else {
  load("cache/randuEx.rda")
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], ncol=3)
```





## Possible modifications

Big issues: smoothness, rotation invariance

### Smoothing

Focusing on scagnostics indices, for which the biggest issues with noisy transitions have been observed. Smoothing can be performed by interpolation, or averaging similar views. For example we can use jittering, or we can use tour functionalities to find a set of randomly selected projections within a small angle of the current view. In this way we can generate a number of similar views. We can then select the mean value, or to avoid large pulls from sharp peaks, we can remove the most extreme values and then calulate the mean from the remaining evaluations. Similar to the study of scagnostics binning we now compare different methods of smoothing when moving into the special view of the Spiral 1000 dataset.


```{r spiralScagSmoothing, fig.cap="Comparing the traces of the three scagnostics indices when smoothing the index values, either by averaging over the index value after jittering the projection by some angle alpha (left) or after jittering the projected datapoints with some amount alpha (right). While the behaviour can be somewhat improved, sharp jumps can be observed in all settings considered."}

jitterScagnostics <-function(proj, d, alpha){
  newProj <- tourr:::basis_nearby(proj, alpha = alpha, method = "geodesic")
  newD <- d %*% newProj
  return(as.vector(scagnostics.default(newD[,1],newD[,2])$s))
}

jitterPointsScagnostics <-function(projData, alpha){
  newD <- jitter(projData, amount=alpha)
  return(as.vector(scagnostics.default(newD[,1],newD[,2])$s))
}


getSgnMean <- function(proj, d, alpha, method){
  dProj <- d %*% proj
  orig <- as.vector(scagnostics.default(dProj[,1],dProj[,2])$s)
  if(method == "jitterAngle"){
    sgnVec <- replicate(10, jitterScagnostics(proj, d, alpha))
  } else if (method=="jitterPoints"){
    sgnVec <- replicate(10, jitterPointsScagnostics(dProj, alpha))
  }
  return(rowMeans(cbind(orig, sgnVec)))
}

getScagSmooth <- function(df, tPath){
  sc <- tibble(
    convex=numeric(),
    skinny=numeric(),
    stringy=numeric(),
    t=numeric(),
    method=character(),
    alpha=numeric())
  n <- length(tPath)
  for (method in c("jitterAngle", "jitterPoints")){
    for (alpha in c(0.01, 0.05, 0.1)){
      for (i in 1:n) {
        scagMean <- getSgnMean(tPath[[i]], as.matrix(df), alpha, method)
        convex <- scagMean[6]
        skinny <- scagMean[7]
        stringy <- scagMean[8]
        sc <- sc %>% add_row(convex=convex, skinny=skinny, stringy=stringy,
                             t=i, method=method, alpha=alpha)
      }
    }
  }
  
  return(sc)
}

if(!file.exists("cache/spiralScagSmoothing.rda")){
  scagSmooth <- getScagSmooth(spiral1000, as.list(t2full))
  scagSmoothMelt <- melt(scagSmooth, id=c("t", "method", "alpha"))
  save(scagSmooth, scagSmoothMelt, file = "cache/spiralScagSmoothing.rda")
} else {
  load("cache/spiralScagSmoothing.rda")
}

ggplot(scagSmoothMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(alpha))) + 
  facet_wrap(variable~method, ncol=2, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE))

```


### Rotation invariance
Do we want to say anything here? big issue for splines2D measure, but also scagnostics

## Summary

Our results can be summarised by evaluating and comparing the advantages and disadvanteges of each index function according to the criteria presented above. An overview is given in Table \ref{tab:summary}. The first three entries are from the scagnostics family. The advantages are good scaling of computing time with sample size and that the set of index functions covers general signatures, making the set flexible in applications (the same is however not true for each single index viewed on its own). On the other hand we have found that they are typically too noisy making optimisation of the function challenging, which also results in poor squinatbility. In particular the stringy index cannot be used with current optimisers. The index functions from @mbgraphic share similar behaviour as they are both found to evolve fairly smoothly between interpolated projections, but have been shown to be strongly rotation dependent. The splines2D index is fast to calculate and found to be the most efficient method for identifying functional dependence. We have only considered the non-binned implementation of dcor2D which is slow to calculate on large datasets. While this index is in principle general, we have found it to be too sensitive to noise to be applicable in most realistic examples, therefore we consider flexibility limited. Finally the MINE indexes are found to be smooth only for large datasets, with some rotation dependence observed for the TIC index. While these indexes are slow to compute especially on large datasets, we have found them to be the most general, allowing good discrimination even with noisy structures. In addition we observed good squinatbility for all three examples considered, given large datasets. Results in Section \ref{sec:params} suggest that tuning of the MINE parameters may result in improved behaviour for smaller datasets.

\begin{table}
\begin{center}
\caption{Summary of findings, showing to what extend the considered index functions pass the criteria for a good PPI. "\checkmark" symbols good behaviour, "$\cdot$" symbols some issues while "$\times$" symbols failure on the corresponding criteria. Each index has particular strengths and drawbacks and selection must be guided by the considered example, see text for details.}
\label{tab:summary}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Index & smooth & squintability & flexible & rotation invariant & speed \\
\hline
Convex & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & \checkmark \\
Skinny & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & \checkmark \\
Stringy & $\times$ & $\times$ & $\cdot$ & $\cdot$ & \checkmark \\
\hline
splines2D & \checkmark & \checkmark & $\cdot$ & $\times$ & \checkmark \\
dcor2D & \checkmark & \checkmark & $\cdot$ & $\times$ & $\cdot$ \\
\hline
MINE & $\cdot$ & \checkmark  & \checkmark & $\cdot$ & $\cdot$ \\
\hline
\end{tabular}
\end{center}
\end{table}




# Application to physics data
\label{sec:phys}

This section describes the application of these projection pursuit indices to find two-dimensional structure in two multidimensional gravitational waves problems. 

The first example shows 2538 posterior samples obtained by fitting source parameters to the observed gravitational wave signal GW170817 from a neutron star merger [@PhysRevLett119161101]. Data has been downloaded from @ligoData. The fitting procedures are described in detail in @Abbott2018exr. There are six parameters of physical interest (6D) with some known relationships. Projection pursuit is used to find the known relationships. 

The second example shows data from @Smith:2016qas, generated from a simulation study of a binary black hole (BBH) merger event. (XXX More info needed.) There are 12 parameters (12-D), with multiple nuisance parameters. Projection pursuit uncovers several new relationships between parameters. 

## Neutron star merger

Figure \ref{fig:neutronStarSPLOM} shows the scatterplot matrix (with transparency) of the six parameters. (In astrophysics, scatterplot matrices are often called "corner plots" [@corner].) The diagonal shows a univariate density plot of each parameter, and the upper triangle of cells displays the correlation between variables. From this it can be seen that m1 and m2 are strongly, and slightly, nonlinearly associated. Between the other variables, some linear association (R1, R2), some nonlinear association (L1, L2, R1, R2),  heteroskedastic variance in most variables and some bimodality (R1, L1, L2, m1, m2). 

The model describes a neutron star merger and contains 6 free parameters, with each neutron star described by its mass $m$ (m1, m2) and radius $R$ (R1, R2), and a so-called tidal deform ability parameter $\Lambda$ (L1, L2) which is a function of the mass and radius, approximately proportional to $(m/R)^{-5}$. 

### Data pre-pocessing

Because m1 and m2 are very strongly associated, m2 is dropped before doing PP. This relationship is obvious from the scatterplots of pairs of variables and does not need to be re-discovered by PP. 

All variables are scaled to range between 0 and 1. The purpose is that range differences in individual variables should not affect the detection  of relationships between multiple variables. Standardising the range will still leave differences between the standard deviations of the variables, and for this problem this is preferred. Differences in the standard deviations can be important for keeping the non-linear relationships visible to PP.  

### Applying PP

With only five parameters, a reasonable start is to examine the 5D space using a grand tour. This quickly shows that the strong nonlinear relationships between the parameters. PP is then used to extract these relationships. The best index for this sort of problem is the splines2d, and it is fast to compute. 

Figure \ref{fig:nsePlotOrig} shows the optimal projection found by splines2d, a reconstructed view obtained by manually combining parameters, and a plot of the known relationship between parameters. 

<!--
(left) clearly recovers functional relations between some of the parameters. From the matrix representation we read off that parameters m1, L1 and R1 are important in this projection. Using the original (non-rescaled) data and taking into account the difference in scale between m1 and R1 we can reproduce a similar picture shown in Fig \ref{fig:nsePlotOrig} (middle), which we compare to the dependence of L1 on the ratio R1/m1 (right). For this example the linear combination can thus recover the approximate relation between these parameters.
-->

<!--
 A posterior sample containing 2538 points is available @ligoData and can be used to constrain the parameters. Here we use the "Parametrized-EoS\_nomaxmass\_posterior\_samples.dat" dataset and aim to recover the dependence between the three parameters of the better constrained system out of a larger parameter set. For details about the modelling and statistical procedure see @Abbott2018exr. A common representation of posterior samples used for interpretation are so-called "corner plots", i.e. density displays for all 2-d parameter combinations as well as the 1-d profile, highlighting empirically evaluated regions of highest posterior density, which correspond to credibility regions at a given level. The information can similarly be presented in a scatter plot matrix using transparency to convey the density information. Since in the following we work with 2-d scatter plots we choose to show the data in this fashion, see Fig. \ref{fig:neutronStarSPLOM}.
-->

```{r neutronStarSPLOM, fig.height=8, fig.width=8, fig.cap="Scatter plot matrix of the neutron star dataset, darker regions represent higher marginalised posterior densities."}
nsD <- read_csv("data/samples.csv") 
ggpairs(nsD, lower=list(continuous = wrap("points", alpha = 0.05)))
```

<!-- The following features can be observed: the combination of the two mass parameters is well constrained (a consequence of how a particular combination, the "chirp mass" enters predictions), other parameters also show patterns of dependence, they are however less pronounces. We also see the non-linear relation between the radius $R_1$ and $\Lambda$ parameter $L1$ and the two-pronged disribution for the radius $R_1$.
For further study we therefore remove the second mass $m2$ from the dataset.
\textcolor{red}{Looking at the distribution in a grand tour display, using a scatter plot display combined with alpha transparency to study the density distribution in the 5-d parameter space, indicates that the data points are found on a curved surface in the five dimensional space. This stems from the non-linear relation between the parameters, which we will try to uncover using the above developed formalism for new guided tour index functions.}


### Results

Since we expect a functional relation between some of the parameters, we optimise the "splines2d" index function. Note that all parameters are standardised to the range $[0,1]$ before running the optimisation, and we do not sphere the data for ease of interpretation of the results.
--> 

```{r neutronStarEq,  results="hide"}
nsM <- nsD %>%
  select(-m2) %>% # remove m2 to avoid finding well known correlation
  rescale() # rescaling, this returns a matrix
if(!file.exists("cache/neutronStarEq.rda")){
  set.seed(2018)
  nseTour <- save_history(nsM, guided_tour(splineIndex()))
  nseTourFull <- as.list(interpolate(nseTour))
  save(nseTour, nseTourFull, file = "cache/neutronStarEq.rda")
} else {
  load("cache/neutronStarEq.rda")
}
iLast <- length(nseTourFull)
fProj <- nseTourFull[[iLast]]
dProj <- as.tibble(nsM %*% fProj)
colnames(fProj) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj <- as_tibble(fProj) %>%
  add_column(label=colnames(nsM))
```

```{r axesDrawing}
#select cntrx, cntry for placement
#sclx and scly should be 20% of overall range
getCircle <- function(cntrx, cntry, sclx, scly){
  theta <- seq(0, 2 * pi, length = 50)
  circ <- tibble(x=cos(theta), y=sin(theta))
  circ <- circ %>% 
    mutate(x=x*sclx+cntrx, y=y*scly+cntry)
}
getAxes <- function(fProj, cntrx, cntry, sclx, scly){
  x1 <- rep(cntrx, length(fProj$PP1))
  y1 <- rep(cntry, length(fProj[,1]))
  x2 <- fProj$PP1*sclx+cntrx
  y2 <- fProj$PP2*scly+cntry
  lab <- fProj$label
  axes <- tibble(x1=x1, y1=y1, x2=x2, y2=y2, label=lab)
}
```

```{r nsePlotOrig, fig.cap="Comparison of guided tour final view (left), approximation based on original parameters (middle) and expected relation based on analysis setup (right)."}
p1 <- ggplot(dProj, aes(PP1, PP2)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0,0.6,0.32,0.22), aes(x=x, y=y), color="grey") + 
  xlim(-1.1, 0.5) + ylim(-0.2, 0.9) +
  geom_segment(data=getAxes(fProj, 0,0.6,0.32,0.22), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, 0,0.6,0.32,0.22), aes(x=x2, y=y2, label=label), size=2.5)
p2 <- ggplot(nsD, aes(R1-2*pi*m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
p3 <- ggplot(nsD, aes(R1/m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3) 
```

To further investigate relationships between parameters, $L1$ is removed and PP with the splines2D is applied to the remaining four parameters. The dependence of $L2$ on the mass and radius of the lighter neutron star, is revealed (Figure \ref{fig:nseRemL1} left plot). A manual reconstruction shows this is a relationship between L2, R1, R2 and m1 (middle plot), but it is effectively the known relationship between L2, R2 and m2 (right plot) -- m2 is latently in the relationship though m1. 

```{r nseRemL1, fig.cap="Removing L1 and optimise again of the remaining parameters, where m2 remains removed from the set. Because of parameter correlations we can recover clear description of L2 as a function of the other parameters, despite m2 missing."}
nsM2 <- nsD %>%
  select(-m2, -L1) %>% # remove m2 to avoid finding well known correlation
  rescale()
if(!file.exists("cache/neutronRemL1.rda")){
  set.seed(2018)
  nseTour2 <- save_history(nsM2, guided_tour(splineIndex()))
  save(nseTour2, file = "cache/neutronRemL1.rda")
} else{
  load("cache/neutronRemL1.rda")
}
tL <- as.list(nseTour2)
iLast <- length(tL)
fProj <- tL[[iLast]]
dProj <- as.tibble(nsM2 %*% fProj)
colnames(fProj) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj <- as_tibble(fProj) %>%
  add_column(label=colnames(nsM2))
p1 <- ggplot(as.tibble(dProj), aes(PP1, PP2))+geom_point(alpha=0.1)+ theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.5,0.6,0.3,0.18), aes(x=x, y=y), color="grey") + 
  xlim(0, 1.5) + ylim(-0.1, 0.8) +
  geom_segment(data=getAxes(fProj, 0.5,0.6,0.3,0.18), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, 0.5,0.6,0.3,0.18), aes(x=x2, y=y2, label=label), size=2.5)
p2 <- ggplot(as.tibble(nsM2), aes(m1+R1+1.5*R2, L2)) + geom_point(alpha=0.1)+ theme(aspect.ratio=1)
p3 <- ggplot(nsD, aes(R2/m2, L2))+ geom_point(alpha=0.1)+ theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3)
```


## Black hole simulation

This data contains posterior samples for simulation from a model describing a binary black hole (BBH) merger event. There are twelve model parameters. Flat priors are used for most model parameters. 

Figure \ref{fig:bbhSimulation} shows a scatterplot matrix, of nine of the twelve parameters. (Parameter m2 is not shown because it is strongly linearly associated with m1, phi$\_$jl and psi are not shown because they uniform, and not associated with other parameters.) Among the nine plotted parameters, strong nonlinear relationships can be seen between the parameters ra, dec and time. The first two describe the position of the event in the sky, and time is the merging time (in GPS units). Because of the elliptical relationship between dec and time, the TIC index is used for PP, even though it is slow to compute. Between the other parameters, the main structure seen is multimodality and some skewness. These patterns are representative of the likelihood function, since most priors are flat, or built to capture growth with volume rather than distance. 

```{r bbhSimulation, fig.height=8, fig.width=8, dev = "png", dpi=300, fig.cap="Scatter plot matrix showing most of the variables included in the BBH dataset. Strong correlation between the parameters time, dec and ra can be observed."}
bbhD <- read_csv("data/posterior_samples.csv")
bbhDsmall <- select(bbhD, -phi_jl, -m2, -psi, -chi_eff)
ggpairs(bbhDsmall, lower=list(continuous = wrap("points", alpha = 0.02)))
```

### Data pre-processing

The analysis is conducted on 11 of the twelve parameters. One variable is removed, m2, because it is so strongly associated with m1. All parameters are scaled into the range 0 to 1. 

### Applying PP

#### Exploring 11D with all PP indexes

All seven PP indexes are applied to the data. Figure \ref{fig:bbhGuided} showing the projections that maximise three of the indexes. TIC and splines2D indexes identify very similar projections, that are based on the three parameters, dec, time and ra. This is ot be expected based on the pairwise scatterplots. \textcolor{red}{maybe having tour of those three dimensions would be helpful here}. On the other hand, the 1-convex index finds a very different view, but actually the problem is that the optimisation doesn't adequately reach a maximum for this index. 


```{r bbhGuided, results="hide", fig.cap="Projections corresponding to the maxima of three indices: TIC, splines2D and 1-convex. Projections a, b found by TIC and splines3D are very similar, and involve the same three parameters, ra, dec and time. The 1-convex index finds a very different view.", dev = "png", dpi=300}
bbhM <- rescale(select(bbhD,-m2, -chi_eff))
if(!file.exists("cache/bbhGuidedTIC.rda")){
  set.seed(2018)
  bbhTour1 <- save_history(bbhM, guided_tour(mineIndex("TIC")))
  bbhTourFull1 <- as.list(interpolate(bbhTour1))
  save(bbhTour1, bbhTourFull1, file = "cache/bbhGuidedTIC.rda")
} else {
  load("cache/bbhGuidedTIC.rda")
}
if(!file.exists("cache/bbhGuidedSpline.rda")){
  set.seed(2018)
  bbhTour2 <- save_history(bbhM, guided_tour(splineIndex()))
  bbhTourFull2 <- as.list(interpolate(bbhTour2))
  save(bbhTour2, bbhTourFull2, file = "cache/bbhGuidedSpline.rda")
} else {
  load("cache/bbhGuidedSpline.rda")
}
if(!file.exists("cache/bbhGuidedConvex.rda")){
  set.seed(2018)
  bbhTour3 <- save_history(bbhM, guided_tour(invConvexIndex()))
  bbhTourFull3 <- as.list(interpolate(bbhTour3))
  save(bbhTour3, bbhTourFull3, file = "cache/bbhGuidedConvex.rda")
} else {
  load("cache/bbhGuidedConvex.rda")
}

iLast <- length(bbhTourFull1)
fProj1 <- bbhTourFull1[[iLast]]
dProj1 <- as.tibble(bbhM %*% fProj1)
colnames(fProj1) <- c("PP1", "PP2")
colnames(dProj1) <- c("PP1", "PP2")
fProj1 <- as_tibble(fProj1) %>%
  add_column(label=colnames(bbhM))
p1 <- ggplot(dProj1, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.4,0.5,0.26,0.24), aes(x=x, y=y), color="grey") + 
  xlim(0.1, 1.4) + ylim(-0.3, 0.9) +
  geom_segment(data=getAxes(fProj1, 0.4,0.5,0.26,0.24), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj1, 0.4,0.5,0.26,0.24), aes(x=x2, y=y2, label=label), size=2.5) + ggtitle("a. TIC: 779.46")
iLast <- length(bbhTourFull2)
fProj2 <- bbhTourFull2[[iLast]]
dProj2 <- as.tibble(bbhM %*% fProj2)
colnames(fProj2) <- c("PP1", "PP2")
colnames(dProj2) <- c("PP1", "PP2")
fProj2 <- as_tibble(fProj2) %>%
  add_column(label=colnames(bbhM))
p2 <- ggplot(dProj2, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.3,-1,0.24,0.22), aes(x=x, y=y), color="grey") + 
  xlim(-0.1, 1.1) + ylim(-1.3, -0.2) +
  geom_segment(data=getAxes(fProj2,0.3,-1,0.24,0.22), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj2, 0.3,-1,0.24,0.22), aes(x=x2, y=y2, label=label), size=2.5) + ggtitle("b. splines2D: 0.97")
iLast <- length(bbhTourFull3)
fProj3 <- bbhTourFull3[[iLast]]
dProj3 <- as.tibble(bbhM %*% fProj3)
colnames(fProj3) <- c("PP1", "PP2")
colnames(dProj3) <- c("PP1", "PP2")
fProj3 <- as_tibble(fProj3) %>%
  add_column(label=colnames(bbhM))
p3 <- ggplot(dProj3, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.75, 1, 0.3, 0.28), aes(x=x, y=y), color="grey") + 
  xlim(-0.2, 1.3) + ylim(-0.1, 1.3) +
  geom_segment(data=getAxes(fProj3,0.75, 1, 0.3, 0.28), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj3, 0.75, 1, 0.3, 0.28), aes(x=x2, y=y2, label=label), size=2.5) + ggtitle("c. 1-convex: 0.50")
grid.arrange(p1, p2, p3, ncol=3) 
```


```{r indexTableBBH, eval=FALSE}
getIdxComp <- function(dProj, n){
  ticI <- mine(dProj$PP1, dProj$PP2)$TIC
  splineI <- splines2d(dProj$PP1, dProj$PP2)
  invConvI <- 1-scagnostics(as.matrix(dProj))[,"Convex"]
  return(tibble(TIC=ticI, splines2D=splineI, invConv=invConvI, nView=n))
}
idxT <- tibble(TIC=numeric(),
               splines2D=numeric(),
               invConv=numeric(),
               nView=numeric())
idxT <- rbind(idxT, getIdxComp(dProj1, 1),
      getIdxComp(dProj2, 2),
      getIdxComp(dProj3, 3))
colnames(idxT) <- c("TIC", "splines2D", "1-convex", "Projection")
knitr::kable(idxT,  caption = "Matrix of index values, for all three indices and all three final projections. \\label{tab:indexTableBBH}", digits=2) 
```


#### Exploring reduced space

The variables time, dec and ra are dropped from the data, and PP is applied to the remaining 8D space. Figure \ref{fig:bbhGuided2} shows the projections which maximise the TIC, splines2D and 1-convex indices. The results provide similar information as laready learned from the scatterplot matrix (Figure \ref{bbhSimulation}). The parameters chi$\_$tot and chi$\_$p are linearly related (TIC maxima), and theta$\_$jn has a bimodal distribution yielding the figure 8 shape found by the splines2D index. The 1-convex index finds nothing interesting. 


```{r bbhGuided2, results="hide", fig.cap="Projections of the reduced 8D space corresponding to the maxima of three indices: TIC, splines2D and 1-convex.", dev = "png", dpi=300}
bbhM2 <- rescale(select(bbhD,-m2, -chi_eff, -time, -dec, -ra))
if(!file.exists("cache/bbhGuidedTIC2.rda")){
  set.seed(2018)
  bbhTour12 <- save_history(bbhM2, guided_tour(mineIndex("TIC")))
  bbhTourFull12 <- as.list(interpolate(bbhTour12))
  save(bbhTour12, bbhTourFull12, file = "cache/bbhGuidedTIC2.rda")
} else {
  load("cache/bbhGuidedTIC2.rda")
}
if(!file.exists("cache/bbhGuidedSpline2.rda")){
  set.seed(2018)
  bbhTour22 <- save_history(bbhM2, guided_tour(splineIndex()))
  bbhTourFull22 <- as.list(interpolate(bbhTour22))
  save(bbhTour22, bbhTourFull22, file = "cache/bbhGuidedSpline2.rda")
} else {
  load("cache/bbhGuidedSpline2.rda")
}
if(!file.exists("cache/bbhGuidedConvex2.rda")){
  set.seed(2018)
  bbhTour32 <- save_history(bbhM2, guided_tour(invConvexIndex()))
  bbhTourFull32 <- as.list(interpolate(bbhTour32))
  save(bbhTour32, bbhTourFull32, file = "cache/bbhGuidedConvex2.rda")
} else {
  load("cache/bbhGuidedConvex2.rda")
}

iLast <- length(bbhTourFull12)
fProj1 <- bbhTourFull12[[iLast]]
dProj <- as.tibble(bbhM2 %*% fProj1)
colnames(fProj1) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj1 <- as_tibble(fProj1) %>%
  add_column(label=colnames(bbhM2))
iv <- mine(dProj$PP1,dProj$PP2)["TIC"]
p1 <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(1, 0, 0.34, 0.25), aes(x=x, y=y), color="grey") + 
  xlim(-0.2, 1.5) + ylim(-0.25, 1) +
  geom_segment(data=getAxes(fProj1,1, 0, 0.34, 0.25), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj1, 1, 0, 0.34, 0.25), aes(x=x2, y=y2, label=label), size=2.5) +
  ggtitle(paste0("a, TIC: ", toString(format(iv, digits=2))))
iLast <- length(bbhTourFull22)
fProj2 <- bbhTourFull22[[iLast]]
dProj <- as.tibble(bbhM2 %*% fProj2)
colnames(fProj2) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj2 <- as_tibble(fProj2) %>%
  add_column(label=colnames(bbhM2))
iv <- splines2d(dProj$PP1,dProj$PP2)
p2 <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(0.75, 0, 0.25, 0.24), aes(x=x, y=y), color="grey") + 
  xlim(-0.25, 1) + ylim(-0.9, 0.3) +
  geom_segment(data=getAxes(fProj2, 0.75, 0, 0.25, 0.24), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj2, 0.75, 0, 0.25, 0.24), aes(x=x2, y=y2, label=label), size=2.5) +
  ggtitle(paste0("b, splines2D: ", toString(format(iv, digits=2))))
iLast <- length(bbhTourFull32)
fProj3 <- bbhTourFull32[[iLast]]
dProj <- as.tibble(bbhM2 %*% fProj3)
colnames(fProj3) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj3 <- as_tibble(fProj3) %>%
  add_column(label=colnames(bbhM2))
iv <- 1-scagnostics(as.matrix(dProj))[,"Convex"]
p3 <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(0, 1.25, 0.32, 0.32), aes(x=x, y=y), color="grey") + 
  xlim(-0.4, 1.2) + ylim(0, 1.6) +
  geom_segment(data=getAxes(fProj3, 0, 1.25, 0.32, 0.32), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj3, 0, 1.25, 0.32, 0.32), aes(x=x2, y=y2, label=label), size=2.5) +
  ggtitle(paste0("c, 1-convex: ", toString(format(iv, digits=2))))
grid.arrange(p1, p2, p3, ncol=3) 
```

#### Effect of random starts, and subsets used

The initial conditions for the optimisation, and the subset of variables used, can have a large effect on the projections returned. We illustrate this using only the splines2D index, and find that there is one more association that can be learned that was masked earlier. 

Fig. \ref{fig:bbhGuided3} shows six maxima obtained by different starts, for two types of parameters: first, spin related parameters (i.e. alpha, theta\_jn, chi\_tot and chi\_p), and second position related parameters (i.e. ra, dec and distance). Four of the six (a-d) are almost identical, but not interesting projections. Projection f has the highest PP index value but it is primarily the view seen in the bivariate plot of dec and ra.  

Choosing a different subset of variables reveals something new. The subspace of m1, ra, chi\_tot, alpha, distance, dec produces a more refined view of Fig. \ref{fig:bbhGuided3} projection f. When alpha contributes in contrast to dec, the relationship between the points is almost perfectly on a curve. This is shown in Figure \ref{constructedExample}. Manually reconstructing this can be done by differencing the two parameters, in their original units. \textcolor{red}{This result is unexpected, it can be understood from ????? (Does it mean that knowing the precision angle gives more precise information about the location? Knowing ra uniquely determines dec-alpha rather than dec it seems.)}

```{r bbhGuided3, results="hide", fig.cap=paste("Final views identified in the dataset considering the seven dimensional parameter space (alpha, theta\\_jn, chi\\_tot, chi\\_p, ra, dec, distance), differing only by randomly selected starting plane."), dev = "png", dpi=300}
bbhM3 <- rescale(select(bbhD, alpha, theta_jn, chi_tot, chi_p, ra, dec, distance))
getResWithSeed <- function(s){
  set.seed(s)
  startM <- orthonormalise(matrix(runif(14),ncol = 2))
  tourRes <- save_history(bbhM3, guided_tour(splineIndex()))
}

if(!file.exists("cache/bbhGuidedSpline3.rda")){
  set.seed(2018)
  seedVals <- sample(1:10000, 6)
  bbhRes1 <- getResWithSeed(seedVals[1])
  bbhRes2 <- getResWithSeed(seedVals[2])
  bbhRes3 <- getResWithSeed(seedVals[3])
  bbhRes4 <- getResWithSeed(seedVals[4])
  bbhRes5 <- getResWithSeed(seedVals[5])
  bbhRes6 <- getResWithSeed(seedVals[6])
  save(bbhRes1, bbhRes2, bbhRes3, bbhRes4, bbhRes5, bbhRes6, file = "cache/bbhGuidedSpline3.rda")
} else {
  load("cache/bbhGuidedSpline3.rda")
}
resList <- list(as.list(bbhRes1), as.list(bbhRes2), as.list(bbhRes3),
                as.list(bbhRes4), as.list(bbhRes5), as.list(bbhRes6))
pList <- list()
idxV <- NULL
i <- 1
xmins <- c(-0.5,-0.5,-0.4,-0.9,-1,0.25)
xmaxs <- c(0.75,0.75,0.8,0.4,0.25,1.5)
ymins <- c(-0.8,-0.8,-0.7,-0.8,-1.2,-0.6)
ymaxs <- c(0.7,0.6,0.7,0.7,0.,0.9)
xscales <- c(0.25,0.25,0.24,0.26,0.25,0.25)
yscales <- c(0.3,0.28,0.28,0.3,0.24,0.3)
xcents <- c(0.25,-0.25,-0.1,0.1,-0.75,1.25)
ycents <- c(0.4,-0.4,-0.4,-0.4,-0.75,0.5)
titleL <- c("a: ","b: ","c: ","d: ","f: ","e: ")
for (r in resList){
  iLast <- length(r)
  fProj <- r[[iLast]]
  dProj <- as.tibble(bbhM3 %*% fProj)
  colnames(fProj) <- c("PP1", "PP2")
  colnames(dProj) <- c("PP1", "PP2")
  fProj <- as_tibble(fProj) %>%
    add_column(label=colnames(bbhM3))
  idxV <- splines2d(dProj$PP1, dProj$PP2)
  pList[[i]] <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(xcents[i],ycents[i],xscales[i],yscales[i]), aes(x=x, y=y), color="grey") + 
  xlim(xmins[i], xmaxs[i]) + ylim(ymins[i], ymaxs[i]) +
  geom_segment(data=getAxes(fProj, xcents[i],ycents[i],xscales[i],yscales[i]), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, xcents[i],ycents[i],xscales[i],yscales[i]), aes(x=x2, y=y2, label=label), size=2.5) +
    ggtitle(paste0(titleL[i], toString(format(idxV, digits=2))))
  i <- i+1
}
grid.arrange(pList[[1]], pList[[2]], pList[[3]],pList[[4]], pList[[6]], pList[[5]], ncol=3)
```

```{r constructedExample, results="hide", fig.cap="Selecting subset of 6 parameters, for which spline index can be used to improve constraint on parameter combination wrt ra vs dec.", dev = "png", dpi=300}
bbhC <- rescale(select(bbhD, m1, ra, chi_tot, alpha, distance, dec))
if(!file.exists("cache/constructedExample.rda")){
  set.seed(1999)
  tC <- save_history(bbhC, guided_tour(splineIndex()))
  save(tC, file = "cache/constructedExample.rda")
} else {
  load("cache/constructedExample.rda")
}
iL <- length(as.list(tC))
dProj <- bbhC %*% as.list(tC)[[iL]]
fProj <- as.list(tC)[[iL]]
colnames(fProj) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj <- as_tibble(fProj) %>%
  add_column(label=colnames(bbhC))
p1 <- ggplot(as.tibble(dProj), aes(PP1, PP2))+geom_point(alpha=0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(-0.7, 0.6, 0.2, 0.24), aes(x=x, y=y), color="grey") + 
  xlim(-0.9, 0.1) + ylim(-0.1, 1.1) +
  geom_segment(data=getAxes(fProj, -0.7, 0.6, 0.2, 0.24), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, -0.7, 0.6, 0.2, 0.24), aes(x=x2, y=y2, label=label), size=2.5)
p2 <- ggplot(as.tibble(bbhD),aes(dec-alpha, ra)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)
p3 <- ggplot(as.tibble(bbhD),aes(dec, ra)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3)
```

<!--

# Applications to collections of time series 

This could be another example.

```{r eval=FALSE}
music <- read_csv("data/tigs_music.csv")
music <- apply(music, 2, function(x) (x-mean(x))/sd(x))
musicTour <- save_history(music,
                         guided_tour(mineIndex("TIC"), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTour <- save_history(music,
                         guided_tour(dcorIndex(), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTourFull <- as.list(interpolate(musicTour))
iLast <- length(musicTourFull)
fProj <- musicTourFull[[iLast]]
dProj <- as.tibble(as.matrix(music) %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
fullRes <- getProj(as.matrix(music), musicTourFull, "TIC", 100)
fullResMelt <- melt(fullRes, id=c("t", "name", "size"))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```

-->

# Discussion



# References
