---
title: An investigation into using scagnostics to find interesting projections of high-dimensional data

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Ursula Laa
  thanks: The authors gratefully acknowledge the support of the Australian Research Council
  affiliation: Department of Physics, Monash University
  email: \email{ursula.laa@monash.edu}
  
- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
- name: Heike Hofmann
  affiliation: Department of Statistics, Iowa State University
  
- name: Hadley Wickham
  affiliation: RStudio
  
- name: Antony Unwin
  affiliation: Department of Mathematics, Augsburg University
  
- name: Katrin Grimm
  affiliation: ???

keywords:
- tour
- projection pursuit 
- statistical graphics
- data visualisation
- exploratory data analysis
- high-dimensional data

abstract: |
  Projection pursuit describes a procedure for searching high-dimensional data for "interesting" low-dimensional projections via the optimization of a criterion function called the projection pursuit index. Most indexes developed focus on finding projections with cluster structure, outliers, or separations between known groups. Here, we are interested in finding projections revealing potentially complex relations between combinations of parameters, by combining the notion of projection pursuit with index functions developed for the detection of interesting bivariate views in the data, including scagnostics and maximum information coefficient.

bibliography: bibliography.bib
output: rticles::asa_article
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{amssymb}
---

```{r initial, echo = FALSE, include = FALSE}
library(knitr)
opts_chunk$set(
  warning = FALSE, message = FALSE, echo = FALSE, 
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.show = 'hold', cache = FALSE, external = TRUE, dev = "pdf",
  fig.height = 5, fig.width = 8, out.width = "\\textwidth"
)
```

# Introduction

<!-- 
Main point of paper:

- Some metrics for exploring high-dimensional data by finding interesting pairs of variables, can be used to find interesting projections, but some have problems. 
- What are the ways to evaluate.
- What adjustments can be done.
- How it can be applied

Organisation of this section:

- Motivation first?
- PP
- Scagnostics
- section needs shortening
-->

The term "projection pursuit" was coined by @ff74 to describe a procedure for searching high (say $p-$)dimensional data for "interesting" low-dimensional projections ($k=1$ or $2$ usually). The procedure, originally suggested by @kr69, involves defining a criterion function, or index, that measures the "interestingness" of each $k$-dimensional projection of $p$-dimensional data. This criterion function is optimized over the space of all $k$-dimensional projections of $p$-space, searching for both global and local maxima. It is hoped that the resulting solutions reveal low-dimensional structure in the data not found by methods such as principal component analysis.

A large number of projection pursuit indices have been developed, to detect departure from multivariate normal, which includes clusters or outliers or separations between known groups (e.g. @f87, @CBC92, @lckl2005, @AHC02, @CEM:CEM2568, @JS87, @5508437).  Less work has been done on indexes to find nonlinear dependence between variables. 

<!-- check Perisic and Posse (2012) and recent paper by Austrian guys -->


A related topic is variable selection. With high-dimensional data, even plotting all pairs can be daunting, so "scagnostics" [@scag] [@WW08] have been developed to find the most interesting pairs of variables. There are 8 scagnostics, vividly named: "Outlying", "Skewed", "Sparse", "Clumpy", "Striated", "Convex", "Skinny" and "Stringy". Our question is whether these can be extended into projection pursuit indexes that can be used to find two-dimensional projections of high-dimensional data with unusual features. 

Our motivation is based on applications in physics, to aid the interpretation of model fits on experimental results. Consider a physical model with a set of $m$ free parameters, that cannot be measured directly and are determined by fitting a set of $p$ experimental observations, for which predictions can be calculated when fixing all $m$ parameters. Note here, that while we often have analytic expressions for the predictions, this is not always the case and we may have to rely on numerical computation. Moreover, a single prediction can be a complicated function of all free parameters. In addition, we often deal with large number of observations, $p \sim 100-1000$ as well as sizable parameter spaces $m \sim 10$. The results of a fit are generally interpreted using combinations of variables selected by intuition and prior knowledge. But this begs the question, whether important associations are missed because tools to search are not available.

<!--
suggest that some correlations clearly dictated by the data may be missed, as we are typically only studying selected views of the results. The idea of finding special views in scatter plots fits naturally with the concept of using posterior samples obtained in a Bayesian fit.

The idea is that by finding special views of the posterior samples we gain insight into the physics relevant to accurately describe the experimental data. Apart from improved intuition, finding special regions in parameter space is important as it allows us to further scrutinize the model. There are many examples of this, for example in particle physics detailed studies have been performed e.g. in the alignment limit of the 2HDM or in co-annihilation regions in models of dark matter.-->

[@Grimm2016] explored the behaviour of scagnostics for selecting variables, and proposed two more, based on smoothing splines that have some nicer properties. In addition the availability of another two indices based on information criteria, maximal and total information coefficient (MIC and TIC) [@Reshef1518] round out our collection of metrics. \footnote{It is interesting to note that entropy, closely related to the notion of mutual information, has previously been considered as a projection pursuit index when searching for interesting one-dimensional projections, see \cite{nason1992}, \cite{NIPS1997_1408}.}
These indices are all available in R packages [@rref]: scagnostics [@HWscagR], mbgraphic [@mbgraphic] and minerva [@minerva]. The projection pursuit guided tour is available in the R package, tourr [@tourr].

<!-- 
A related question is the selection of pair-wise scatter plots in datasets with large numbers of parameters, for which different methods have been developed. One approach are scatterplot diagnostics, "scagnostics", developed by @scag, and explored in @WW08. \footnote{Scagnostics are available in two R packages @LWscagR and @HWscagR. This work is built upon the scagnostics package by @HWscagR because the underlying base is built using C, works across platforms.} They are calculated from a set of elementary graph-theoretic building blocks: the convex hull, the alpha hull and the minimal spanning tree (MST). These are used to define eight measures: Outlying, Skewed, Sparse, Clumpy, Striated, Convex, Skinny and Stringy. While in principle each measure could be used on its own to define a ranking, the original idea suggested considering pair-wise scatter plots of the scagnostics measures for identification of "characteristic" and "unusual" 2-d views in the original parameter space. Alternative methods suggested by (Katrins Thesis or maybe her R package?) suggest using parallel coordinates, glyphs or a so-called scagramm (based on the idea of corrgrams) that should be coupled to interactive brushing for selection of pair-wise scatter plots to be displayed.
(moved scagnostics definitions to appendix, not sure if needed at all.)
(Issues with scagnostics already for selection of pairs have been described in Katrins Thesis, should have short summary of that here?)

Given the various shortcomings of the scagnostics approach we consider several other measures that can directly be interpreted as ranking of information contained in a given 2-d view.
Two such additional measures have been defined in @mbgraphic. The first is based on smoothing splines, and is defined such that it will take maximum values when the data distribution can be described by a functional form. For a scatterplot with variable X and Y we calculate it as
\begin{equation}
splines_{2d} = max(1-\frac{Var(res_{X\sim Y})}{Var(X)}, 1-\frac{Var(res_{Y\sim X})}{Var(Y)})
\end{equation}
where $Var(.)$ is the variance and $res_{X\sim Y}$ are the residuals in the spline model where $X$ is considered a function of $Y$.
The second new measure introduced in @mbgraphic is aiming to detect also non-functional relations and is based on the idea of distance correlation [@szekely2007]. (need to add definition)
Finally we will also consider the maximal and total information coefficient (MIC and TIC) defined in @Reshef1518 which are based on mutual information found when imposing grids on pair-wise variable plots. These functions are available in R via the interface implemented in the minerva package [@minerva]. 
(read paper and add more detail. how are they similar/different compared to scagnostics and distance correlation? the way that binning is being built into it may make difference in how MIC/TIC may be smoother?)


A projection pursuit index, a function of all possible projections of the data, invariably has many "hills and valleys" and "knife-edge ridges" because of the varying shapes in the underlying density of observations from one projection to the next. From an exploratory data analysis perspective it is interesting to combine numerical optimisation with visualisation to watch the structure of the data moving into a maximum, to jump away from this projection and follow the optimisation again, invariably moving from local maxima to local maxima, and perhaps even a global maximum. Each of these maxima can reveal different information about the data. Projection pursuit optimisation combined with geodesic interpolations between projections provided by a tour [@As85], is called a projection pursuit guided tour [@CBCH94]. Software to run a projection pursuit guided tour is available in the R package, "tourr" [@tourr].
(Motivation strongly relates to intuition, i.e. guided tour, local tour and manual tour may be useful to understand structure in the multivariate context.)
(Even just for the optimisation part the guided tour implementation is useful.)
(Traditional projection pursuit indices are fast to compute and therefore the optimisation would be done in "real time" while displaying the tour. For the measures considered here this generally does not apply, as a consequence we "disentangle" the two into subsequent steps.)
-->

The paper investigates the behaviour of these newly-defined indexes. Section \ref{sec:construct} discusses index construction, and how they fit into the guided tour. Section \ref{sec:investigate} investigates the behaviour of the indexes, particularly in relation to optimisation. The new guided tour methods are applied to an example where the posterior distributions from physics models (Section \ref{sec:phys}) are explored.

# Constructing a projection pursuit index
\label{sec:construct}

<!--
Explain here how to construct the pp index

- Definition of pp index
- Optimisation
- Illustration of what is already available in tourr
- New index explanations
-->
A projection pursuit index (ppi) is a scalar function $f$ defined on an $n$-dimensional data distribution. Typically the definition is such that larger values of $f$ indicate more interesting distribution, and therefore maximising $f$ over all possible $n$ dimensional projections of a data set with $p>n$ parameters will find the most interesting projections. As most indices characterise deviations from a normal distribution, one would first map the empiric data distribution onto a density for which such deviations may be defined in different ways, see e.g. @CBC93. When departing from the idea of characterising differences to the normal distribution it may however be preferred to work directly with the empirical data distribution to better capture the true distribution, and instead characterise the distribution by measures based on e.g. the minimal spanning tree or the mutual information.

## Standardization
Lower dimensional projections are highly sensitive to standardization performed on the original distribution. We want to avoid e.g.  highlighting directions based only on different scales, or artifically extending the parameter range by including outliers, which may result in seemingly linear behaviour for most points. Methods to consider include rescaling of individual directions to a common interval (e.g. to fall between $[0,1]$), sphering, outlier removal or transformation to a logarithmic scale. The method(s) of choice should be informed by the distributions found in the data set as well as aims of the analysis. ((particular challenges: noise only directions, identification of outliers?))

## Optimization
Given a ppi we are confronted with the task of finding the maximum over all possible $n$ dimensional projections. An important challenge is to avoid getting trapped in local maxima that are only a result of sampling fluctuatins or a consequence of noisy index functions, and @f87 suggested a two-step procedure: the first step is using a large step size to find the approximate global maximum while stepping over pseudomaxima. A second step is then starting from the projection corresponding to the approximate maximum and employing a gradient directed optimisation for the identification of the maximum. On the other hand the global maximum may not be the only projection of interest, and one may want to observe the selected views in the context of the full distribution rather than a static view. These issues are addressed when combining projection pursuit with the grand tour [@CBCH94]. In this case the properties of a suitable optimization algorithm include monotonicity of the index value, a variable step-size to avoid overshoothing and to increase the chance of reaching the nearest maximum, and a stopping criterion allowing to move out of a local maximum and into a new search region [@tourr]. A possible alogrithm is inspired by simulated annealing and has been described in @lckl2005, this has been implemented in the \texttt{search\_better} and \texttt{search\_better\_random} search functions in the tourr package. In addition the tourr package provides the \texttt{search\_geodesic} search function, which first selects a promising direction by comparing index values between a selected number of small random steps, and then optimises the function over the line along the geodesic in that direction considering projections up to $\pi/4$ away from the current view.

## New index functions
\label{sec:indexDef}
Focusing now on $n=2$ dimensional projections, we aim to identify projections that reveal interesting structures or correlations between combinations of variables. We therefore draw on index functions that have been developped for the selection of variable pairs that show interesting features in a scatter plot, but we generalise the notion such that instead of variable pairs we consider any $2$ dimensional projection of the multivariate data distribution. It means that we consider the projection axes as input to the index functions, which will thus score the interestingness of the given projection. What we are optimising over are then the entries of the $p\times2$ dimensional projection matrix, considering that they obey orthonormality conditions.
Concretely here we consider the following types of index functions:

* From the scagnostics family select those best suited to detect shapes:
    + Convex: The convex measure is defined as $c_{convex}= \frac{area(A)}{area(H)}$ where $A$ is the alpha hull and $H$ the convex hull. This is the only measure where interesting projections will take low values and one may either minimize the index function or maximize $1-c_{convex}$.
    + Skinny: The skinnyness of a polygon can be measured as the ratio of the perimeter to the area. We define $c_{skinny} = 1 -  \frac{\sqrt{4\pi area(A)}}{perimeter(A)}$, where the normalisation is chosen such that $c_{skinny} = 0$ for a full circle, and values close to one or skinny polygons.
    + Stringy: The stringy index is a measure of the branching structure of the minimal spanning tree (MST), $c_{stringy} = \frac{diameter(MST)}{length(MST)}$ where the diameter is the longest connected path, and the length is the total length (sum of all edges), i.e. $c_{stringy}=1$ if the MST contains no branches.

* New index functions available in @mbgraphic:
    + Distance correlation defined in @szekely2007: The definition is based on empiric measures of distance covariance and variance defined on the dataset 
    \begin{equation}
(X,Y) = \{(x_1,y_1), ..., (x_n, y_n) \in \mathbb{R}^p \times \mathbb{R}^q\}
\end{equation}
 as
 \begin{equation}
 Cov^2(X,Y) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}B_{kl},\\
 Var^2(X) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}^2
 \end{equation}
 where $A_{kl}=a_{kl}-\bar{a}_{k.}-\bar{a}_{l.}+\bar{a}_{..}$ with
 \begin{equation}
 a_{kl} = | x_k - x_l|_p,\\
 \bar{a}_{k.} = \frac{1}{n} \sum_{l=1}^n a_{kl},\\
  \bar{a}_{.l} = \frac{1}{n} \sum_{k=1}^n a_{kl},\\
 \bar{a}_{..} = \frac{1}{n^2}\sum_{k,l=1}^n a_{kl}
 \end{equation}
 and reads
 \begin{equation}
 dCor(X,Y) = \begin{cases}
 \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} & Var(X)Var(Y) > 0\\
 0 & Var(X)Var(Y) = 0
 \end{cases}
 \end{equation}
    + Spline based measure: Given a spline model of the data distribution that is obtained via the R implentation in the \texttt{gam} function in the mgcv R package [@w16], we define the index based on the variance of the residuals:
    \begin{equation}
    splines_{2d} = max(1- \frac{Var(res_{X\sim Y})}{Var(X)}, 1-\frac{Var(res_{Y\sim X})}{Var(Y)}),
    \end{equation}
    which takes large values if the distribution is well described by the spline model, indicating functional dependence.
    
* Information based index functions from @Reshef1518 are based on the mutual information $I$ when considering the distribution induced by the data points $D \subset \mathbb{R}^2$ on an $x-$by$y$ grid $G$. With $I^{\ast} (D, x, y) = \max I (D|_G)$ the characteristic matrix $M$ is defined as 
\begin{equation}
M(D)_{x,y} = \frac{I^{\ast} (D, x, y)}{\log \min{x,y}}.
\end{equation}
We can now define the following metrics:
    + Maximum Information Coefficient (MIC):
    \begin{equation}
    MIC(D) = \max_{xy<B(n)}\{M(D)_{x,y}\},
    \end{equation}
    where by default $B(n) = n^{0.6}$
    + Total Information Coefficient (TIC), see @JMLRv1715308:
    \begin{equation}
    TIC(D) = \sum_{xy<B(n)} M(D)_{x,y}
    \end{equation}
  While $MIC$ is normalised to fall between $[0,1]$, the upper bound of $TIC$ depends on the sample size via the bounding function $B(n)$. $TIC$ was designed to be more stable test against null hypothesis of independence, using the full information of the characteristic matrix rather than just the maximum, and as such is expected to be better suited as a ppi.
  
((What is upper bound of TIC as function of sample size? If we define $n' = n^{0.6}$, rounded to integer values only, can we calculate it as $TIC_{max} = 2 \sum_{i=1}^{n'} \frac{n'}{i}$, keeping in mind that each entry of the characteristic matrix should be between [0,1]. Each entry in the sum should be rounded also, only integer number of bins enter actual calculation, so this should be approximation only.))

((Notes on holes/cmass indices)):

Holes index in $d=2$ dimensional projections, we have projected data given by 
\begin{equation}
m = \begin{pmatrix}
x_1 & y_1 \\
x_2 & y_2 \\
\vdots & \vdots \\
x_n & y_n
\end{pmatrix}
\end{equation}
In tourr calculate holes index as $num/den$ where for $d=2$ we have
\begin{equation}
num = 1 - \frac{1}{n} \sum_{i=1}^n e^{(x_i^2+y_i^2)/2}
\end{equation}
and $den$ is fixed for fixed number of dimensions, here $den = 1 - \frac{1}{e}$. Considering now the case where all $x_i, y_i \leq 1$ we can see that points at the edge may lead to larger values than points on a circle, $x_i^2+y_i^2 = 1$. Note also that we cannot give generic upper limit on the index values, it will depend on the range, in particular values $>1$ also possible. Cmass calculated as $1-holes$, will take negative values if $holes>1$, e.g. the case for entries $x_i, y_i > 1$, example: 

```{r load}
library(tourr)
library(tidyverse)
library(reshape2)
library(scagnostics)
library(gridExtra)
library(tictoc) #timer
library(mbgraphic) #Katrins package
library(GGally)
library(geozoo)
library(minerva) #MINE indices
```


```{r sillyExample, echo=TRUE}

mX <- matrix(c(2,2,2,2), nrow=2)
holes(mX)
cmass(mX)
# compare random points on circle to extreme positions
mC <- matrix(c(cos(1.2*pi), sin(1.2*pi), cos(pi/3), sin(pi/3), cos(0.6*pi), sin(0.6*pi)), ncol = 2)
mT <- matrix(c(1,0,0,1,1,1), ncol=2)
holes(mC)
holes(mT)
```

# Investigation of indices
\label{sec:investigate}
We now examine the indices listed in Section \ref{sec:indexDef} to test which of them are suitable as projection pursuit measures, and how we may modify them for improved performance. We start by considering the relevant criteria:

  - smoothness: the index function should evolve smoothly between nearby projections, i.e. on an interpolated path we expect to observe slow and consistent change in the index function. On the other hand if two similar projections produce very different index values, i.e. we observe sharp jumps when looking at the evolution of the index function, and in particular if those fluctuations dominate the overall evolution, the index is not suitable for projection pursuit.
  - speed: the computation time is an important factor in particular when the guided tour is used to view the optimisation in real time. Note that longer computation times may be acceptable when first recording the optimisation and replaying the guided tour once the maximum has been identified.
  - ability to find fine and broad structure: We consider a broad structure one that is roughly observable over a large angle around the optimal projection, it should in general be easy to identify and a good ppi should show smooth increase over a large number of projections as we are approaching the optimal view. On the other hand fine structures are difficult to identify as they are only visible close to the optimal view. This poses problems in particular for the optimization, but we may use index definitions to amplify larger index values while suppressing smaller ones, such that the optimization will be less sensitive to noise.
  - distribution of index values: the distribution of index values when studying data points without structure should have a low variance, and be clearly separated from index values obtained when looking at structured data distributions.
  
##Simulation study
To test the behaviour of the index functions we use three types of simulated data sets based on different underlying distributions, and with different built-in features. They are defined in $n=6$ dimensions with $p$ randomly selected points $x_i, i = 1,..,n$ according to:

1. $p$ points on an n-dimensional sphere randomly selected using the geozoo package [@geozoo], as an example of dataset without special views, labelled "sphere"
2. $p$ points where $n-1$ points are independently drawn from a uniform distribution between $[-1,1]$, and enforcing $x_n^2 + x_{n-1}^2 = 1 \pm 0.1$ by rejection sampling as an example of a dataset with special view that is not described by functional dependence, and where sampling boundaries are apparent in the distribution, labelled "pipe"
3. $p$ points where $n-1$ points are independently drawn from a normal distribution with mean 0 and variance 1, and with $x_n = \sin(x_{n-1}) + \mathrm{jittering}$ as an example dataset with special view described by a function, labelled "sine"

For each distribution we consider a small sample of $p=100$ and a large sample with $p=1000$.

To understand the potential for the various indices defined we use the following overview procedures on the simulated data sets:

- index values for pair of variables, explicitly comparing the uninformative and special view of each dataset, as well as discussing the dependence on the sample size
- tour between pairs of variables, considering a planned tour that moves from the $x_1 - x_2$ view either to the $x_3 - x_4$ view (no special structure encountered over the full path) or to the $x_5 - x_6$ (moving from uninformed to special view)
- optimisation path, starting from $x_1 - x_2$, how can we use the available index functions and optimisation algorithms to identify the special view
  
###Dataset overview


```{r util}
#defining index functions to be used with the tour
scagIndex <- function(scagType){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(sR[scagType])
  }
}

splineIndex <- function(){
  function(mat){
    return(splines2d(mat[,1], mat[,2]))
  }
}

dcorIndex <- function(){
  function(mat){
    return(dcor2d(mat[,1], mat[,2]))
  }
}

mineIndex <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2])[[mineIndex]])
  }
}
```


```{r datasetFunctions}
sphereData <- function(n, p){
  dRet <- geozoo::sphere.solid.random(n,p)
  return(as.tibble(dRet$points))
}

pipeData <- function(n, p){
  i <- 1
  dRet <- NULL
  while(i <= p){
    v <- runif(n, -1, 1)
    if(abs(v[n-1]*v[n-1] + v[n]*v[n] - 1) < 0.1){
      dRet <- rbind(dRet, v)
      i <- i+1
    }
  }
  return(as.tibble(dRet))
}

sinData <- function(n, p){
  vName <- paste0("V",n)
  vNameM1 <- paste0("V",n-1)
  expr <- paste0(vName,"=sin(",vNameM1,")") # need string expression if I want to use tibble here
  dRet <- as.tibble(matrix(rnorm((n-1)*p), ncol=(n-1))) #generate normal distributed n-1 dim data
  dRet <- mutate_(dRet, expr) #string evaluation calculates var(n) as tan(var(n-1))
  colnames(dRet)[n] <- vName #correct name of new variable
  dRet[vName] <- jitter(dRet[[vName]]) #adding noise
  return(dRet)
}
```

<!-- I think you need to standardise the scale of the data -->

```{r datasets}
set.seed(1984)
sphere100 <- sphereData(6, 100) %>% rescale() %>% as_tibble()
sphere1000 <- sphereData(6, 1000) %>% rescale() %>% as_tibble()

pipe100 <- pipeData(6, 100) %>% rescale() %>% as_tibble()
pipe1000 <- pipeData(6,1000) %>% rescale() %>% as_tibble()

sin100 <- sinData(6, 100) %>% rescale() %>% as_tibble()
sin1000 <- sinData(6, 1000) %>% rescale() %>% as_tibble()
```

To get an overview of the dataset we show views of the first two parameters showing the independent distribution and the last two parameters showing the special view. The enforced relationships will also affect the 1-d densities as can be seen from \ref{fig:dataPlotsDensity}, which means that views including sizable fractions of V6 (and V5 for the Pipe distribution) will be distinct from uninformed views of distributions 2 and 3.

<!-- 
Table and figure captions need to have three components: (1) what is the plot about, (2) specific details of plot, like what type of display and how variables are mapped, (3) the most important thing that the reader should learn.

-->

```{r dataPlotsV1V2, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the first two parameters V1 and V2, i.e. showing a typical uninformed view for each dataset."}
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V1, V2)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

```{r dataPlotsV5V6, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the last two parameters V5 and V6, i.e. showing the special view built into the Pipe and Sine datasets."}
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V5, V6)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

<!--
its possible to do a single figure legend - need to look up code for this
-->

```{r dataPlotsDensity, fig.width=6, fig.height=7, fig.cap="One dimensional density distribution for all parameters and all data sets considered. Apart from statistical fluctuations we see that for the Pipe datasets the parameters V5 and V6 are pushed towards more extreme values by vetoing points away from the circle outline, and for the Sine dataset the parameter V6 follows a very distinct distribution. Note also that the first five paramters of the Sine dataset appear to be drawn from different normal distributions, which is a result of the rescaling, shifting the modes to different values for each direction."}
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  distData <- melt(ds)
  pC <- ggplot(distData, aes(value, color=variable)) +
    geom_density() +
    ggtitle(pLabels[i])
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

We next compare the various index values for three views: V1 vs V2, V5 vs V6 and V1 vs V6 (capturing differences from the different distribution in V6). For the Sphere distribution by construction we see only random fluctuations between the different views, which can be sizable in particular for small datasets. Considering next the Pipe distribution, we see that skinny, stringy and dcor2d are increased for the special view, but in particular MIC and TIC index are much larger and may be used to detect this type of structure in the dataset. Moreover, minimising the convex index (or maximising 1-convex) should also allow us to detect the special view. Finally the Sine distribution shows that several of the indices are maximised in the special view, in particular also the splines2d index designed for the detection of fictional dependence.

```{r indexTable, echo=FALSE}
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
indexOverview <- tibble(convex=numeric(),
               skinny=numeric(),
               stringy=numeric(),
               dcor2d=numeric(),
               splines2d=numeric(),
               MIC=numeric(),
               TIC=numeric(),
               name=character(),
               vars=character())
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  ds <- as.tibble(rescale(ds))
  for(vars in list(c("V1","V2"),c("V5","V6"),c("V1","V6"))){
    dprj <- as.matrix(select_(ds, vars[1], vars[2]))
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    indexOverview <- add_row(indexOverview, convex=scagRes[,"Convex"],
                    skinny=scagRes[,"Skinny"], stringy=scagRes[,"Stringy"],
                    dcor2d=dcorRes, splines2d=splineRes,
                    MIC=mineRes$MIC, TIC=mineRes$TIC,
                    name=pLabels[i], vars=paste0(vars[1],"-",vars[2]))
  }
  i <- i+1
}
knitr::kable(indexOverview,  caption = "Summary of index values.", digits=2) 
```



## Smoothness
We first study the smoothness of each measure calculated on a sequence of 2-d projections obtained via an interpolated planned tour path moving between the projection onto V1-V2 to the projection onto V3-V4 and using the default interpolation angle of 0.05. Each time step refers to an interpolation step. Since the value of the TIC index is strongly dependent on the input distribution sample size we have normalised the TIC measure by its maximum for each individual dataset, all other measures are reproduced as reported by the calculation. The results are shown in Figure \ref{fig:plotV1V2toV3V4}. The standard index functions "holes" and "cmass" show the desired behaviour in that the change is smooth over interpolated views ((it is however curious to see the actual evolution, which seems not to depend on the underlying random distributions, but is clearely evolving over the path of the planned tour)). Considering now the indices from the scagnostics family, we first note stronge dependence on the sample size. The convex index is largely sensitive to random fluctuations when considering small samples (p=100), while being mostly flat for larger samples, especially in the case of an underlying uniform distribution. The skinny index on the other hand is systematically larger for the smaller samples, while stringy appears not to be sensitive to the sample size, but resulting in large fluctuations and sharp jumps for all datasets. Distance correlation and the spline based index show similar behaviour, i.e. when considering large enough data samples (p=1000) the values are consistently close to zero, as expected for a distribution without structure. On the other hand somewhat larger values (but still far from the maximum value of 1) are found when considering small datasets, where distributions are however mostly smooth, with few kinks depending on index function and underlying distribution. Finally the information based metrics also show systematic differences based on the sample size, with sampling fluctuations being significant for small datasets. Note here that fluctuations present in Figure \ref{fig:plotV1V2toV3V4} may be amplified, as maximum values for most index functions are far from the theoretic maximum.

<!-- index values shouldn't be affected by sample size, on average it should be
same value. Maybe some need to be normalised by n. Something needs fixing here.

can we get "theoretical" min/max values, to assess the scale of the index for any particular data set.
-->

```{r getProj}
getProj <- function(df, tPath, nameStr, size){
  sc <- tibble(
    holes=numeric(),
    cmass=numeric(),
    convex=numeric(),
    skinny=numeric(),
    stringy=numeric(),
    dcor2d=numeric(),
    splines2d=numeric(),
    MIC=numeric(),
    TIC1=numeric(),
    t=numeric(),
    name=character(),
    size=numeric())
  n <- length(tPath)
  df <- rescale(df)
  for (i in 1:n) {
    dprj <- df %*% tPath[[i]]
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    cmassRes <- cmass(dprj)
    sc <- add_row(sc, holes=holesRes, cmass=cmassRes,
                  convex=scagRes[,"Convex"], skinny=scagRes[,"Skinny"],
                  stringy=scagRes[,"Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC1=mineRes$TIC, t=i, name=nameStr, size=size)
  }
  maxTIC <- max(sc$TIC1)
  sc <- sc %>%
    mutate(TIC = TIC1/maxTIC) %>%
    select(-TIC1)
  
  return(sc)
}
```

```{r V1V2toV3V4}
if(!file.exists("cache/V1V2toV3V4.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,1,0,0,0,0,0,0,1,0,0),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t1 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t1full <- as.list(interpolate(t1))
  fullRes <- getProj(sin100, t1full, "sin", 100) %>%
    rbind(getProj(sin1000, t1full, "sin", 1000)) %>%
    rbind(getProj(sphere100, t1full, "sphere", 100)) %>%
    rbind(getProj(sphere1000, t1full, "sphere", 1000)) %>%
    rbind(getProj(pipe100, t1full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t1full, "pipe", 1000))
  save(t1, t1full, fullRes, file = "cache/V1V2toV3V4.rda")
} else {
  load("cache/V1V2toV3V4.rda")
}
```

```{r plotV1V2toV3V4, fig.width=6, fig.height=7, fig.cap="Short tour paths of uninformed views, moving from the projection onto V1-V2 to the projection onto V3-V4, for all considered datasets and index functions. Small samples (p=100) are shown in red, large samples (p=1000) in blue."}
fullResMelt <- melt(fullRes, id=c("t", "name", "size"))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```


## Transition
We repeat the same exercise, but now we are interested in how the index values change when going from the view in V1 vs V2 to the view in V5 vs V6, which will also allow us to view the picture between typical low and high values of the index function. Results are shown in Figure \ref{fig:plannedtour}.
We can see a pretty clear and smooth increase in dcor2d and splines2d as well as MIC and TIC index when approaching the special view of the Sine distribution, which should therefore be efficient to optimise. On the other hand while we also see smooth increase in MIC and TIC when approaching the special view of the Pipe distribution, the increase only becomes apparent much closer to the final projection, making this structure more challenging to detect with a guided tour.
Note that the various indices appear less noisy than for the grand tour shown before, but should note that the tour path here is shorter, and the y-axis scale is different (typically reaching higher index values in this example, whereas smaller fluctuations are more amplified in the previous views).
(What does this mean for optimisation? How can I detect such "smaller scale increase"? Maybe try "search_better" instead of search_geodesic? increase alpha and decrease cooling? increase max.tries?)
Can force it when using search_better with large alpha, cooling=1, max.tries=1000, but not ideal..
Difficulties also from "fake" structure from the sampling boundaries?
Possible issue with search_geodesic is that the search window is fixed in the code..

TIC was suggested in @JMLRv1715308 as an alternative to MIC with better properties for testing against independence, this is because the full information of the characteristics is used, implying that more information is considered. We also see that TIC is smoother when moving into the special view.
(As I understand TIC is however not equitable, i.e. it will prefer linear relations, and it may be instructive to optimise MIC and TIC)

FIXME need to check the MIC_e and TIC_e, supposedly faster computation and better discrimination!

Here we also see how the existing indices in the tourr package are inadequate for the detection of these type of dependence, the holes index taking maximum values before the final view and decreasing when moving into the special view. On the other hand, surprisingly the cmass index increases when moving into the special view, but the intermediate minimum makes it unsuitable as a projection pursuit index here. QUESTION: how come they are all almost the same for the various distributions? Especially pipe and sphere seem to give almost identical behaviour, can it be understood from the index definition?



```{r plannedtourpath}
if(!file.exists("cache/plannedtourpath.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,0,0,1,0,0,0,0,0,0,1),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t2 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t2full <- as.list(interpolate(t2))
  fullResPlanned <- getProj(sin100, t2full, "sin", 100) %>%
    rbind(getProj(sin1000, t2full, "sin", 1000)) %>%
    rbind(getProj(sphere100, t2full, "sphere", 100)) %>%
    rbind(getProj(sphere1000, t2full, "sphere", 1000)) %>%
    rbind(getProj(pipe100, t2full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t2full, "pipe", 1000))
  save(t2, t2full, fullResPlanned, file = "cache/plannedtourpath.rda")
} else {
  load("cache/plannedtourpath.rda")
}
```

```{r plannedtour, fig.width=6, fig.height=7, fig.cap="Short tour paths with scagnostics computed on projections. Most of the indices exhibit sharp jumps in scagnostic value."}
fullResMeltPlanned <- melt(fullResPlanned, id=c("t", "name", "size"))
ggplot(fullResMeltPlanned, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```





## Guided tour

Given the behaviour of the various index measures under an interpolated tour path we next try to use the tourr implementation of the guided tour together with selected index functions to uncover the special views in datasets 2 and 3 with 1000 datapoints.

### Review of guided tour optimisation

We use the default method "search_geodesic" implemented in the tourr package. (Should I try other optimisation methods, i.e. search_better, search_better_random?)
Search geodesic first collects $n=5$ samples by default, amongst which the most promising direction is selected. In a second step a linear search along the geodesic is performed in that direction. The optimisation is stopped when ???

### Looking down the pipe

Double challengind: fine structure + (coherent) noise

As seen before (reference figure) MIC or TIC should be able to find the view down the pipe. The optimisation itself is however expected to be challenging because one has to be fairly close to the optimal projection to observe the increase in index value. We therefore suggest a two step procedure for the uncovering of such "small scale structures". The first step consists in "scouting" projections. Here we use the "search_better" optimisation available with the tourr package, with a standard search window $\alpha = 0.5$ and disabling the cooling (in practice this can be done by fixing the cooling parameter to $1$), together with a large number of tries (setting max.tries to 5000 here). The resulting search will quickly steer away from the most uninteresting projections, followed by a broad scan of the thus identified region. While the lack of cooling means that this will not converge towards the maximum, each step in this first search will increase the index value, thus it can be used to provide an informed starting plane that can be used in a second step where we use the "search_geodesic" optimization to improve the index further towards the maximum.
The described procedure is CPU time intensive, however it is promising that we manage to uncover such small scale structure in the distribution. On the other hand no "guarantee" can be provided for uncovering the special view. Parameters for the scouting phase will need to be adapted to different examples, and with increasing number of dimensions it may be better to consider several runs with different starting projections.
This gives some indication on how to handle difficult datasets. As we will see below most of the time standard "search_geodesic" optimisation will be efficient enough to uncover the interesting views.

The results are shown below, first Fig. \ref{fig:pipeFirstRun} shows how the considered index values evolve in the scouting phase. Note that this is the interpolated path and there are small local maxima and minima along the index value, especially because we keep the step size in the guided tour large. We can see the clear maximum of the TIC index in the final projection, as well as clear minimum of the convex index that carries similar information On the other hand we see that MIC or the dcor2d index are not suitable for uncovering the special view here.

UPDATE: highlight times corresponding to new basis planes by red vertical line, as we approach the special view higher index values of TIC are often found on interpolated bases, this is why generally we want cooling but here we are trying to find overall maximum, but instead have good chance of finding a plane close to the special view allowing optimisation in a second step
By no means stable procedure, when changing seed we easily end up not finding special view but rather end up with plane that is not useful for further optimisation to identify pipe view.

```{r findpipe, results="hide"}
if(!file.exists("cache/findpipe.rda")){
  set.seed(1984)
  pipeResc <- rescale(pipe1000) 
  pipeTour <- save_history(pipeResc,
                          guided_tour(mineIndex("TIC"), search_f = tourr:::search_better,
                                     cooling=1, alpha = 0.5, max.tries = 5000))
  pipeTourFull <- as.list(interpolate(pipeTour))
  save(pipeResc, pipeTour, pipeTourFull, file = "cache/findpipe.rda")
} else {
  load("cache/findpipe.rda")
}
```

```{r pipeFirstRun,fig.width=6, fig.height=7, fig.cap="Index value on tour path when scouting for high TIC index values."}
if(!file.exists("cache/pipeFirstRun.rda")){
  pipeTourRes <- getProj(pipe1000, pipeTourFull, "Pipe", 1000)
  pipeTourResMelt <- melt(pipeTourRes, id=c("t", "name", "size"))
  save(pipeTourRes, pipeTourResMelt, file = "cache/pipeFirstRun.rda")
} else {
  load("cache/pipeFirstRun.rda")
}
newBasisPipeTour <- which(attributes(interpolate(pipeTour))$new_basis)
ggplot(pipeTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")+
  geom_vline(data=as.tibble(newBasisPipeTour), mapping=aes(xintercept=value), color="red")
```

We next show in Fig \ref{fig:testpipe} the final view corresponding to the highest index found in the TIC scouting phase. While this is clearly not the optimal view of the pipe feature, we can already guess the type of dependence in the distribution, and we expect to be able to find the maximum when using this plane as a starting point of the geodesic optimisation. Indeed performing a standard guided tour with the TIC index and the identified starting plane efficiently identifies the optimal view shown in Fig ...
((Probably should remove second trace plot as it does not give much information, maybe show final view in same figure as final scouting view...))

```{r testpipe, out.width=".5\\textwidth", fig.cap="Final view discovered by the scouting phase."}
iLast <- length(pipeTourFull)
fProj <- pipeTourFull[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```

```{r refinepipe, results="hide"}
if(!file.exists("cache/refinepipe.rda")){
  pipeTour2 <- save_history(pipeResc, guided_tour(mineIndex("TIC")), start = fProj)
  pipeTourFull2 <- as.list(interpolate(pipeTour2))
  pipeTourRes2 <- getProj(pipe1000, pipeTourFull2, "Pipe", 1000)
  pipeTourResMelt2 <- melt(pipeTourRes2, id=c("t", "name", "size"))
  save(pipeTour2, pipeTourFull2, pipeTourRes2, pipeTourResMelt2, file = "cache/refinepipe.rda")
} else {
  load("cache/refinepipe.rda")
}
ggplot(pipeTourResMelt2, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")

```

```{r testpipe2, out.width=".5\\textwidth"}
iLast <- length(pipeTourFull2)
fProj <- pipeTourFull2[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```

### Finding sine waves

We next work with distribution 3 and aim to identify the view showing functional dependence using the splines2d index. Given the results from Fig 6 ((add proper ref)), we expect to be able to efficiently identify this view without relying on the two step procedure described above. Indeed the guided tour quickly converges in this case, the evolution of the index functions is shown in Fig \ref{fig:findsine}. Here there are four index functions that appear to be suitable for the optimisation: splines2d, dcor2d, MIC and TIC. On the other hand, while some of the scagnostics indices are reaching their maximum/minimum in the final projection, they do not show a smooth increase/decrease thus preventing efficient optimisation when considering them as index functions. The final view identified by the guided tour is shown in Fig \ref{}


```{r findsine, fig.width=6, fig.height=7, fig.cap="Guided tour optimising the splines2d index for the Sine 1000 dataset.", results="hide"}
set.seed(2018)
if(!file.exists("cache/findsine.rda")){
  sineResc <- rescale(sin1000)
  sineTour <- save_history(sineResc,
                          guided_tour(splineIndex()))
  sineTourFull <- as.list(interpolate(sineTour))
  sineTourRes <- getProj(sin1000, sineTourFull, "Sine", 1000)
  sineTourResMelt <- melt(sineTourRes, id=c("t", "name", "size"))
  save(sineResc, sineTour, sineTourFull, sineTourRes, sineTourResMelt, file = "cache/findsine.rda")
} else {
  load("cache/findsine.rda")
}
ggplot(sineTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")
```

```{r testsine, out.width=".5\\textwidth", fig.cap="Final view identified by the optimisation."}
iLast <- length(sineTourFull)
fProj <- sineTourFull[[iLast]]
dProj <- as.tibble(sineResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```

### Note on calculation time
Considerations: time to calculate index for single 2-d plot, parameters to turn for guided tour (max.tries, alpha, cooling)


# Application to physics data
\label{sec:phys}

We now want to apply the developed tools to distributions from realistic physics application. Here we choose as an example to use posterior samples obtained when fitting source parameters to the observed gravitational wave signal GW170817 @PhysRevLett119161101. The fit details were described in @Abbott2018exr and the corresponding data is available online @ligoData.

## Data description
The model describes a neutron star merger and contains 6 free parameters, i.e. each neutron star is described by its mass and radius, and a so-called tidal deform ability parameter $\Lambda$. A posterior sample containing 2538 points is available @ligoData and can be used to constrain the parameters. Here we use the "Parametrized-EoS\_nomaxmass\_posterior\_samples.dat" dataset. For details about the modelling and statistical procedure see @Abbott2018exr. A common representation of posterior samples used for interpretation are so-called "corner plots", i.e. density displays for all 2-d parameter combinations as well as the 1-d profile, highlighting empirically evaluated regions of highest posterior density, which correspond to credibility regions at a given level. The information can similarly be presented in a scatter plot matrix using transparency to convey the density information. Since in the following we work with 2-d scatter plots we choose to show the data in this fashion, see Fig.

```{r neutronStarSPLOM, fig.height=8, fig.width=8}
nsD <- read_csv("data/samples.csv") 
ggpairs(nsD, lower=list(continuous = wrap("points", alpha = 0.05)))
```

We point out the following features seen from Fig. XX: the combination of the two mass parameters is well constrained (a consequence of how a particular combination, the "chirp mass" enters predictions), other parameters also show patterns of dependence, they are however less pronounces. We highlight the non-linear relation between the radius $R_1$ and $\Lambda$ parameter and the two-pronged structure of the radius $R_1$.
For further study we therefore remove m2 from the dataset. We first look at the distribution in a grand tour display, where we use a scatter plot display combined with alpha transparency to study the density distribution in the 5-d parameter space. The grand tour indicates that the data points are found on a curved surface in the five dimensional space. This may stem from a non-linear relation between the parameters, which we will try to uncover using the above developed formalism for new guided tour index functions.

## Using the guided tour

Given the observations from the grand tour display we expect a functional relation between some of the parameters, and we therefore optimise the "splines2d" index function. Note that all parameters are standardised to the range $[0,1]$ before running the optimisation.
Show final view found from the guided tour, give corresponding projection matrix, make corresponding plot from combination of original parameters.

```{r neutronStarEq,  results="hide"}
set.seed(2018)
nsD <- read_csv("data/samples.csv") 
nsM <- nsD %>%
  select(-m2) %>% # remove m2 to avoid finding well known correlation
  rescale() # rescaling, this returns a matrix
if(!file.exists("cache/neutronStarEq.rda")){
  nseTour <- save_history(nsM, guided_tour(splineIndex()))
  nseTourFull <- as.list(interpolate(nseTour))
  save(nseTour, nseTourFull, file = "cache/neutronStarEq.rda")
} else {
  load("cache/neutronStarEq.rda")
}
iLast <- length(nseTourFull)
fProj <- nseTourFull[[iLast]]
dProj <- as.tibble(nsM %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
```

```{r nseMatrix, echo=FALSE}
knitr::kable(fProj ,  caption = "Matrix representation of best projection.")
```

The resulting projection clearly shows how the data selects a region that is well defined as a combination of several parameters. From the matrix representation we read off that parameters m1, L1 and R1 are important in this projection. Using the original (non-rescaled) data and taking into account the difference in scale between m1 and R1 we can reproduce a similar picture shown in Fig \ref{nsePlotOrig} (right), which we compare to the next closest bivariate plot shown on the left. It is clear that in this example additional information is found when considering general projections rather than focusing only on bivariate relations.


```{r nsePlotOrig}
p1 <- ggplot(nsD, aes(R1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
p2 <- ggplot(nsD, aes(R1-2*pi*m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
grid.arrange(p1, p2, ncol=2) 
```

This is reflecting the dependence of the deformability parameter $\Lambda$ on the so-called compactness which is proportional to $m/R$.
QUESTIONS: can see this when plotting e.g. R1/m1 vs L1, why also with the linear combination used in the plot (only works if I get the factor about right)? not clear how this was built into the simulation, seems not strict equality mentioned in the paper

## Higher D

To complicate things can work with bigger dataset that Rory sent me, need to check if this is public, also need help with interpretation, but here optimising TIC finds interesting projection

# Applications to collections of time series 

This could be another example.

```{r eval=FALSE}
music <- read_csv("data/tigs_music.csv")
music <- apply(music, 2, function(x) (x-mean(x))/sd(x))
musicTour <- save_history(music,
                         guided_tour(mineIndex("TIC"), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTour <- save_history(music,
                         guided_tour(dcorIndex(), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTourFull <- as.list(interpolate(musicTour))
iLast <- length(musicTourFull)
fProj <- musicTourFull[[iLast]]
dProj <- as.tibble(as.matrix(music) %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
fullRes <- getProj(as.matrix(music), musicTourFull, "TIC", 100)
fullResMelt <- melt(fullRes, id=c("t", "name", "size"))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```

# Discussion

# Apendix
The following measures have been defined:

* Outlying: Outliers are defined in non-parametric fashion via the edge length in the MST, where the 25% and 75% quantile edge length define a threshold value $w=q_{75}+1.5(q_{75}-q_{25})$ above which an edge is considered long. The outlying measure is then calculated as $c_{outlying} = \frac{\mathrm{Total\ length\ of\ long\ edges}}{\mathrm{Total\ length\ of\ all\ edges}}$. Note that outlying points are not considered in the calculation of the other measures.
* Skewed: Skewness is defined via the distribution of edge length in the MST, $q_{skew}=\frac{q_{90}-q{50}}{q_{90}-q_{10}}$. This is inverted as $c_{skew}= 1-w(1-q_{skew})$ to account for binning effects that result in decreasing values of $q_{skew}$ with increasing n.
* Sparse: A second measure based on the distribution of edge lenths, we define $c_{sparse} = w q_{90}$. It takes large values if points are concentrated in well separated parts of the plane, and low values if they are distributed approximately on a lattice.
* Clumpy: To indicate the clustering of points the Clumpy measure is defined as $c_{clumpy}=\max\limits_{j}[1-\max\limits_{k}[\frac{length(e_k)}{length(e_j)}]]$ where we maximise the edge length $length(e_k)$ in the smaller of two subgraphs generated by removing a single edge $e_j$ from the MST. Note that $c_{clumpy}$ will take large values (close to one) if a long edge is separating clustered points connected by short edges.
* Striated: We define an edge as striated if they have a large angle with one of the neighboring edges, concretely $cos(angle) < -0.75$, and the corresponding measure $c_{striate} = \frac{Number of striated edges}{Number of all edges}$. FIXME is this really correct? Maybe reason for discreet behaviour observed below..
* Convex: The convex measure is defined as $c_{convex}= w\frac{area(A)}{area(H)}$ where $A$ is the alpha hull and $H$ the convex hull.
* Skinny: The skinnyness of a polygon can be measured as the ratio of the perimeter to the area. We define $c_{skinny} = 1 -  \frac{\sqrt{4\pi area(A)}}{perimeter(A)}$, where the normalisation is chosen such that $c_{skinny} = 0$ for a circle, and values close to one for skinny polygons.
* Stringy: need to check, should be $c_{stringy} = \frac{diameter(MST)}{length(MST)}$ where diameter is longest connected path, and length the total length (sum of all edges), i.e. we are measuring branch structure, no branches means $c_{stringy}=1$.


# References
