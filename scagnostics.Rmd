---
title: An investigation into using scagnostics to find interesting projections of high-dimensional data

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Ursula Laa
  thanks: The authors gratefully acknowledge the support of the Australian Research Council
  affiliation: Department of Physics, Monash University
  email: \email{ursula.laa@monash.edu}
  
- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
- name: Heike Hofmann
  affiliation: Department of Statistics, Iowa State University
  
- name: Hadley Wickham
  affiliation: RStudio
  
- name: Antony Unwin
  affiliation: Department of Mathematics, Augsburg University
  
- name: Katrin Grimm
  affiliation: ???

keywords:
- tour
- projection pursuit 
- statistical graphics
- data visualisation
- exploratory data analysis
- high-dimensional data

abstract: |
  Projection pursuit describes a procedure for searching high-dimensional data for "interesting" low-dimensional projections via the optimization of a criterion function called the projection pursuit index. Most indexes developed focus on finding projections with cluster structure, outliers, or separations between known groups. Here, we are interested in finding projections revealing potentially complex relations between combinations of parameters, by combining the notion of projection pursuit with index functions developed for the detection of interesting bivariate views in the data, including scagnostics and maximum information coefficient.

bibliography: bibliography.bib
output: rticles::asa_article
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{amssymb}
  - \usepackage{xcolor}
---

```{r initial, echo = FALSE, include = FALSE}
library(knitr)
opts_chunk$set(
  warning = FALSE, message = FALSE, echo = FALSE, 
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.show = 'hold', cache = FALSE, external = TRUE, dev = "pdf",
  fig.height = 5, fig.width = 8, out.width = "\\textwidth"
)
```

# Introduction

<!-- 
Main point of paper:

- Some metrics for exploring high-dimensional data by finding interesting pairs of variables, can be used to find interesting projections, but some have problems. 
- What are the ways to evaluate.
- What adjustments can be done.
- How it can be applied

Organisation of this section:

- Motivation first?
- PP
- Scagnostics
- section needs shortening
-->

The term "projection pursuit" was coined by @ff74 to describe a procedure for searching high (say $p-$)dimensional data for "interesting" low-dimensional projections ($k=1$ or $2$ usually). The procedure, originally suggested by @kr69, involves defining a criterion function, or index, that measures the "interestingness" of each $k$-dimensional projection of $p$-dimensional data. This criterion function is optimized over the space of all $k$-dimensional projections of $p$-space, searching for both global and local maxima. It is hoped that the resulting solutions reveal low-dimensional structure in the data not found by methods such as principal component analysis.

A large number of projection pursuit indices have been developed, to detect departure from multivariate normal, which includes clusters or outliers or separations between known groups (e.g. @f87, @CBC92, @lckl2005, @AHC02, @CEM:CEM2568, @JS87, @5508437).  Less work has been done on indexes to find nonlinear dependence between variables. 

<!-- check Perisic and Posse (2012) and recent paper by Austrian guys -->


A related topic is variable selection. With high-dimensional data, even plotting all pairs can be daunting, so "scagnostics" [@scag] [@WW08] have been developed to find the most interesting pairs of variables. There are 8 scagnostics, vividly named: "Outlying", "Skewed", "Sparse", "Clumpy", "Striated", "Convex", "Skinny" and "Stringy". Our question is whether these can be extended into projection pursuit indexes that can be used to find two-dimensional projections of high-dimensional data with unusual features. 

Our motivation is based on applications in physics, to aid the interpretation of model fits on experimental results. Consider a physical model with a set of $m$ free parameters, that cannot be measured directly and are determined by fitting a set of $p$ experimental observations, for which predictions can be calculated when fixing all $m$ parameters. Note here, that while we often have analytic expressions for the predictions, this is not always the case and we may have to rely on numerical computation. Moreover, a single prediction can be a complicated function of all free parameters. In addition, we often deal with large number of observations, $p \sim 100-1000$ as well as sizable parameter spaces $m \sim 10$. The results of a fit are generally interpreted using combinations of variables selected by intuition and prior knowledge. But this begs the question, whether important associations are missed because tools to search are not available.

<!--
Suggestion German:
We are motivated by physics applications, specifically to assist the interpretation of models fit to experimental results. In these problems one has a physical model with a set of m free parameters which cannot be measured directly, but that are determined instead by fitting a set of p experimental observations. The model predicts all the observations in terms of the m parameters, often analytically, but occasionally only numerically. Moreover, a single prediction can be a complicated function of all the free parameters requiring intensive numerical resources. 

These problems often deal with a large number of observations and parameters, with p in the hundreds or even thousands and m of order 10. 
The results of a fit have been generally interpreted in terms of combinations of parameters selected by intuition or prior knowledge. Recently, these selections have been also assisted by machine learning techniques including BDT and neural networks.

Our visualisation tools complement these procedures providing a high dimensional picture which assists with the interpretation of the results,  serves as a check for other methods and possibly revealing new associations.
-->

[@Grimm2016] explored the behaviour of scagnostics for selecting variables, and proposed two more, based on smoothing splines and distance correlation, that have some nicer properties. In addition the availability of another two indices based on information criteria, maximal and total information coefficient (MIC and TIC) [@Reshef1518] round out our collection of metrics. \footnote{It is interesting to note that entropy, closely related to the notion of mutual information, has previously been considered as a projection pursuit index when searching for interesting one-dimensional projections, see \cite{nason1992}, \cite{NIPS1997_1408}.}
These indices are all available in R packages [@rref]: scagnostics [@HWscagR], mbgraphic [@mbgraphic] and minerva [@minerva]. The projection pursuit guided tour is available in the R package, tourr [@tourr].


The paper investigates the behaviour of these newly-defined indexes. Section \ref{sec:construct} discusses index construction, and how they fit into the guided tour. Section \ref{sec:investigate} investigates the behaviour of the indexes, particularly in relation to optimisation. The new guided tour methods are applied to an example where the posterior distributions from physics models (Section \ref{sec:phys}) are explored.

# Constructing a projection pursuit index
\label{sec:construct}

<!--
Explain here how to construct the pp index

- Definition of pp index
- Optimisation
- Illustration of what is already available in tourr
- New index explanations
-->
A projection pursuit index (ppi) is a scalar function $f$ defined on an $n$-dimensional data distribution. Typically the definition is such that larger values of $f$ indicate more interesting distribution, and therefore maximising $f$ over all possible $n$ dimensional projections of a data set with $p>n$ parameters will find the most interesting projections. As most indices characterise deviations from a normal distribution, one would first map the empiric data distribution onto a density for which such deviations can then be defined in different ways, see e.g. @CBC93. When departing from the idea of characterising differences to the normal distribution it may however be preferred to work directly with the empirical data distribution and instead characterise it by measures based on e.g. the minimal spanning tree or the mutual information.

## Standardization
Lower dimensional projections are highly sensitive to standardization performed on the original distribution. We want to avoid e.g.  highlighting directions based only on different scales, or artifically extending the parameter range by including outliers, which may result in seemingly linear behaviour for most points. Methods to consider include rescaling of individual directions to a common interval (e.g. to fall between $[0,1]$), sphering, outlier removal or transformation to a logarithmic scale. The method(s) of choice should be informed by the distributions found in the data set as well as aims of the analysis. \textcolor{red}{particular challenges: noise only directions, identification of outliers?}
\textcolor{red}{for holes/cmass index they are only well defined when data is standardised, how about the other index functions?}
\textcolor{red}{When keeping in mind the concrete example of posterior samples outliers should not be an issue, if extreme outliers are found one should probably investigate origin. Also noise only directions, e.g. nuissance parameters, are also not an issue as they contain no structure, see BBH example.}

## Optimization
Given a ppi we are confronted with the task of finding the maximum over all possible $n$ dimensional projections. One challenge is to avoid getting trapped in local maxima that are only a result of sampling fluctuatins or a consequence of noisy index functions, and @f87 suggested a two-step procedure: the first step is using a large step size to find the approximate global maximum while stepping over pseudomaxima. A second step is then starting from the projection corresponding to the approximate maximum and employing a gradient directed optimisation for the identification of the maximum. On the other hand the global maximum may not be the only projection of interest, and one may want to observe the selected views in the context of the full distribution rather than a static view. These issues are addressed when combining projection pursuit with the grand tour [@CBCH94]. In this case the properties of a suitable optimization algorithm include monotonicity of the index value, a variable step-size to avoid overshoothing and to increase the chance of reaching the nearest maximum, and a stopping criterion allowing to move out of a local maximum and into a new search region [@tourr]. A possible alogrithm is inspired by simulated annealing and has been described in @lckl2005, this has been implemented in the \texttt{search\_better} and \texttt{search\_better\_random} search functions in the tourr package. In addition the tourr package provides the \texttt{search\_geodesic} search function, which first selects a promising direction by comparing index values between a selected number of small random steps, and then optimises the function over the line along the geodesic in that direction considering projections up to $\pi/4$ away from the current view.

## New index functions
\label{sec:indexDef}
Focusing now on $n=2$ dimensional projections, we aim to identify projections that reveal interesting structures or correlations between combinations of variables. We therefore draw on index functions that have been developped for the selection of variable pairs that show interesting features in a scatter plot, but we generalise the notion such that instead of variable pairs we consider any $2$ dimensional projection of the multivariate data distribution. It means that we consider the projection axes as input to the index functions, which will thus score the interestingness of the given projection. What we are optimising over are then the entries of the $p\times2$ dimensional projection matrix, considering that they obey orthonormality conditions.
Concretely here we consider the following types of index functions:\footnote{We note that all descriptions are idealized, while in practice approximations and binning are used to obtain reasonable computing time, we refer to original references and package documentation for details.}

* From the scagnostics family select those best suited to detect shapes, with definitions given below. They are defined to take values between [0,1]. \textcolor{red}{The definition of these should be rotation invariant, however, some small dependence is introduced via the hex binning that is done before calculating the scagnostics indexes.}
    + Convex: The convex measure is defined as $c_{convex}= \frac{area(A)}{area(H)}$ where $A$ is the alpha hull and $H$ the convex hull. This is the only measure where interesting projections will take low values and one may either minimize the index function or maximize $1-c_{convex}$.
    + Skinny: The skinnyness of a polygon can be measured as the ratio of the perimeter to the area. We define $c_{skinny} = 1 -  \frac{\sqrt{4\pi area(A)}}{perimeter(A)}$, where the normalisation is chosen such that $c_{skinny} = 0$ for a full circle, and values close to one or skinny polygons.
    + Stringy: The stringy index is a measure of the branching structure of the minimal spanning tree (MST), $c_{stringy} = \frac{diameter(MST)}{length(MST)}$ where the diameter is the longest connected path, and the length is the total length (sum of all edges), i.e. $c_{stringy}=1$ if the MST contains no branches.

* New index functions available in @mbgraphic, defined to fall between [0,1] \textcolor{red}{and not rotation invariant, as I think Var(X)*Var(Y) introduces rotation invariance even for distance correlation?}:
    + Distance correlation defined in @szekely2007: The definition is based on empiric measures of distance covariance and variance defined on the dataset 
    \begin{equation}
(X,Y) = \{(x_1,y_1), ..., (x_n, y_n) \in \mathbb{R}^p \times \mathbb{R}^q\}
\end{equation}
 as
 \begin{equation}
 Cov^2(X,Y) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}B_{kl},\\
 Var^2(X) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}^2
 \end{equation}
 where $A_{kl}=a_{kl}-\bar{a}_{k.}-\bar{a}_{l.}+\bar{a}_{..}$ with
 \begin{equation}
 a_{kl} = | x_k - x_l|_p,\\
 \bar{a}_{k.} = \frac{1}{n} \sum_{l=1}^n a_{kl},\\
  \bar{a}_{.l} = \frac{1}{n} \sum_{k=1}^n a_{kl},\\
 \bar{a}_{..} = \frac{1}{n^2}\sum_{k,l=1}^n a_{kl}
 \end{equation}
 and reads
 \begin{equation}
 dCor(X,Y) = \begin{cases}
 \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} & Var(X)Var(Y) > 0\\
 0 & Var(X)Var(Y) = 0
 \end{cases}
 \end{equation}
    + Spline based measure: Given a spline model of the data distribution that is obtained via the R implentation in the \texttt{gam} function in the mgcv R package [@w16], we define the index based on the variance of the residuals:
    \begin{equation}
    splines_{2d} = max(1- \frac{Var(res_{X\sim Y})}{Var(X)}, 1-\frac{Var(res_{Y\sim X})}{Var(Y)}),
    \end{equation}
    which takes large values if the distribution is well described by the spline model, indicating functional dependence.
    
* Information based index functions from @Reshef1518 are based on the mutual information $I$ when considering the distribution induced by the data points $D \subset \mathbb{R}^2$ on an $x-$by$y$ grid $G$. With $I^{\ast} (D, x, y) = \max I (D|_G)$ the characteristic matrix $M$ is defined as
\begin{equation}
M(D)_{x,y} = \frac{I^{\ast} (D, x, y)}{\log \min{x,y}},
\end{equation}
where the denominator is chosen such that each entry of $M$ will take values between [0,1].
We can now define the following metrics:
    + Maximum Information Coefficient (MIC):
    \begin{equation}
    MIC(D) = \max_{xy<B(n)}\{M(D)_{x,y}\},
    \end{equation}
    where by default $B(n) = n^{0.6}$
    + Total Information Coefficient (TIC), see @JMLRv1715308:
    \begin{equation}
    TIC(D) = \sum_{xy<B(n)} M(D)_{x,y}
    \end{equation}
  While $MIC$ is normalised to fall between $[0,1]$, the upper bound of $TIC$ depends on the sample size via the bounding function $B(n)$. $TIC$ was designed to be more stable test against null hypothesis of independence, using the full information of the characteristic matrix rather than just the maximum, and as such is expected to be better suited as a ppi.
  
\textcolor{red}{What is upper bound of TIC as function of sample size? If we define $n' = n^{0.6}$, rounded to integer values only, can we calculate it as $TIC_{max} = 2 \sum_{i=1}^{n'} \frac{n'}{i}$, keeping in mind that each entry of the characteristic matrix should be between [0,1]. Each entry in the sum should be rounded also, only integer number of bins enter actual calculation, so this should be approximation only.}




```{r load}
library(tourr)
library(tidyverse)
library(reshape2)
library(scagnostics)
library(gridExtra)
library(tictoc) #timer
library(mbgraphic) #Katrins package
library(GGally)
library(geozoo)
library(minerva) #MINE indices
```


# Investigation of indices
\label{sec:investigate}
We now examine the indices listed in Section \ref{sec:indexDef} to test which of them are suitable as projection pursuit measures, and how we may modify them for improved performance. We start by considering the relevant criteria:

  - smoothness: the index function should evolve smoothly between nearby projections, i.e. on an interpolated path we expect to observe slow and consistent change in the index function. On the other hand if two similar projections produce very different index values, i.e. we observe sharp jumps when looking at the evolution of the index function, and in particular if those fluctuations dominate the overall evolution, the index is not suitable for projection pursuit.
  - speed: the computation time is an important factor in particular when the guided tour is used to view the optimisation in real time. Note that longer computation times may be acceptable when first recording the optimisation and replaying the guided tour once the maximum has been identified.
  - ability to find fine and broad structure: We consider a broad structure one that is roughly observable over a large angle around the optimal projection, it should in general be easy to identify and a good ppi should show smooth increase over a large number of projections as we are approaching the optimal view. On the other hand fine structures are difficult to identify as they are only visible close to the optimal view. This poses problems in particular for the optimization, but we may use index definitions to amplify larger index values while suppressing smaller ones, such that the optimization will be less sensitive to noise.
  - distribution of index values: the distribution of index values when studying data points without structure should have a low variance, and be clearly separated from index values obtained when looking at structured data distributions.
  - rotation invariance: in the tour there is an assumed degeneracy between all projections obtained when rotating within the projected space only, for 2-d projections it means that we cannot assing x- and y- axis without ambiguity. A projection pursuit index should therefore be invariant under rotation of the basis in the projection plane. By costruction this is not the case for most index functions defined above. (One way around could be profiling the function over all possible rotations in the plane, howevere computing time may be a major concern in this case.) (Other examples where rotation invariance is important include image recognition and texture classification, rotation angle may be built in parameter for summary calculation, e.g. https://doi.org/10.1016/S0146-664X(75)80008-6, other example is interpolating over angles when building NN, see e.g. https://arxiv.org/pdf/1805.12301.pdf)
  
##Simulation study
To test the behaviour of the index functions we use three types of simulated data sets based on different underlying distributions, and with different built-in features. They are defined in $n=6$ dimensions with $p$ randomly selected points $x_i, i = 1,..,n$ according to:

1. $p$ points on an n-dimensional sphere randomly selected using the geozoo package [@geozoo], as an example of dataset without special views, labelled "sphere"
2. $p$ points where $n-1$ points are independently drawn from a uniform distribution between $[-1,1]$, and enforcing $x_n^2 + x_{n-1}^2 = 1 \pm 0.1$ by rejection sampling as an example of a dataset with special view that is not described by functional dependence, and where sampling boundaries are apparent in the distribution, labelled "pipe"
3. $p$ points where $n-1$ points are independently drawn from a normal distribution with mean 0 and variance 1, and with $x_n = \sin(x_{n-1}) + \mathrm{jittering}$ as an example dataset with special view described by a function, labelled "sine"

For each distribution we consider a small sample of $p=100$ and a large sample with $p=1000$.

To understand the potential for the various indices defined we use the following overview procedures on the simulated data sets:

- index values for pair of variables, explicitly comparing the uninformative and special view of each dataset, as well as discussing the dependence on the sample size
- tour between pairs of variables, considering a planned tour that moves from the $x_1 - x_2$ view either to the $x_3 - x_4$ view (no special structure encountered over the full path) or to the $x_5 - x_6$ (moving from uninformed to special view)
- optimisation path, starting from $x_1 - x_2$, how can we use the available index functions and optimisation algorithms to identify the special view
  
###Dataset overview


```{r util}
#defining index functions to be used with the tour
scagIndex <- function(scagType){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(sR[scagType])
  }
}

invConvexIndex <- function(){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(1 - sR["Convex"])
  }
}

splineIndex <- function(){
  function(mat){
    return(splines2d(mat[,1], mat[,2]))
  }
}

dcorIndex <- function(){
  function(mat){
    return(dcor2d(mat[,1], mat[,2]))
  }
}

mineIndex <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2])[[mineIndex]])
  }
}
```


```{r datasetFunctions}
sphereData <- function(n, p){
  dRet <- geozoo::sphere.solid.random(n,p)
  return(as.tibble(dRet$points))
}

pipeData <- function(n, p){
  i <- 1
  dRet <- NULL
  while(i <= p){
    v <- runif(n, -1, 1)
    if(abs(v[n-1]*v[n-1] + v[n]*v[n] - 1) < 0.1){
      dRet <- rbind(dRet, v)
      i <- i+1
    }
  }
  return(as.tibble(dRet))
}

sinData <- function(n, p){
  vName <- paste0("V",n)
  vNameM1 <- paste0("V",n-1)
  expr <- paste0(vName,"=sin(",vNameM1,")") # need string expression if I want to use tibble here
  dRet <- as.tibble(matrix(rnorm((n-1)*p), ncol=(n-1))) #generate normal distributed n-1 dim data
  dRet <- mutate_(dRet, expr) #string evaluation calculates var(n) as tan(var(n-1))
  colnames(dRet)[n] <- vName #correct name of new variable
  dRet[vName] <- jitter(dRet[[vName]]) #adding noise
  return(dRet)
}
```

<!-- I think you need to standardise the scale of the data -->

```{r datasets}
set.seed(1984)
sphere100 <- sphereData(6, 100) %>% rescale() %>% as_tibble()
sphere1000 <- sphereData(6, 1000) %>% rescale() %>% as_tibble()
sphere100S <- scale(sphere100) %>% as_tibble()
sphere1000S <- scale(sphere1000) %>% as_tibble()

pipe100 <- pipeData(6, 100) %>% rescale() %>% as_tibble()
pipe1000 <- pipeData(6,1000) %>% rescale() %>% as_tibble()
pipe100S <- scale(pipe100) %>% as_tibble()
pipe1000S <- scale(pipe1000) %>% as_tibble()

sin100 <- sinData(6, 100) %>% rescale() %>% as_tibble()
sin1000 <- sinData(6, 1000) %>% rescale() %>% as_tibble()
sin100S <- scale(sin100) %>% as_tibble()
sin1000S <- scale(sin1000) %>% as_tibble()
```

To get an overview of the dataset we show views of the first two parameters showing the independent distribution and the last two parameters showing the special view. The enforced relationships will also affect the 1-d densities as can be seen from \ref{fig:dataPlotsDensity}, which means that views including sizable fractions of V6 (and V5 for the Pipe distribution) will be distinct from uninformed views of distributions 2 and 3.


```{r dataPlotsV1V2, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the first two parameters V1 and V2, i.e. showing a typical uninformed view for each dataset."}
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V1, V2)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

```{r dataPlotsV5V6, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the last two parameters V5 and V6, i.e. showing the special view built into the Pipe and Sine datasets."}
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V5, V6)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

<!--
its possible to do a single figure legend - need to look up code for this
-->

```{r dataPlotsDensity, fig.width=6, fig.height=7, fig.cap="One dimensional density distribution for all parameters and all data sets considered. Apart from statistical fluctuations we see that for the Pipe datasets the parameters V5 and V6 are pushed towards more extreme values by vetoing points away from the circle outline, and for the Sine dataset the parameter V6 follows a very distinct distribution. Note also that the first five paramters of the Sine dataset appear to be drawn from different normal distributions, which is a result of the rescaling, shifting the modes to different values for each direction."}
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  distData <- melt(ds)
  pC <- ggplot(distData, aes(value, color=variable)) +
    geom_density() +
    ggtitle(pLabels[i])
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

We next compare the various index values for three views: V1 vs V2, V5 vs V6 and V1 vs V6 (capturing differences from the different distribution in V6). For the Sphere distribution by construction we see only random fluctuations between the different views, which can be sizable in particular for small datasets. Considering next the Pipe distribution, we see that skinny, stringy and dcor2d are increased for the special view, but in particular MIC and TIC index are much larger and may be used to detect this type of structure in the dataset. Moreover, minimising the convex index (or maximising 1-convex) should also allow us to detect the special view. Finally the Sine distribution shows that several of the indices are maximised in the special view, in particular also the splines2d index designed for the detection of fictional dependence.

```{r indexTable, echo=FALSE}
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000",
             "Sphere 100 S", "Sphere 1000 S", "Pipe 100 S", "Pipe 1000 S", "Sine 100 S", "Sine 1000 S")
indexOverview <- tibble(convex=numeric(),
               skinny=numeric(),
               stringy=numeric(),
               dcor2d=numeric(),
               splines2d=numeric(),
               MIC=numeric(),
               TIC=numeric(),
               holes=numeric(),
               name=character(),
               vars=character())
dsL <- list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000,
            sphere100S, sphere1000S, pipe100S, pipe1000S, sin100S, sin1000S)
for(ds in dsL){
  for(vars in list(c("V1","V2"),c("V5","V6"),c("V1","V6"))){
    dprj <- as.matrix(select_(ds, vars[1], vars[2]))
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    indexOverview <- add_row(indexOverview, convex=scagRes[,"Convex"],
                    skinny=scagRes[,"Skinny"], stringy=scagRes[,"Stringy"],
                    dcor2d=dcorRes, splines2d=splineRes,
                    MIC=mineRes$MIC, TIC=mineRes$TIC, holes=holesRes,
                    name=pLabels[i], vars=paste0(vars[1],"-",vars[2]))
  }
  i <- i+1
}
knitr::kable(indexOverview,  caption = "Summary of index values.", digits=2) 
```



## Smoothness
We first study the smoothness of each measure calculated on a sequence of 2-d projections obtained via an interpolated planned tour path moving between the projection onto V1-V2 to the projection onto V3-V4 and using the default interpolation angle of 0.05. Each time step refers to an interpolation step. Since the value of the TIC index is strongly dependent on the input distribution sample size we have normalised the TIC measure by its maximum for each individual dataset, all other measures are reproduced as reported by the calculation. The results are shown in Figure \ref{fig:plotV1V2toV3V4}. The standard index functions "holes" and "cmass" show the desired behaviour in that the change is smooth over interpolated views ((it is however curious to see the actual evolution, which seems not to depend on the underlying random distributions, but is clearely evolving over the path of the planned tour)). Considering now the indices from the scagnostics family, we first note stronge dependence on the sample size. The convex index is largely sensitive to random fluctuations when considering small samples (p=100), while being mostly flat for larger samples, especially in the case of an underlying uniform distribution. The skinny index on the other hand is systematically larger for the smaller samples, while stringy appears not to be sensitive to the sample size, but resulting in large fluctuations and sharp jumps for all datasets. Distance correlation and the spline based index show similar behaviour, i.e. when considering large enough data samples (p=1000) the values are consistently close to zero, as expected for a distribution without structure. On the other hand somewhat larger values (but still far from the maximum value of 1) are found when considering small datasets, where distributions are however mostly smooth, with few kinks depending on index function and underlying distribution. Finally the information based metrics also show systematic differences based on the sample size, with sampling fluctuations being significant for small datasets. Note here that fluctuations present in Figure \ref{fig:plotV1V2toV3V4} may be amplified, as maximum values for most index functions are far from the theoretic maximum.

<!-- index values shouldn't be affected by sample size, on average it should be
same value. Maybe some need to be normalised by n. Something needs fixing here.

can we get "theoretical" min/max values, to assess the scale of the index for any particular data set.
-->

```{r getProj}
getProj <- function(df, tPath, nameStr, size){
  sc <- tibble(
    holes=numeric(),
    cmass=numeric(),
    convex=numeric(),
    skinny=numeric(),
    stringy=numeric(),
    dcor2d=numeric(),
    splines2d=numeric(),
    MIC=numeric(),
    TIC1=numeric(),
    t=numeric(),
    name=character(),
    size=numeric())
  n <- length(tPath)
  for (i in 1:n) {
    dprj <- as.matrix(df) %*% tPath[[i]]
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    cmassRes <- cmass(dprj)
    sc <- add_row(sc, holes=holesRes, cmass=cmassRes,
                  convex=scagRes[,"Convex"], skinny=scagRes[,"Skinny"],
                  stringy=scagRes[,"Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC1=mineRes$TIC, t=i, name=nameStr, size=size)
  }
  maxTIC <- max(sc$TIC1)
  sc <- sc %>%
    mutate(TIC = TIC1/maxTIC) %>%
    select(-TIC1)
  
  return(sc)
}
```

```{r V1V2toV3V4}
if(!file.exists("cache/V1V2toV3V4.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,1,0,0,0,0,0,0,1,0,0),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t1 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t1full <- as.list(interpolate(t1))
  fullRes <- getProj(sin100, t1full, "sin", 100) %>%
    rbind(getProj(sin1000, t1full, "sin", 1000)) %>%
    rbind(getProj(sphere100, t1full, "sphere", 100)) %>%
    rbind(getProj(sphere1000, t1full, "sphere", 1000)) %>%
    rbind(getProj(pipe100, t1full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t1full, "pipe", 1000)) %>%
    rbind(getProj(sin100S, t1full, "sin-S", 100)) %>%
    rbind(getProj(sin1000S, t1full, "sin-S", 1000)) %>%
    rbind(getProj(sphere100S, t1full, "sphere-S", 100)) %>%
    rbind(getProj(sphere1000S, t1full, "sphere-S", 1000)) %>%
    rbind(getProj(pipe100S, t1full, "pipe-S", 100)) %>%
    rbind(getProj(pipe1000S, t1full, "pipe-S", 1000))
  save(t1, t1full, fullRes, file = "cache/V1V2toV3V4.rda")
} else {
  load("cache/V1V2toV3V4.rda")
}
```

```{r plotV1V2toV3V4, fig.width=6, fig.height=7, fig.cap="Short tour paths of uninformed views, moving from the projection onto V1-V2 to the projection onto V3-V4, for all considered datasets and index functions. Small samples (p=100) are shown in red, large samples (p=1000) in blue."}
fullResMelt <- melt(fullRes, id=c("t", "name", "size")) %>%
  mutate(sphered = str_detect(name,"-")) %>%
  mutate(name=str_replace(name,"-S",""))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size), linetype = factor(sphered))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```


## Transition
We repeat the same exercise, but now we are interested in how the index values change when going from the view in V1 vs V2 to the view in V5 vs V6, which will also allow us to view the picture between typical low and high values of the index function. Results are shown in Figure \ref{fig:plannedtour}.
We can see a pretty clear and smooth increase in dcor2d and splines2d as well as MIC and TIC index when approaching the special view of the Sine distribution, which should therefore be efficient to optimise. On the other hand while we also see smooth increase in MIC and TIC when approaching the special view of the Pipe distribution, the increase only becomes apparent much closer to the final projection, making this structure more challenging to detect with a guided tour.
Note that the various indices appear less noisy than for the grand tour shown before, but should note that the tour path here is shorter, and the y-axis scale is different (typically reaching higher index values in this example, whereas smaller fluctuations are more amplified in the previous views).

TIC was suggested in @JMLRv1715308 as an alternative to MIC with better properties for testing against independence, this is because the full information of the characteristics is used, implying that more information is considered. We also see that TIC is smoother when moving into the special view.

Here we also see how the existing indices in the tourr package are inadequate for the detection of these type of dependence, the holes index taking maximum values before the final view and decreasing when moving into the special view. On the other hand, surprisingly the cmass index increases when moving into the special view, but the intermediate minimum makes it unsuitable as a projection pursuit index here.


```{r plannedtourpath}
if(!file.exists("cache/plannedtourpath.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,0,0,1,0,0,0,0,0,0,1),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t2 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t2full <- as.list(interpolate(t2))
  fullResPlanned <- getProj(sin100, t2full, "sin", 100) %>%
    rbind(getProj(sin1000, t2full, "sin", 1000)) %>%
    rbind(getProj(sphere100, t2full, "sphere", 100)) %>%
    rbind(getProj(sphere1000, t2full, "sphere", 1000)) %>%
    rbind(getProj(pipe100, t2full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t2full, "pipe", 1000)) %>%
    rbind(getProj(sin100S, t2full, "sin-S", 100)) %>%
    rbind(getProj(sin1000S, t2full, "sin-S", 1000)) %>%
    rbind(getProj(sphere100S, t2full, "sphere-S", 100)) %>%
    rbind(getProj(sphere1000S, t2full, "sphere-S", 1000)) %>%
    rbind(getProj(pipe100S, t2full, "pipe-S", 100)) %>%
    rbind(getProj(pipe1000S, t2full, "pipe-S", 1000))
  save(t2, t2full, fullResPlanned, file = "cache/plannedtourpath.rda")
} else {
  load("cache/plannedtourpath.rda")
}
```

```{r plannedtour, fig.width=6, fig.height=7, fig.cap="As Figure \\ref{fig:plotV1V2toV3V4} but moving from the projection onto V1-V2 to the projection onto V5-V6, i.e. starting from random distribution view, and moving into the special view."}
fullResMeltPlanned <- melt(fullResPlanned, id=c("t", "name", "size")) %>%
  mutate(sphered = str_detect(name,"-")) %>%
  mutate(name=str_replace(name,"-S",""))
ggplot(fullResMeltPlanned, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size),linetype = factor(sphered))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```


## Distributions of index values

Compare typical index values over the two tour paths. Notation: path 1 / 2 labels tour from V1-V2 to V3-V4 / V5-V6. Typically find broader distributions for small samples because of the large fluctuations in this case. Some indices show preferred behaviour, i.e. sharp peaks around low values for path 1, and very broad distributions leading up to maximum values for path 2. For other indices we can see the strong dependence of the index on sample size, the most extreme example is the skinny index.

\textcolor{red}{Need better representation, in particular normalise TIC to upper limit for given sample size, adapt scales so sharp peaks don't dominate some of the plots.}

```{r indexDist, fig.width=6, fig.height=7, fig.cap="Comparing distributions of index values found in the interpolated tour paths 1 and 2 as described in the text."}
fullResMelt1 <- fullResMelt %>% add_column(path="path1")
fullResMeltPlanned1 <- fullResMeltPlanned %>% add_column(path="path2")
allRes <- rbind(fullResMelt1, fullResMeltPlanned1)
ggplot(allRes, aes(x=value)) +
  geom_density(aes(color=path, linetype = factor(size))) +
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE))
```


## Note on calculation time
Computing time differs for each index, and depends strongly on the number of data points (as well as probably some dependence on distribution type), here we use as an example the 6-d sphere and calculate the computing time for different indices over 100 interpolated grand tour projections as a function of the sample size.
We see that the evaluation of the spline index shows weak dependence on the sample size, making it the least efficient for very small data sets, but one of the fastest when considering large data sets. The situation is similar for calculating the scagnositics measures, which are however faster to compute on datasets of around 10 points. This is because the implementation is first binning the data points, which is fast compared to the evaluation that requires triangulation of the binned points. The distance correlation based measure exhibits strong increase with sample size, we note however that the implementation also allowes for evaluation on binned data points which we have not considered. Finally for the MINE family the computing time increases exponentially as can be expected from the dependence on the sample size, where we see that the improved $MIC_e$ algorithm is somewhat faster than the original implementation, see also @JMLRv1715308.

```{r getTimer}

mineE <- function(x,y){
  return(mine(x=x, y=y, est = "mic_e"))
}


timeThis <- function(d, t, idx, pmax){ #d=data matrix, t=interpolated tour path, idx=index function, pmax=max number of projections
    i <- 1
    tic.clearlog()
    tic() #start timer
    for(pMatrix in t){
      if(i>pmax) break
      dProj <- d %*% pMatrix
      sgnst <- idx(dProj[,1],dProj[,2])
      i = i+1
    }
    toc(log=TRUE,quiet=TRUE)
    scTd <- unlist(tic.log(format=FALSE))["toc.elapsed"]-unlist(tic.log(format=FALSE))["tic.elapsed"]
}

```


```{r timer}
if(!file.exists("cache/timer.rda")){
  set.seed(2018)
  grandTour100 <- save_history(sphere100, grand_tour(2), max=4) %>%
    interpolate() %>%
    as.list()

  sizeL <- c(10, 100, 500, 1000, 5000, 10000)
  n <- length(sizeL)
  dfTimer = data.frame( n=rep(0, n), scagT=rep(0,n), dcorT=rep(0,n), splineT=rep(0,n), mineT=rep(0,n),
                        mineeT=rep(0,n))
  t <- 1
  for(sampleSize in sizeL){
    set.seed(sampleSize + 11) # new seed for each sample size
    if(sampleSize == 0) sampleSize = 10 # smallest considered sample has 10 points
    thisMatrix <-  sphereData(6, sampleSize) %>% rescale()
    scagT <- timeThis(thisMatrix, grandTour100, scagnostics.default, 100)
    dcorT <- timeThis(thisMatrix, grandTour100, dcor2d, 100)
    splineT <- timeThis(thisMatrix, grandTour100, splines2d, 100)
    mineT <- timeThis(thisMatrix, grandTour100, mine, 100)
    mineeT <- timeThis(thisMatrix, grandTour100, mineE, 100)
    dfTimer[t,] <- c(sampleSize, scagT, dcorT, splineT, mineT,mineeT)
    t <-  t+1
  }
  save(dfTimer, file = "cache/timer.rda")
} else {
  load("cache/timer.rda")
}

timerMelt <- melt(dfTimer, id="n")
ggplot(timerMelt, aes(x = n, y = value, color=variable)) + 
  geom_point() +
  scale_x_log10() +
  scale_y_log10() +
  labs(x = "Sample Size", y = "Time [s]", color = "Index family")
```


## Guided tour

Given the behaviour of the various index measures under an interpolated tour path we next try to use the tourr implementation of the guided tour together with selected index functions to uncover the special views in datasets 2 and 3 with 1000 datapoints.


### Looking down the pipe

\textcolor{red}{Double challenging: fine structure + (coherent) noise}

As seen before (reference figure) MIC or TIC should be able to find the view down the pipe. The optimisation itself is however expected to be challenging because one has to be fairly close to the optimal projection to observe the increase in index value. We therefore suggest a two step procedure for the uncovering of such "small scale structures". The first step consists in "scouting" projections. Here we use the "search_better" optimisation available with the tourr package, with a standard search window $\alpha = 0.5$ and disabling the cooling (in practice this can be done by fixing the cooling parameter to $1$), together with a large number of tries (setting max.tries to 5000 here). The resulting search will quickly steer away from the most uninteresting projections, followed by a broad scan of the thus identified region. While the lack of cooling means that this will not converge towards the maximum, each step in this first search will increase the index value, thus it can be used to provide an informed starting plane that can be used in a second step where we use the "search_geodesic" optimization to improve the index further towards the maximum.
The described procedure is CPU time intensive, however it is promising that we manage to uncover such small scale structure in the distribution. On the other hand no "guarantee" can be provided for uncovering the special view. Parameters for the scouting phase will need to be adapted to different examples, and with increasing number of dimensions it may be better to consider several runs with different starting projections.
This gives some indication on how to handle difficult datasets. As we will see below most of the time standard "search_geodesic" optimisation will be efficient enough to uncover the interesting views.

The results are shown below, first Fig. \ref{fig:pipeFirstRun} shows how the considered index values evolve in the scouting phase. Note that this is the interpolated path and there are small local maxima and minima along the index value, especially because we keep the step size in the guided tour large. We can see the clear maximum of the TIC index in the final projection, as well as clear minimum of the convex index that carries similar information On the other hand we see that MIC or the dcor2d index are not suitable for uncovering the special view here.

\textcolor{red}{Sometimes index value decreases, because index is not rotationally invariant.}


```{r findpipe, results="hide"}
if(!file.exists("cache/findpipe.rda")){
  set.seed(1984)
  pipeResc <- rescale(pipe1000) 
  pipeTour <- save_history(pipeResc,
                          guided_tour(mineIndex("TIC"), search_f = tourr:::search_better,
                                     cooling=1, alpha = 0.5, max.tries = 5000))
  pipeTourFull <- as.list(interpolate(pipeTour))
  save(pipeResc, pipeTour, pipeTourFull, file = "cache/findpipe.rda")
} else {
  load("cache/findpipe.rda")
}
```

```{r pipeFirstRun,fig.width=6, fig.height=7, fig.cap="Index value on tour path when scouting for high TIC index values."}
if(!file.exists("cache/pipeFirstRun.rda")){
  pipeTourRes <- getProj(pipe1000, pipeTourFull, "Pipe", 1000)
  pipeTourResMelt <- melt(pipeTourRes, id=c("t", "name", "size"))
  save(pipeTourRes, pipeTourResMelt, file = "cache/pipeFirstRun.rda")
} else {
  load("cache/pipeFirstRun.rda")
}
newBasisPipeTour <- which(attributes(interpolate(pipeTour))$new_basis)
ggplot(pipeTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")+
  geom_vline(data=as.tibble(newBasisPipeTour), mapping=aes(xintercept=value), color="red")
```

We next show in Fig \ref{fig:testpipe} (left) the final view corresponding to the highest index found in the TIC scouting phase. While this is clearly not the optimal view of the pipe feature, we can already identify the feature in the distribution, and we expect to be able to find the maximum when using this plane as a starting point of the geodesic optimisation. Indeed performing a standard guided tour with the TIC index and the identified starting plane efficiently identifies the optimal view shown in the right plot. In this case the optimisation quickly converges, following a smooth, monotonic increas of the TIC index.


```{r refinepipe, results="hide"}
if(!file.exists("cache/refinepipe.rda")){
  pipeTour2 <- save_history(pipeResc, guided_tour(mineIndex("TIC")), start = fProj)
  pipeTourFull2 <- as.list(interpolate(pipeTour2))
  pipeTourRes2 <- getProj(pipe1000, pipeTourFull2, "Pipe", 1000)
  pipeTourResMelt2 <- melt(pipeTourRes2, id=c("t", "name", "size"))
  save(pipeTour2, pipeTourFull2, pipeTourRes2, pipeTourResMelt2, file = "cache/refinepipe.rda")
} else {
  load("cache/refinepipe.rda")
}
```


```{r testpipe,  fig.height=4, fig.cap="Final view discovered by the scouting phase (left) and after second optimisation (right)."}
iLast <- length(pipeTourFull)
fProj <- pipeTourFull[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
p1 <- ggplot(dProj, aes(V1,V2)) + geom_point()
iLast <- length(pipeTourFull2)
fProj <- pipeTourFull2[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
p2 <- ggplot(dProj, aes(V1,V2)) + geom_point()
grid.arrange(p1, p2, ncol=2)
```

### Finding sine waves

We next work with distribution 3 and aim to identify the view showing functional dependence using the splines2d index. Given the results from the planned tour we expect to be able to efficiently identify this view without relying on the two step procedure described above. Indeed the guided tour quickly converges in this case, the evolution of the index functions is shown in Fig \ref{fig:findsine}. Here there are four index functions that appear to be suitable for the optimisation: splines2d, dcor2d, MIC and TIC. On the other hand, while some of the scagnostics indices are reaching their maximum/minimum in the final projection, they do not show a smooth increase/decrease thus preventing efficient optimisation when considering them as index functions. The final view identified by the guided tour is shown in Fig \ref{}


```{r findsine, fig.width=6, fig.height=7, fig.cap="Guided tour optimising the splines2d index for the Sine 1000 dataset.", results="hide"}
set.seed(2018)
if(!file.exists("cache/findsine.rda")){
  sineResc <- rescale(sin1000)
  sineTour <- save_history(sineResc,
                          guided_tour(splineIndex()))
  sineTourFull <- as.list(interpolate(sineTour))
  sineTourRes <- getProj(sin1000, sineTourFull, "Sine", 1000)
  sineTourResMelt <- melt(sineTourRes, id=c("t", "name", "size"))
  save(sineResc, sineTour, sineTourFull, sineTourRes, sineTourResMelt, file = "cache/findsine.rda")
} else {
  load("cache/findsine.rda")
}
ggplot(sineTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")
```

```{r testsine, out.width=".5\\textwidth", fig.cap="Final view identified by the optimisation."}
iLast <- length(sineTourFull)
fProj <- sineTourFull[[iLast]]
dProj <- as.tibble(sineResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```



# Application to physics data
\label{sec:phys}

<!--
- First example
    - Automatically find the functional relationship between three parameters
- Second example
    - Stronger relationship between already nonlinearly associated variables, when additional variable is incorporated
    - 
-->

We now want to apply the developed tools to distributions from realistic physics application. Here we choose as an example to use posterior samples obtained when fitting source parameters to the observed gravitational wave signal GW170817 @PhysRevLett119161101. The fit details were described in @Abbott2018exr and the corresponding data is available online @ligoData.

## Data description
The model describes a neutron star merger and contains 6 free parameters, i.e. each neutron star is described by its mass and radius, and a so-called tidal deform ability parameter $\Lambda$. A posterior sample containing 2538 points is available @ligoData and can be used to constrain the parameters. Here we use the "Parametrized-EoS\_nomaxmass\_posterior\_samples.dat" dataset. For details about the modelling and statistical procedure see @Abbott2018exr. A common representation of posterior samples used for interpretation are so-called "corner plots", i.e. density displays for all 2-d parameter combinations as well as the 1-d profile, highlighting empirically evaluated regions of highest posterior density, which correspond to credibility regions at a given level. The information can similarly be presented in a scatter plot matrix using transparency to convey the density information. Since in the following we work with 2-d scatter plots we choose to show the data in this fashion, see Fig.

```{r neutronStarSPLOM, fig.height=8, fig.width=8}
nsD <- read_csv("data/samples.csv") 
ggpairs(nsD, lower=list(continuous = wrap("points", alpha = 0.05)))
```

We point out the following features seen from Fig. XX: the combination of the two mass parameters is well constrained (a consequence of how a particular combination, the "chirp mass" enters predictions), other parameters also show patterns of dependence, they are however less pronounces. We highlight the non-linear relation between the radius $R_1$ and $\Lambda$ parameter and the two-pronged structure of the radius $R_1$.
For further study we therefore remove m2 from the dataset. We first look at the distribution in a grand tour display, where we use a scatter plot display combined with alpha transparency to study the density distribution in the 5-d parameter space. The grand tour indicates that the data points are found on a curved surface in the five dimensional space. This may stem from a non-linear relation between the parameters, which we will try to uncover using the above developed formalism for new guided tour index functions.

## Using the guided tour

Given the observations from the grand tour display we expect a functional relation between some of the parameters, and we therefore optimise the "splines2d" index function. Note that all parameters are standardised to the range $[0,1]$ before running the optimisation.
Show final view found from the guided tour, give corresponding projection matrix, make corresponding plot from combination of original parameters.

```{r neutronStarEq,  results="hide"}
set.seed(2018)
nsD <- read_csv("data/samples.csv") 
nsM <- nsD %>%
  select(-m2) %>% # remove m2 to avoid finding well known correlation
  rescale() # rescaling, this returns a matrix
if(!file.exists("cache/neutronStarEq.rda")){
  nseTour <- save_history(nsM, guided_tour(splineIndex()))
  nseTourFull <- as.list(interpolate(nseTour))
  save(nseTour, nseTourFull, file = "cache/neutronStarEq.rda")
} else {
  load("cache/neutronStarEq.rda")
}
iLast <- length(nseTourFull)
fProj <- nseTourFull[[iLast]]
dProj <- as.tibble(nsM %*% fProj)
```

```{r nseMatrix, echo=FALSE}
knitr::kable(fProj ,  caption = "Matrix representation of the final projection identified by the guided tour.")
```

\textcolor{red}{should have function formatting projectino matrices for readable output.}

The resulting projection clearly shows how the data selects a region that is well defined as a combination of several parameters. From the matrix representation we read off that parameters m1, L1 and R1 are important in this projection. Using the original (non-rescaled) data and taking into account the difference in scale between m1 and R1 we can reproduce a similar picture shown in Fig \ref{fig:nsePlotOrig} (right), which we compare to the next closest bivariate plot shown on the left. It is clear that in this example additional information is found when considering general projections rather than focusing only on bivariate relations.


```{r nsePlotOrig, fig.cap="Comparison of guided tour final view (left), approximation based on original parameters (middle) and expected relation based on analysis setup (right)."}
p1 <- ggplot(dProj, aes(V1,V2)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
p2 <- ggplot(nsD, aes(R1-2*pi*m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
p3 <- ggplot(nsD, aes(R1/m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3) 
```

This is reflecting the dependence of the deformability parameter $\Lambda$ on the so-called compactness which is approximately proportional to $(m/R)^{-5}$ [@Abbott2018exr].
\textcolor{red}{Move up, this is part of the data description, we get back what was put in.}

## Higher D
The above example is using only a reduced set of the free parameters relevant to the physical interpretation. In general many additional nuissance parameters will also enter the fit, see e.g. @Smith:2016qas. We use a simulation study \textcolor{red}{simulating a binary black hole (BBH) merger event...need more info maybe, no reference but Rory offered to write short description} including the full set of nuissance parameters as a test case of larger parameter space in which we aim to detect structure. The dataset is summarised in \ref{fig:bbhSimulation} where we have removed a subset of less informative parameters to obtain a clear picture. Interesting correlations can be seen between the parameters "ra", "dec" and "time", where the first two describe the position of the BBH event in the sky, and the "time" being the merging time (in GPS units). Note in particular the relation between "dec" and "time" is not functional, which motivates us to use the TIC index for this example. Some other interesting patterns can also be identified, and we next optimise the index in a guided tour.

```{r bbhSimulation, fig.height=8, fig.width=8, dev = "png", dpi=300, fig.cap="Scatter plot matrix showing most of the variables included in the BBH dataset. Strong correlation between the parameters time, dec and ra can be observed."}
bbhD <- read_csv("data/posterior_samples.csv")
bbhDsmall <- select(bbhD, -phi_jl, -m2, -psi, -chi_eff)
ggpairs(bbhDsmall, lower=list(continuous = wrap("points", alpha = 0.02)))
```

```{r bbhGuided, results="hide", fig.cap="Final views identified by the guided tour using the TIC index (left), the splines2d index (middle) or 1-convex (right)."}
bbhM <- rescale(select(bbhD,-m2, -chi_eff))
if(!file.exists("cache/bbhGuidedTIC.rda")){
  set.seed(2018)
  bbhTour1 <- save_history(bbhM, guided_tour(mineIndex("TIC")))
  bbhTourFull1 <- as.list(interpolate(bbhTour1))
  save(bbhTour1, bbhTourFull1, file = "cache/bbhGuidedTIC.rda")
} else {
  load("cache/bbhGuidedTIC.rda")
}
if(!file.exists("cache/bbhGuidedSpline.rda")){
  set.seed(2018)
  bbhTour2 <- save_history(bbhM, guided_tour(splineIndex()))
  bbhTourFull2 <- as.list(interpolate(bbhTour2))
  save(bbhTour2, bbhTourFull2, file = "cache/bbhGuidedSpline.rda")
} else {
  load("cache/bbhGuidedSpline.rda")
}
if(!file.exists("cache/bbhGuidedConvex.rda")){
  set.seed(2018)
  bbhTour3 <- save_history(bbhM, guided_tour(invConvexIndex()))
  bbhTourFull3 <- as.list(interpolate(bbhTour3))
  save(bbhTour3, bbhTourFull3, file = "cache/bbhGuidedConvex.rda")
} else {
  load("cache/bbhGuidedConvex.rda")
}

iLast <- length(bbhTourFull1)
fProj1 <- bbhTourFull1[[iLast]]
dProj <- as.tibble(bbhM %*% fProj1)
p1 <- ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
iLast <- length(bbhTourFull2)
fProj2 <- bbhTourFull2[[iLast]]
dProj <- as.tibble(bbhM %*% fProj2)
p2 <- ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
iLast <- length(bbhTourFull3)
fProj3 <- bbhTourFull3[[iLast]]
dProj <- as.tibble(bbhM %*% fProj3)
p3 <- ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3) 
```

Indeed the TIC index guided tour found an informative view between the three parameters time, ra and dec whithin the full datase (after once again removing the second mass parameter, see explanation above), which is not described by a function. We can approximate it by showing the sum of the rescaled ra and dec variable against time, or when using the original parameter values by approximate scaling, see Figure \ref{fig:bbhRes}, but clearly the view in Fig. \ref{fig:bbhGuided} has higher TIC score. 

\textcolor{red}{Things to add here: comparison of final projection matrices for all three, comparison of index evolution between the three index functions over the TIC guided tour path, plotly animation of the guided tour (check with Nick S. about function to readily to this)}


# Applications to collections of time series 

This could be another example.

```{r eval=FALSE}
music <- read_csv("data/tigs_music.csv")
music <- apply(music, 2, function(x) (x-mean(x))/sd(x))
musicTour <- save_history(music,
                         guided_tour(mineIndex("TIC"), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTour <- save_history(music,
                         guided_tour(dcorIndex(), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTourFull <- as.list(interpolate(musicTour))
iLast <- length(musicTourFull)
fProj <- musicTourFull[[iLast]]
dProj <- as.tibble(as.matrix(music) %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
fullRes <- getProj(as.matrix(music), musicTourFull, "TIC", 100)
fullResMelt <- melt(fullRes, id=c("t", "name", "size"))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```

# Discussion



# References
