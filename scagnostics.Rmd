---
title: Using Scagnostics to find Interesting Projections of Multivariate Data

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Ursula Laa
  thanks: The authors gratefully acknowledge the support of the Australian Research Council
  affiliation: Department of Physics, Monash University
  email: \email{ursula.laa@monash.edu}
  
- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
- name: Heike Hofmann
  affiliation: Department of Statistics, Iowa State University
  
- name: Hadley Wickham
  affiliation: RStudio
  
- name: Antony Unwin
  affiliation: Department of Mathematics, Augsburg University
  
- name: Katrin Grimm
  affiliation: ???

keywords:
- tour
- projection pursuit 
- statistical graphics
- data visualisation
- exploratory data analysis
- high-dimensional data

abstract: |
  Projection pursuit describes a procedure for searching high-dimensional data for "interesting" low-dimensional projections via the optimization of a criterion function called the projection pursuit index. Most indexes developed focus on finding projections with cluster structure, outliers, or separations between known groups. Here, we are interested in finding projections where the shapes are odd, or where known groups have different shapes despite having the same mean and variance. The new indexes are based on scagnostics, which were originally developed to choose pairs of interesting variables from a large collection. An examination of the properties of the scagnostics is included, and code in the form of an R package is provided.  

bibliography: bibliography.bib
output: rticles::asa_article
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
---

```{r initial, echo = FALSE, cache = FALSE, include = FALSE}
library(knitr)
opts_chunk$set(
  warning = FALSE, message = FALSE, echo = FALSE, 
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.show = 'hold', cache = TRUE, external = TRUE, dev = "pdf",
  fig.height = 5, fig.width = 8, out.width = "\\textwidth"
)
```

# Introduction

The term "projection pursuit" was coined by @ff74 to describe a procedure for searching high-(say $p$)dimensional data for "interesting" low- dimensional projections ($k=1$ or $2$ usually). The procedure, originally suggested by @kr69, involves defining a criterion function, or index, that measures the "interestingness" of each $k$-dimensional projection of $p$-dimensional data. This criterion function is optimized over the space of all $k$-dimensional projections of $p$-space, searching for both global and local maxima. It is hoped that the resulting solutions reveal low-dimensional structure in the data not necessarily found by methods such as principal component analysis.

A large number of projection pursuit indices have been developed [@f87] [@CBC92] [@Lee05] [@AHC02] [@CEM:CEM2568] [@JS87] [@5508437]. Many of these are developed to detect departure from multivariate normal, which includes clusters or outliers, or nonlinearity. Some have been developed to detect separations between known groups. However, if one wants to find unusual shapes, or shape differences between groups there are no options. One reason, is that it is hard, because defining what is meant by shape is not straightforward.

(Motivation - something along the lines of the following paragraph, but more understandable, maybe giving concrete example - e.g. dec vs time from neutron star posterior?)
One motivation to consider projection pursuit indices beyond the ones considered so far is coming from physics applications where a standard method is the determination of physcis parameters by means of global fits to large experimental datasets. Typically models will come with a handful of free parameters, model predictions will be calculated by means of numeric tools and the best fit parameters are evaluated by comparing predictions to a large number of observations. Fit results will encode relations between the model parameters dictaded by the available observations, for example if two parameters control predictions for a well constrained observable, looking at the distribution of parameter combinations which are a good fit will be instructive. A standard example is to study the likelihood or posterior distribution in pair-wise 2-d plots, where the selection of parameters if often guided by knowledge of the underlying physics. A major challenge is the progressing complexity in all three aspects: models with larger parameter spaces are being studied, computation of observable predictions is increasingly complex and the number of experimental observations relevant to a given model is growing. Methods for the detection of "interesting" views that can aid physics intuition are therefore more and more relevant.


A related question is the selection of pair-wise scatter plots in datasets with large numbers of parameters, for which different methods have been developed. One approch are scatterplot diagnostics, "scagnostics", developed by @scag, and explored in @WW08. They are calculated from a set of elementary graph-theoretic building blocks: the convex hull, the alpha hull and the minimal spanning tree (MST). These are used to define eight measures: Outlying, Skewed, Sparse, Clumpy, Striated, Convex, Skinny and Stringy. While in principle each measure could be used on its own to define a ranking, the original idea suggested considering pair-wise scatter plots of the scagnostics measures for identification of "characteristic" and "unusual" 2-d views in the original parameter space. Alternative methods suggested by (Katrins Thesis or maybe her R package?) suggest using parallel coordinates, glyphs or a so-called scagramm (based on the idea of corrgrams) that shouold be coupled to interactive brushing for selection of pair-wise scatter plots to be displayed.
(FIXME just give reference rather than all definitions here?)
(Issues with scagnostics already for selection of pairs have been described in Katrins Thesis, maybe have short summary of that here?)
Given the various shortcomings of the scagnostics approach we consider several other measures that can directly be interpreted as ranking of information contained in a given 2-d view.
Two such additional measures have been defined in @mbgraphic. The first is based on smoothing splines, and is defined such that it will take maximum values when the data distribution can be described by a functional form. For a scatterplot with variable X and Y we calculate it as
\begin{equation}
splines_{2d} = max(1-\frac{Var(res_{X\sim Y})}{Var(X)}, 1-\frac{Var(res_{Y\sim X})}{Var(Y)})
\end{equation}
where $Var(.)$ is the variance and $res_{X\sim Y}$ are the residuals in the spline model where $X$ is considered a function of $Y$.
The second new measure introduced in @mbgraphic is aiming to detect also non-functional relations and is based on the idea of distance correlation @szekely2007. (should I repeat definitions here?)
Finally we will also consider the maximal and total information coefficient (MIC and TIC) defined in @Reshef1518 which are based on mutual information found when imposing grids on pair-wise variable plots. These functions are availabel in R via the interface implemented in the minerva package @minerva.
(read paper and add more detail. how are they similar/different compared to scagnostics and distance correlation? the way that binning is being built into it may make difference in how MIC/TIC may be smoother?)

Scagnostics are scatterplot diagnostics. They were primarily developed to extract pairs of interesting variables from a large collection.
XXX (Ursula to fill in based loosely on material at the German language wikipedia page - it would be good to translate this page and provide an English version on wikipedia).
Nine graph-theoretic measures have been defined to characterise scatterplots. They are calculated from a set of elementary building blocks: the convex hull, the alpha hull and the minimal spanning tree (MST). For definiteness all variables are rescaled to a [0,1] intervall, such that all measures will take values between 0 and 1 as well.
The following measures have been defined:

* Outlying: Outliers are defined in non-parametric fashion via the edge length in the MST, where the 25% and 75% quantil edge length define a threshold value $w=q_{75}+1.5(q_{75}-q_{25})$ above which an edge is considered long. The outlying measure is then calculated as $c_{outlying} = \frac{\mathrm{Total\ length\ of\ long\ edges}}{\mathrm{Total\ length\ of\ all\ edges}}$. Note that outlying points are not considered in the calculation of the other measures.
* Skewed: Skewness is defined via the distribution of edge length in the MST, $q_{skew}=\frac{q_{90}-q{50}}{q_{90}-q_{10}}$. This is inverted as $c_{skew}= 1-w(1-q_{skew})$ to account for binning effects that result in decreasing values of $q_{skew}$ with increasing n.
* Sparse: A second measure based on the distribution of edge lenths, we define $c_{sparse} = w q_{90}$. It takes large values if points are concentrated in well separated parts of the plane, and low values if they are distributed approximately on a lattice.
* Clumpy: To indicate the clustering of points the Clumpy measure is defined as $c_{clumpy}=\max\limits_{j}[1-\max\limits_{k}[\frac{length(e_k)}{length(e_j)}]]$ where we maximise the edge length $length(e_k)$ in the smaller of two subgraphs generated by removing a single edge $e_j$ from the MST. Note that $c_{clumpy}$ will take large values (close to one) if a long edge is separating clustered points connected by short edges.
* Striated: We define an edge as striated if they have a large angle with one of the neighboring edges, concretely $cos(angle) < -0.75$, and the corresponding measure $c_{striate} = \frac{Number of striated edges}{Number of all edges}$. FIXME is this really correct? Maybe reason for discreet behaviour observed below..
* Convex: The convex measure is defined as $c_{convex}= w\frac{area(A)}{area(H)}$ where $A$ is the alpha hull and $H$ the convex hull.
* Skinny: The skinnyness of a polygon can be measured as the ratio of the perimiter to the area. We define $c_{skinny} = 1 -  \frac{\sqrt{4\pi area(A)}}{perimeter(A)}$, where the normalisation is chosen such that $c_{skinny} = 0$ for a circle, and values close to one for skinny polygons.
* Stringy: need to check, should be $c_{stringy} = \frac{diameter(MST)}{lenght(MST)}$ where diameter is longest connected path, and lenght the total lenght (sum of all edgest), i.e. we are measuring branch structure, no branches means $c_{stringy}=1$.

Note that in practice, for efficiency and robustness, the measures are calculated from binned data, and after removing outliers.
Moreover, scaling factors are used to mitigate dependence on sample size.
Scagnostics are available in two R packages @LWscagR and @HWscagR. This work is built upon the scagnostics package by @HWscagR because the underlying base is built using C, works across platforms. 

A projection pursuit index, a function of all possible projections of the data, invariably has many "hills and valleys" and "knife-edge ridges" because of the varying shapes in the underlying density of observations from one projection to the next. From an exploratory data analysis perpective it is interesting to combine numerical optimisation with visualisation to watch the structure of the data moving into a maximum, to jump away from this projection and follow the optimisation again, invariably moving from local maxima to local maxima, and perhaps even a global maximum. Each of these maxima can reveal different information about the data. Projection pursuit optimisation combined with geodesic interpolations between projections provided by a tour [@As85], is called a projection pursuit guided tour [@CBCH94]. Software to run a projection pursuit guided tour is available in the R package, "tourr" [@tourr].
(Even just for the optimisation part the guided tour implementation is useful.)
(Traditional projection pursuit indices are fast to compute and therefore the optimisation would be done in "real time" while displaying the tour. For the measures considered here this generally does not apply, as a consequence we "disentangle" the two into subsequent steps.)

The paper investigates the nature of scagnostics, particularly when considered as candidates for projection pursuit indices (Section \ref{sec:investigate})). It develops indexes for finding shapes as defined by the scagnostics, and for finding different shapes between multiple groups (Section \ref{sec:indexes}). Changes to the optimisation of the projection pursuit guided tour were needed to accommodate some of the complexities of the scagnostic indexes. The new guided tour methods are applied to contemporary problems from paarticle physics (Section \ref{sec:phys}). The paper is accompanied by the R package "binostics" available on CRAN, and the new indices have been implemented in the "tourr" package. 

# Investigation of scagnostics
\label{sec:investigate}


```{r load}
library(tourr)
library(tidyverse)
library(reshape2)
library(scagnostics)
library(gridExtra)
library(tictoc) #timer
library(mbgraphic) #Katrins package
library(GGally)
library(geozoo)
library(minerva) #MINE indices
```

The nine scagnostics measures are designed to be computed together, producing a matrix of values with dimensions corresponding to the number of all pairs of variables, and the number of scagnostics. If there are 100 variables, the dimension will be $4950\times 9$. It is typically used to extract the pairs of variables that have the highest scagnostic values, and thus considered to be the most interesting. It can also be used to do more in-depth exploratory analysis, and is interesting to study the distribution of the scagnostics resulting from the data, as a data set in itself. In a sense this the data's "fingerprint".

To use scagnostics for projection pursuit requires that each of the measures for functions that are relatively smooth over the space of all 2D projections, and that they can be computed rapidly enough that the display can be updated many times a second during optimisation.

## Dataset

We consider three example distributions in n=6 dimensions with p randomly selected points $x_i, i = 1,..,n$:
1. p points on an n-dimensional sphere as an example of dataset without special views
2. p points where n-1 points are independently drawn from a uniform distribution between [-1,1], and enforcing $x_n^2 + x_{n-1}^2 = 1 \pm 0.1$ by rejection sampling as an example of a dataset with special view that is not showing functional dependence, and where sampling boundaries are apparant in the distribution 
3. p points where n-1 points are independently drawn from a normal distribution with mean 0 and variance 1, and with $x_n = \sin(x_{n-1}) + \mathrm{jittering}$ as an example dataset with special view described by a function

## Smoothness
We first study the smoothness of a subset of measures calculated on a sequence of 2-d projections obtained via an interpolated grand tour path between 4 anchor planes and using the default interpolation angle of 0.05. For this comparison we rescale the data such that each parameter takes values between [0,1]. The temporal trace of a selected set of measures is shown below, where each time step refers to an interpolation step. Note that the value of TIC is strongly dependent on the input distribution and we have therefore normalised the TIC measure by its maximum for each individual dataset, all other measures are reproduced as reported by the calculation.


```{r util}
#defining index functions to be used with the tour
scagIndex <- function(scagType){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(sR[scagType])
  }
}

splineIndex <- function(){
  function(mat){
    return(splines2d(mat[,1], mat[,2]))
  }
}

dcorIndex <- function(){
  function(mat){
    return(dcor2d(mat[,1], mat[,2]))
  }
}

mineIndex <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2])[[mineIndex]])
  }
}
```

```{r datasetFunctions}
sphereData <- function(n, p){
  dRet <- geozoo::sphere.solid.random(n,p)
  return(as.tibble(dRet$points))
}

pipeData <- function(n, p){
  i <- 1
  dRet <- NULL
  while(i <= p){
    v <- runif(n, -1, 1)
    if(abs(v[n-1]*v[n-1] + v[n]*v[n] - 1) < 0.1){
      dRet <- rbind(dRet, v)
      i <- i+1
    }
  }
  return(as.tibble(dRet))
}

sinData <- function(n, p){
  vName <- paste0("V",n)
  vNameM1 <- paste0("V",n-1)
  expr <- paste0(vName,"=sin(",vNameM1,")") # need string expression if I want to use tibble here
  dRet <- as.tibble(matrix(rnorm((n-1)*p), ncol=(n-1))) #generate normal distributed n-1 dim data
  dRet <- mutate_(dRet, expr) #string evaluation calculates var(n) as tan(var(n-1))
  colnames(dRet)[n] <- vName #correct name of new variable
  dRet[vName] <- jitter(dRet[[vName]]) #adding noise
  return(dRet)
}
```

```{r datasets}
sphere100 <- sphereData(6, 100)
sphere1000 <- sphereData(6, 1000)

pipe100 <- pipeData(6, 100)
pipe1000 <- pipeData(6,1000)

sin100 <- sinData(6, 100)
sin1000 <- sinData(6, 1000)
```

```{r getProj}
getProj <- function(df, tPath, nameStr, size){
  sc <- tibble(convex=numeric(),
               clumpy=numeric(),
               skinny=numeric(),
               stringy=numeric(),
               dcor2d=numeric(),
               splines2d=numeric(),
               MIC=numeric(),
               TIC1=numeric(),
               t=numeric(),
               name=character(),
               size=numeric())
  n <- length(tPath)
  df <- rescale(df)
  for (i in 1:n) {
    dprj <- df %*% tPath[[i]]
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    sc <- add_row(sc, convex=scagRes["Convex"],
                  clumpy=scagRes["Clumpy"], skinny=scagRes["Skinny"],
                  stringy=scagRes["Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC1=mineRes$TIC, t=i, name=nameStr, size=size)
  }
  maxTIC <- max(sc$TIC1)
  sc <- sc %>%
    mutate(TIC = TIC1/maxTIC) %>%
    select(-TIC1)
  
  return(sc)
}
```

```{r grandtourpath}
set.seed(1988)
t1 <- save_history(sphere100, grand_tour(2), max=4)
t1full <- as.list(interpolate(t1))
fullRes <- getProj(sin100, t1full, "sin", 100) %>%
  rbind(getProj(sin1000, t1full, "sin", 1000)) %>%
  rbind(getProj(sphere100, t1full, "sphere", 100)) %>%
  rbind(getProj(sphere1000, t1full, "sphere", 1000)) %>%
  rbind(getProj(pipe100, t1full, "pipe", 100)) %>%
  rbind(getProj(pipe1000, t1full, "pipe", 1000))
```

```{r plotgrandtour, fig.width=6, fig.height=7, fig.cap="Short tour paths with scagnostics computed on projections. Most of the indices exhibit sharp jumps in scagnostic value."}
fullResMelt <- melt(fullRes, id=c("t", "name", "size"))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=name, linetype=factor(size))) + 
  facet_wrap(~variable, ncol=1, scales = "free_y")
```

Sharp jumps can be observed in all measures except the distance correlation measure when considering small datasets of only 100 points. Generally we observe smoother behaviour for larger datasets, notably all considered scagnostics measures show only small variation around a flat baseline. Interestingly all other indices indicate preferred viewing directions for dataset 3 when studying large samples, but not for dataset 2 which contains a special view in the same direction. We therefore show 2-d view defined by the tour step $t=58$, with projection matrix
```{r t58matrix, echo=FALSE}
knitr::kable(t1full[[58]] ,  caption = "Matrix representation of projection 58.")
```
i.e. while the last two parameters enter significantly, they will not dominate the view and we do not expect to fully observe the built in features.
```{r plot58, fig.width=6, fig.height=7, fig.cap="View of all datasets defined by at tour step 58."}
p58 <- t1full[[58]]
pL <- list()
i <- 1
pLabels <- c("Sphere 100", "Sphere 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(sphere100, sphere1000, pipe100, pipe1000, sin100, sin1000)){
  dm <- rescale(ds)
  pData <- as.tibble(dm %*% p58)
  pC <- ggplot(pData, aes(V1, V2)) +
    geom_point() +
    ggtitle(pLabels[i])
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]],
             pL[[3]], pL[[4]],
             pL[[5]], pL[[6]],
             ncol=2)

```
We see from Fig. ((how to add cross references?)) that the built in special views cannot be seen in this projection, but confirm that the sine 1000 dataset presents some correlation in this view.



## Guided tour
Given the behaviour of the various index measures under an interpolated tour path we next try to use the tourr implementation of the guided tour together with selected index functions to uncover the special views in datasets 2 and 3 with 1000 datapoints.

### Review of guided tour optimisation
We use the default method "search_geodesic" implemented in the tourr package. (Should I try other optimisation methods, i.e. search_better, search_better_random?)
Search geodesic first collects $n=5$ samples by default, amongst which the most promising direction is selected. In a second step a linear search along the geodesic is performed in that direction. The optimisation is stopped when ???

### Looking down the pipe
TIC should be able to uncover special view, compare performance of all indices on guided tour path. (also cmass and holes?)
Not currently finding "best" view. Options: compare results from n different random starting planes, try with smaller space (e.g. omitting variable 1 and 2), turn some of the guided tour parameters
```{r findpipe, fig.width=6, fig.height=7, fig.cap="Guided tour optimising TIC index for Pipe 1000 dataset."}
set.seed(2018)
pipeResc <- rescale(pipe1000)
pipeTour <- save_history(pipeResc,
                         guided_tour(mineIndex("TIC")), start = p58)
pipeTourFull <- as.list(interpolate(pipeTour))
pipeTourRes <- getProj(pipe1000, pipeTourFull, "Pipe", 1000)
pipeTourResMelt <- melt(pipeTourRes, id=c("t", "name", "size"))
ggplot(pipeTourResMelt, aes(x=t, y=value)) +
  geom_line() + 
  facet_wrap(~variable, ncol=1, scales = "free_y")
```

```{r testpipe}
iLast <- length(pipeTourFull)
fProj <- pipeTourFull[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```

### Finding sine waves
Use spline2d to try and find sine shape, compare performance of all indices on guided tour path.

### Note on calculation time
Considerations: time to calculate index for single 2-d plot, parameters to turn for guided tour (max.tries, alpha, cooling)


# Scagnostics indices
\label{sec:indexes}

## Individual 


## Two group differences


## Multiple group differences


# Application to particle physics data
\label{sec:phys}

## Data description

## Software setup

## Scagnostics of grand tour projections

# Discussion

# References
