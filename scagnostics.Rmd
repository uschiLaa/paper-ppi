---
title: Using tours to visually investigate properties of new projection pursuit indexes with application to problems in physics

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Ursula Laa
  thanks: The authors gratefully acknowledge the support of the Australian Research Council
  affiliation: Department of Physics, Monash University
  email: \email{ursula.laa@monash.edu}
  
- name: Dianne Cook
  affiliation: Department of Econometrics and Business Statistics, Monash University
  
keywords:
- scagnostics
- statistical graphics
- data visualisation
- exploratory data analysis
- data science 
- guided tour

abstract: |
  Projection pursuit is used to find interesting low-dimensional projections of high-dimensional data by optimizing an index over all possible projections. Most indexes have been developed to detect departure from known distributions, such as normality, or to find separations between known groups. Here, we are interested in finding projections revealing potentially complex bivariate patterns, using new indexes constructed from scagnostics and a maximum information coefficient, with a purpose to detect unusual relationships between model parameters describing physics phenomena. The performance of these indices is examined with respect to ideal behaviour, using simulated data, and then applied to several physics problems. The implementation builds upon the projection pursuit tools available in the R package, tourr, with indices constructed from code in the R package, scagnostics.

bibliography: bibliography.bib
output: rticles::asa_article
header-includes:
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{amssymb}
  - \usepackage{xcolor}
---

```{r initial, echo = FALSE, include = FALSE}
library(knitr)
opts_chunk$set(
  warning = FALSE, message = FALSE, echo = FALSE, 
  fig.path = 'figure/', cache.path = 'cache/', fig.align = 'center', 
  fig.show = 'hold', cache = FALSE, external = TRUE, dev = "pdf",
  fig.height = 5, fig.width = 8, out.width = "\\textwidth"
)
opts_knit$set(eval.after = "fig.cap") #so I can use chunck output in the caption
```

# Introduction

<!-- 
Main point of paper:

- Some metrics for exploring high-dimensional data by finding interesting pairs of variables, can be used to find interesting projections, but some have problems. 
- What are the ways to evaluate.
- What adjustments can be done.
- How it can be applied

Organisation of this section:

- Motivation first?
- PP
- Scagnostics
- section needs shortening
-->

The term "projection pursuit" was coined by @FT74 to describe a procedure for searching high (say $p-$)dimensional data for "interesting" low-dimensional projections ($d=1$ or $2$ usually). The procedure, originally suggested by @kr69, involves defining a criterion function, or index, that measures the "interestingness" of each $d$-dimensional projection of $p$-dimensional data. This criterion function is optimized over the space of all $d$-dimensional projections of $p$-space, searching for both global and local maxima. It is hoped that the resulting solutions reveal low-dimensional structure in the data not found by methods such as principal component analysis.

A large number of projection pursuit indices have been developed, to detect departure from multivariate normal, which includes clusters or outliers or separations between known groups (e.g. @f87, @CBC92, @lckl2005, @AHC02, @CEM:CEM2568, @JS87, @5508437).  Less work has been done on indexes to find nonlinear dependence between variables. 

XXX Need to add something here about the guided tour

<!-- check Perisic and Posse (2012) and recent paper by Austrian guys -->


A related topic is variable selection. With high-dimensional data, even plotting all pairs can be daunting, so "scagnostics" [@scag] [@WW08] have been developed to find the most interesting pairs of variables. There are 8 scagnostics, vividly named: "Outlying", "Skewed", "Sparse", "Clumpy", "Striated", "Convex", "Skinny" and "Stringy". Our question is whether these can be extended into projection pursuit indexes that can be used to find two-dimensional projections of high-dimensional data with unusual features. 

Our motivation is based on applications in physics, to aid the interpretation of model fits on experimental results. Consider a physical model with a set of $p$ free parameters, that cannot be measured directly and are determined by fitting a set of $n$ experimental observations, for which predictions can be calculated when fixing all $p$ parameters. Note here, that while we often have analytic expressions for the predictions, this is not always the case and we may have to rely on numerical computation. Moreover, a single prediction can be a complicated function of all free parameters. In addition, we often deal with large number of observations, $n \sim 100-1000$ as well as sizable parameter spaces $p \sim 10$. The results of a fit are generally interpreted using combinations of variables selected by intuition and prior knowledge. But this begs the question, whether important associations are missed because tools to search are not available.

<!--
Suggestion German:
We are motivated by physics applications, specifically to assist the interpretation of models fit to experimental results. In these problems one has a physical model with a set of m free parameters which cannot be measured directly, but that are determined instead by fitting a set of p experimental observations. The model predicts all the observations in terms of the m parameters, often analytically, but occasionally only numerically. Moreover, a single prediction can be a complicated function of all the free parameters requiring intensive numerical resources. 

These problems often deal with a large number of observations and parameters, with p in the hundreds or even thousands and m of order 10. 
The results of a fit have been generally interpreted in terms of combinations of parameters selected by intuition or prior knowledge. Recently, these selections have been also assisted by machine learning techniques including BDT and neural networks.

Our visualisation tools complement these procedures providing a high dimensional picture which assists with the interpretation of the results,  serves as a check for other methods and possibly revealing new associations.
-->

[@Grimm2016] explored the behavior of scagnostics for selecting variables, and proposed two more, based on smoothing splines and distance correlation, that have some nicer properties. In addition the availability of another two indices based on information criteria, maximal and total information coefficient (MIC and TIC) [@Reshef1518], and computationally more efficient versions (MIC_e, TIC_e) [@JMLRv1715308], round out the collection of metrics. \footnote{It is interesting to note that entropy, closely related to the notion of mutual information, has previously been considered as a projection pursuit index when searching for interesting one-dimensional projections, see discussion in \cite{huber85,jones87}.}
These indices are all available in R packages [@rref]: scagnostics [@HWscagR], mbgraphic [@mbgraphic] and minerva [@minerva]. The projection pursuit guided tour is available in the R package, tourr [@tourr].

The paper investigates the behavior of these newly-defined indexes. Section \ref{sec:construct} discusses index construction, and how they fit into the guided tour. Section \ref{sec:investigate} investigates the behavior of the indexes, particularly in relation to optimization. The new guided tour methods are applied to an example where the posterior distributions from physics models (Section \ref{sec:phys}) are explored.

# Constructing a projection pursuit index
\label{sec:construct}

<!--
Explain here how to construct the pp index

- Definition of pp index
- Optimisation
- Illustration of what is already available in tourr
- New index explanations
-->
A projection pursuit index (PPI) is a scalar function $f$ defined on an $d$-dimensional data distribution. Typically the definition is such that larger values of $f$ indicate more interesting distribution, and therefore maximizing $f$ over all possible $d$ dimensional projections of a data set with $p>d$ parameters will find the most interesting projections. As most indices characterize deviations from a normal distribution, one would first map the empiric data distribution onto a density for which such deviations can then be defined in different ways, see e.g. @CBC93. When departing from the idea of characterizing differences to the normal distribution it may however be preferred to work directly with the empirical data distribution and instead characterize it by measures based on e.g. the minimal spanning tree or the mutual information.

## Standardization
Lower dimensional projections are highly sensitive to standardization performed on the original distribution. We want to avoid e.g.  highlighting directions based only on different scales, or artificially extending the parameter range by including outliers, which may result in seemingly linear behavior for most points. Methods to consider include rescaling of individual directions to a common interval (e.g. to fall between $[0,1]$), sphering, outlier removal or transformation to a logarithmic scale. The method(s) of choice should be informed by the distributions found in the data set as well as aims of the analysis. \textcolor{red}{particular challenges: noise only directions, identification of outliers?}
\textcolor{red}{for holes/cmass index they are only well defined when data is standardized, how about the other index functions?}
\textcolor{red}{When keeping in mind the concrete example of posterior samples outliers should not be an issue, if extreme outliers are found one should probably investigate origin. Also noise only directions, e.g. nuisance parameters, are also not an issue as they contain no structure, see BBH example.}

\textcolor{red}{\cite{jones87} discusses centering and sphering of the data points. Centering is needed if index is not translational invariant (should not be the case for any index considered here). Arguments for sphering are that scale effects should be extracted, we don't want to reproduce information already obtained by PCA. To my mind rescaling to an overall interval is more appropriate here. Rotations between parameters should not affect the outcome, but will make interpretation more difficult. Standardizing the variance will remove information about the posterior variance. Overall we are interested in strong association only, if these structures were affected by the standardization procedure, it would most likely be unwelcome effects?}


## Optimization
Given a PPI we are confronted with the task of finding the maximum over all possible $d$ dimensional projections. One challenge is to avoid getting trapped in local maxima that are only a result of sampling fluctuations or a consequence of noisy index functions, and @f87 suggested a two-step procedure: the first step is using a large step size to find the approximate global maximum while stepping over pseudomaxima. A second step is then starting from the projection corresponding to the approximate maximum and employing a gradient directed optimization for the identification of the maximum. On the other hand the global maximum may not be the only projection of interest, and one may want to observe the selected views in the context of the full distribution rather than a static view. These issues are addressed when combining projection pursuit with the grand tour [@CBCH94]. In this case the properties of a suitable optimization algorithm include monotonicity of the index value, a variable step-size to avoid overshooting and to increase the chance of reaching the nearest maximum, and a stopping criterion allowing to move out of a local maximum and into a new search region [@tourr]. A possible algorithm is inspired by simulated annealing and has been described in @lckl2005, this has been implemented in the \texttt{search\_better} and \texttt{search\_better\_random} search functions in the tourr package. In addition the tourr package provides the \texttt{search\_geodesic} search function, which first selects a promising direction by comparing index values between a selected number of small random steps, and then optimists the function over the line along the geodesic in that direction considering projections up to $\pi/4$ away from the current view.


\textcolor{red}{\cite{posse95b} discusses the optimization, in particular that for most index functions and optimists results are too local, largely dependent on starting point. Fiedmans suggestion of hybrid stepping search + steepest-ascent still remains fairly local. \cite{posse90} suggested random search instead, which seems to perform better, but did not find pdf of that article, so not so sure what he actually does.}


## New index functions
\label{sec:indexDef}
Focusing now on $d=2$ dimensional projections, we aim to identify projections that reveal interesting structures or correlations between combinations of variables. We therefore draw on index functions that have been developed for the selection of variable pairs that show interesting features in a scatter plot, but we generalist the notion such that instead of variable pairs we consider any $2$ dimensional projection of the multivariate data distribution. It means that we consider the projection axes as input to the index functions, which will thus score the interestingness of the given projection. What we are optimizing over are then the entries of the $p\times2$ dimensional projection matrix, considering that they obey orthonormality conditions.
Concretely here we consider the following types of index functions:\footnote{We note that all descriptions are idealized, while in practice approximations and binning are used to obtain reasonable computing time, we refer to original references and package documentation for details.}

* From the scagnostics family select those best suited to detect shapes, with definitions given below. They are defined to take values between [0,1]. \textcolor{red}{The definition of these should be rotation invariant, however, some small dependence is introduced via the hex binning that is done before calculating the scagnostics indexes.}
    + Convex: The convex measure is defined as $c_{convex}= \frac{area(A)}{area(H)}$ where $A$ is the alpha hull and $H$ the convex hull. This is the only measure where interesting projections will take low values and one may either minimize the index function or maximize $1-c_{convex}$.
    + Skinny: The skinnyness of a polygon can be measured as the ratio of the perimeter to the area. We define $c_{skinny} = 1 -  \frac{\sqrt{4\pi area(A)}}{perimeter(A)}$, where the normalization is chosen such that $c_{skinny} = 0$ for a full circle, and values close to one or skinny polygons.
    + Stringy: The stringy index is a measure of the branching structure of the minimal spanning tree (MST), $c_{stringy} = \frac{diameter(MST)}{length(MST)}$ where the diameter is the longest connected path, and the length is the total length (sum of all edges), i.e. $c_{stringy}=1$ if the MST contains no branches.

* New index functions available in @mbgraphic, defined to fall between [0,1] \textcolor{red}{and not rotation invariant, as I think Var(X)*Var(Y) introduces rotation invariance even for distance correlation?}:
    + Distance correlation defined in @szekely2007: The definition is based on empiric measures of distance covariance and variance defined on the dataset 
    \begin{equation}
(X,Y) = \{(x_1,y_1), ..., (x_n, y_n) \in \mathbb{R}^p \times \mathbb{R}^q\}
\end{equation}
 as
 \begin{equation}
 Cov^2(X,Y) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}B_{kl},\\
 Var^2(X) = \frac{1}{n^2} \sum_{k,l=1}^{n} A_{kl}^2
 \end{equation}
 where $A_{kl}=a_{kl}-\bar{a}_{k.}-\bar{a}_{l.}+\bar{a}_{..}$ with
 \begin{equation}
 a_{kl} = | x_k - x_l|_p,\\
 \bar{a}_{k.} = \frac{1}{n} \sum_{l=1}^n a_{kl},\\
  \bar{a}_{.l} = \frac{1}{n} \sum_{k=1}^n a_{kl},\\
 \bar{a}_{..} = \frac{1}{n^2}\sum_{k,l=1}^n a_{kl}
 \end{equation}
 and reads
 \begin{equation}
 dCor(X,Y) = \begin{cases}
 \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}} & Var(X)Var(Y) > 0\\
 0 & Var(X)Var(Y) = 0
 \end{cases}
 \end{equation}
    + Spline based measure: Given a spline model of the data distribution that is obtained via the R implantation in the \texttt{gam} function in the mgcv R package [@w16], we define the index based on the variance of the residuals:
    \begin{equation}
    splines_{2d} = max(1- \frac{Var(res_{X\sim Y})}{Var(X)}, 1-\frac{Var(res_{Y\sim X})}{Var(Y)}),
    \end{equation}
    which takes large values if the distribution is well described by the spline model, indicating functional dependence.
    
* Information based index functions from @Reshef1518 are based on the mutual information $I$ when considering the distribution induced by the data points $D \subset \mathbb{R}^2$ on an $x-$by$y$ grid $G$. With $I^{\ast} (D, x, y) = \max I (D|_G)$ the characteristic matrix $M$ is defined as
\begin{equation}
M(D)_{x,y} = \frac{I^{\ast} (D, x, y)}{\log \min{x,y}},
\end{equation}
where the denominator is chosen such that each entry of $M$ will take values between [0,1].
We can now define the following metrics:
    + Maximum Information Coefficient (MIC):
    \begin{equation}
    MIC(D) = \max_{xy<B(n)}\{M(D)_{x,y}\},
    \label{eq:MIC}
    \end{equation}
    where by default $B(n) = n^{0.6}$
    + Total Information Coefficient (TIC), see @JMLRv1715308:
    \begin{equation}
    TIC(D) = \sum_{xy<B(n)} M(D)_{x,y}
    \end{equation}
  While $MIC$ is normalized to fall between $[0,1]$, the upper bound of $TIC$ depends on the sample size via the bounding function $B(n)$. $TIC$ was designed to be more stable test against null hypothesis of independence, using the full information of the characteristic matrix rather than just the maximum, and as such is expected to be better suited as a PPI.
  
\textcolor{red}{What is upper bound of TIC as function of sample size? It seems from checking by putting in identical parameters as x, y that max(TIC) = 16 for 100 datapoints and max(TIC) = 148 for 1000 datapoints. Don't know how to derive this though, should depend also on c parameter and how the algorithm is approximating the true matrix?.}




```{r load}
library(tourr)
library(tidyverse)
library(scagnostics)
library(gridExtra)
library(tictoc) #timer
library(mbgraphic) #Katrins package
library(GGally)
library(geozoo)
library(minerva) #MINE indices
library(kableExtra)
library(forcats)
```


# Investigation of indices
\label{sec:investigate}

A useful projection pursuit index needs to have several properties. This has been discussed in several seminal papers (\cite{diaconis84}, \cite{huber85}, \cite{jones87}, \cite{posse95b}, \cite{hall89}). The PPI should be  minimized by the normal distribution, because this is not interesting from a data exploration perspective. If all projections are normally distributed, good modeling tools already exist. A PPI should be approximately affine invariant, regardless how the projection is rotated the index value should be the same, and the scale of each variable shouldn't affect the index value. Interestingly the original index proposed by \cite{FT74} was not rotationally invariant. A consistent index means that small perturbations to the sample do not dramatically change the index value. This is particularly important to making optimization feasible, small angles between projections correspond to small perturbations of the sample, and thus should be small changes to index value. \cite{posse95b} suggests that indexes should be resistant to features in a tail of the distribution, but this is debatable because one departure from normality that is interesting to detect are anomalous observations. Some PPI are designed precisely for these reasons. Lastly, because we need to compute the PPI over many projections, it needs to be fast to compute. These form the basis of the criteria upon which the scagnostic indexes, and the several alternative indexes are examined, as explained below.

- **smoothness**: This is the consistency property mentioned above. The index function values are examined over interpolated tour paths, that is, the value is plotted against time, where time indexes the sequence of projections produced by the tour path. The signature of a good PPI is that the plotted function is relatively smooth. The interpolation path corresponds to small angle changes between projections, so the value should be very similar. 
<!-- 
  should evolve smoothly between nearby projections, i.e. on an interpolated path we expect to observe slow and consistent change in the index function. \textcolor{red}{maybe can relate this to consistency consition developed by P. Hall, but don't understand this at the moment. In addition this relates to questions of optimiser choice and efficiency.} We evaluate smoothness both for transitions between noise variables and when moving from noise variables to an informative view in Section \ref{sec:smooth}.
-->
- **squintability**: \cite{barnett1981interpreting} introduced the idea of squint angle to indicate resolution of structure in the data. Fine structure like the parallel planes in the infamous RANDU data \cite{Marsaglia68} has a small squint angle because you have to be very close to the optimal projection plane to be able to see the structure. Structures with small squint angle are difficult to find, because the optimization algorithm needs to get very close to begin hill-climbing to the optimum. The analyst doesn't have control over the data structure, but does have control over the PPI. Squintability is about the shape of the PPI over all projections. It should have smooth low values for noise projections and a clearly larger value, ideally with a big squint angle, for structured projections. The optimizer should be able to clearly see the optimal projections as different from noise. To examine squintability, the PPI values are examined on interpolated tour paths between a noise projection and a distant structured projection. 
<!--
distribution of index values: the distribution of index values when studying data points without structure should have a low variance, and be clearly separated from index values obtained when looking at structured data distributions. \textcolor{red}{I dont really understand this part. I think it can instead be related to the concept of squint angle \cite{barnett1981interpreting}.} We show examples in Section \ref{sec:dist}
-->
- **flexibility**: An analyst can have a toolbox of indices that may cover the range of fine and broad structure, which underlies the scagnostics suite. Early indexes, based on density estimation could be programmed to detect fine or large structure by varying the binwidth. This is examined by using a range of structure in the simulated data examples. 
<!--
ability to find fine and broad structure: \textcolor{red}{there are two different considerations, one  that relates to the concept of squint angle discussed in \cite{posse95b}. On the other hand should also consider detection of "fine" structure vs noise, this would be controlled e.g. by the allowed number of bins for the MINE index functions, additionally this will impact computing times, i.e. trade accuracy for efficiency}. We address these questions in Section \ref{sec:params} with an illustrative example.-->
- **rotation invariance**: The orientation of structure within a projection plane should not change the index value. This is especially important when using the projection pursuit guided tour, because the tour path is defined between planes, along a geodesic path, not bases within planes. If a particular orientation is more optimal, this will get lost as the projection shown pays no attention to orientation. @BCAH05 describes alternative interpolation paths based on Givens and Householder rotations which progress from basis to basis. It may be possible to ignore rotation invariance with these interpolations but there isn't a current implementation, primarily because the within-plane spin that is generated is distracting from a visualization perspective. Rotation invariance is checked for the proposed PPIs by rotating the structured projection, within the plane. <!--
in the tour there is an assumed degeneracy between all projections obtained when rotating within the projected space only, for 2-d projections it means that we cannot assing x- and y- axis without ambiguity. A projection pursuit index should therefore be invariant under rotation of the basis in the projection plane. By construction this is not the case for most index functions defined above. We illustrate the impact of non invariance in Section \ref{sec:rot}. \textcolor{red}{consider frame interpolation rather than plane interpolation, which will avoid issues of rotation when interpolating, reamining issue would be optimisation efficiency.}
-->
- **speed**: Being fast to compute allows the index to be used in real-time in a guided tour, where the optimization can be watched. When the computations are shifted off-line, to watch in replay, computation times matter less. This is checked  by comparing times for benchmark scenarios with varying sample size and dimension. 
    
## Simulation study

### Data construction
\label{sec:dataOv}

Three families of data simulations are used for examining the behavior of the index functions. Each generates  structure in two variables, with the remaining variables containing various types of noise. This is a very simple construction, because there is no need for projection pursuit to find the structure, one could simply use the PPIs on pairs of variables. However, it serves the purpose to also evaluate the PPIs. The three data families are explained below. In each set, $n$ is used for the number of points, $p$ is the number of dimensions, and $d=2$ is the projection dimension. 

- **pipe**: nuisance directions are generated by sampling independently from a uniform distribution between $[-1,1]$, and the circle is generated by sampling randomly on a 2D circle, and adding a small radial noise. The circle should be easy to see by some indices because it is large structure, but the nonlinearity creates a complication. 
- **sine**:  nuisance directions are generated by sampling independently from a standard normal distribution, and the sine curve is generated by $x_p = \sin(x_{p-1}) + \mathrm{jittering}$. The sine is medium nonlinear structure, which should be visible by multiple indices.
- **spiral**:  nuisance directions are generated by sampling independently from a normal distribution, and the structure directions are sampled from an Archimedean spiral, i.e. $r = a + b \theta$, with $a=b=0.1$ and we sample angles $\theta$ from a normal distribution with mean 0 and variance $2\pi$, giving a spiral with higher densities at lower radii. The absolute value of $\theta$ fixes the direction of the spiral shape. This is fine structure which is only visible close to the optimal projection.

```{r util}
#defining index functions to be used with the tour
scagIndex <- function(scagType){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(sR[scagType])
  }
}

scagIndexNbin <- function(scagType, n){
  function(mat){
    sR <- scagnostics.default(mat[,1], mat[,2], bins = n)$s
    return(sR[scagType])
  }
}

invConvexIndex <- function(){
  function(mat){
    sR <- scagnostics.default(mat[,1],mat[,2])$s
    return(1 - sR["Convex"])
  }
}

splineIndex <- function(){
  function(mat){
    return(splines2d(mat[,1], mat[,2]))
  }
}

dcorIndex <- function(){
  function(mat){
    return(dcor2d(mat[,1], mat[,2]))
  }
}

mineIndex <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2])[[mineIndex]])
  }
}

mineIndexE <- function(mineIndex){
  function(mat){
    return(mine(mat[,1], mat[,2], est = "mic_e")[[mineIndex]])
  }
}

mineIndexAlpha <- function(mineIndex, alpha){
  function(mat){
    return(mine(mat[,1], mat[,2], alpha=alpha)[[mineIndex]])
  }
}

##rescaled holes, cmass
holesR <- function(){
  function(mat){
    ret <- tourr::holes(mat)
    if(ret<0.8) ret <- 0.
    else ret <- (ret-0.8) * 5
    return(ret)
  }
}

cmassR <- function(){
  function(mat){
    ret <- tourr::cmass(mat)
    if(ret<0.2) ret <- 0.
    else ret <- (ret-0.2) * 1.25
    return(ret)
  }
}

rescaleHoles <- function(x){
  if(x<0.8) x <- 0.
  else x <- (x-0.8) * 5
  return(x)
}

rescaleCmass <- function(x){
  if(x<0.2) x <- 0.
  else x <- (x-0.2) * 1.25
  return(x)
}


```


```{r datasetFunctions}
sphereData <- function(n, p){
  dRet <- geozoo::sphere.solid.random(n,p)
  return(as.tibble(dRet$points))
}

pipeData <- function(n, p){
  i <- 1
  dRet <- NULL
  while(i <= p){
    v <- runif(n, -1, 1)
    if(abs(v[n-1]*v[n-1] + v[n]*v[n] - 1) < 0.1){
      dRet <- rbind(dRet, v)
      i <- i+1
    }
  }
  return(as.tibble(dRet))
}

sinData <- function(n, p){
  vName <- paste0("V",n)
  vNameM1 <- paste0("V",n-1)
  expr <- paste0(vName,"=sin(",vNameM1,")") # need string expression if I want to use tibble here
  dRet <- as.tibble(matrix(rnorm((n-1)*p), ncol=(n-1))) #generate normal distributed n-1 dim data
  dRet <- mutate_(dRet, expr) #string evaluation calculates var(n) as tan(var(n-1))
  colnames(dRet)[n] <- vName #correct name of new variable
  dRet[vName] <- jitter(dRet[[vName]]) #adding noise
  return(dRet)
}

spiralData <- function(n, p){
  i <- 1
  a <- 0.1
  b <- 0.1
  dRet <- NULL
  while(i <= p){
    v <- rnorm(n-2)
    theta <- abs(rnorm(1,0,2*pi))
    r <- a + b * theta
    x <- r * cos(theta)
    y <- r * sin(theta)
    v <- c(v, x, y)
    dRet <- rbind(dRet, v)
    i <- i+1
  }
  return(dRet)
}

```

```{r datasets}
# sample(1000:9999, 1)
set.seed(3705)
spiral100 <- spiralData(6, 100) %>% scale() %>% as_tibble()
spiral1000 <- spiralData(6, 1000) %>% scale() %>% as_tibble()
#sphere100 <- sphereData(6, 100) %>% scale() %>% as_tibble()
#sphere1000 <- sphereData(6, 1000) %>% scale() %>% as_tibble()

pipe100 <- pipeData(6, 100) %>% scale() %>% as_tibble()
pipe1000 <- pipeData(6,1000) %>% scale() %>% as_tibble()

sin100 <- sinData(6, 100) %>% scale() %>% as_tibble()
sin1000 <- sinData(6, 1000) %>% scale() %>% as_tibble()
```

<!--
To get an overview of the dataset we show views of the first two directions showing the independent distribution and the last two directions showing the special view. The enforced relationships will also affect the 1-d densities as can be seen from \ref{fig:dataPlotsDensity}, which means that views including sizable fractions of $x_6$ (and $x_5$ for the Pipe and Spiral distribution) will be distinct from uninformed views of distributions 2 and 3.
-->

```{r data, fig.width=6, fig.height=7, fig.cap="Scatterplots of pairs of variables from samples of each family, showing the nuisance variables and structured variables."}
dsn <- spiral1000 %>% select(V1, V2) %>%
  rename(X1=V1, X2=V2) %>%
  mutate(family="spiral", type="nuisance")
dss <- spiral1000 %>% select(V5, V6) %>%
  rename(X1=V5, X2=V6) %>%
  mutate(family="spiral", type="structured")
dpn <- pipe1000 %>% select(V1, V2) %>%
  rename(X1=V1, X2=V2) %>%
  mutate(family="pipe", type="nuisance")
dps <- pipe1000 %>% select(V5, V6) %>%
  rename(X1=V5, X2=V6) %>%
  mutate(family="pipe", type="structured")
din <- sin1000 %>% select(V1, V2) %>%
  rename(X1=V1, X2=V2) %>%
  mutate(family="sine", type="nuisance")
dis <- sin1000 %>% select(V5, V6) %>%
  rename(X1=V5, X2=V6) %>%
  mutate(family="sine", type="structured")
d <- bind_rows(dsn, dss, dpn, dps, din, dis)
ggplot(d, aes(x=X1, y=X2)) + geom_point(alpha=0.3) + 
  facet_grid(type~family) + theme(aspect.ratio=1) +
  xlab("") + ylab("")
```

```{r indexTable, echo=FALSE}
tableTemp <- function(){
  tibble(
    index=character(),
    data=character(),
    size=numeric(),
    type=character(),
    value5=numeric(),
    value95=numeric()
  )
}

getSample <- function(dataT, size){
  if (dataT == "Pipe"){
    ret <- pipeData(p = size, n = 6) %>% scale() %>% as_tibble()
  }
  if (dataT == "Spiral"){
    ret <- spiralData(p = size, n = 6) %>% scale() %>% as_tibble()
  }
  if (dataT == "Sine"){
    ret <- sinData(p = size, n = 6) %>% scale() %>% as_tibble()
  }
  return(ret)
}

getIdx <- function(dataT, size, type){
  convex <- numeric(length = 100)
  skinny <- numeric(length = 100)
  stringy <- numeric(length = 100)
  micidx <- numeric(length = 100)
  ticidx <- numeric(length = 100)
  dcor2D <- numeric(length = 100)
  splines2D <- numeric(length = 100)
  holesI <- numeric(length = 100)
  for(i in 1:100){
    cSample <- getSample(dataT, size)
    if(type == "noise"){
      x = as.numeric(cSample$V1)
      y = as.numeric(cSample$V2)
    } else{
      x = as.numeric(cSample$V5)
      y = as.numeric(cSample$V6)
    }
    scag <- scagnostics(x=x, y=y)$s
    midx <- mine(x, y, est = "mic_e")
    skinny[i] <- scag["Skinny"]
    stringy[i] <- scag["Stringy"]
    convex[i] <- scag["Convex"]
    micidx[i] <- midx$MIC
    if (size == 100) {ticidx[i] <- midx$TIC / 16}
    if (size == 1000) {ticidx[i] <- midx$TIC / 148}
    dcor2D[i] <- dcor2d(x=x, y=y)
    splines2D[i] <- splines2d(x,y)
    holesI[i] <- holes(matrix(c(x,y), ncol = 2))
    
  }
  #sortedList <- lapply(list(skinny, stringy, convex, micidx, ticidx, dcor2D, splines2D, holesI), sort)
  skinny <- sort(skinny)
  stringy <- sort(stringy)
  convex <- sort(convex)
  micidx <- sort(micidx)
  ticidx <- sort(ticidx)
  dcor2D <- sort(dcor2D)
  splines2D <- sort(splines2D)
  holesI <- sort(holesI)
  res <- tableTemp() %>%
    add_row(index="skinny", data=dataT, size=size,
            type=type,
            value5=skinny[5], value95=skinny[95]) %>%
    add_row(index="stringy", data=dataT, size=size,
            type=type,
            value5=stringy[5], value95=stringy[95]) %>%
    add_row(index="convex", data=dataT, size=size,
            type=type,
            value5=convex[5], value95=convex[95]) %>%
    add_row(index="MICe", data=dataT, size=size,
            type=type,
            value5=micidx[5], value95=micidx[95]) %>%
    add_row(index="TICe", data=dataT, size=size,
            type=type,
            value5=ticidx[5], value95=ticidx[95]) %>%
    add_row(index="dcor2D", data=dataT, size=size,
            type=type,
            value5=dcor2D[5], value95=dcor2D[95]) %>%
    add_row(index="splines2D", data=dataT, size=size,
            type=type,
            value5=splines2D[5], value95=splines2D[95]) %>%
    add_row(index="holes", data=dataT, size=size,
            type=type,
            value5=holesI[5], value95=holesI[95])
}

if(!file.exists("cache/indexTable.rda")){
  set.seed(505)
  res <- tableTemp()
  for(ds in c("Pipe", "Sine", "Spiral")){
    for (size in c(100, 1000)){
      for (type in c("structure", "noise")){
        res <- bind_rows(res,getIdx(ds, size, type))
      }
    }
  }
  res <- res %>% arrange(data, size, index, type) %>% select(data, size, index, type, value5, value95)
  save(res, file = "cache/indexTable.rda")
} else {
  load("cache/indexTable.rda")
}
#rescale holes index first
res <- res %>%
  rowwise() %>%
  mutate(value5 = if_else(index=="holes", rescaleHoles(value5), value5)) %>%
  rowwise() %>%
  mutate(value95 = if_else(index=="holes", rescaleHoles(value95), value95))
resp <- res %>% filter(size == 1000, data == "Pipe") %>%
  select(index, type, value5, value95) %>%
  rename(lower=value5, upper=value95)
resi <- res %>% filter(size == 1000, data == "Sine") %>%
  select(index, type, value5, value95) %>%
  rename(lower=value5, upper=value95)
ress <- res %>% filter(size == 1000, data == "Spiral") %>%
  select(index, type, value5, value95) %>%
  rename(lower=value5, upper=value95)
res_all <- bind_cols(resp, resi, ress) %>%
  select(-index1, -type1, -index2, -type2) %>%
  mutate(index = factor(index, c("holes", "convex", "skinny", "stringy", "dcor2D", "splines2D", "MICe", "TICe"))) %>%
  arrange(index) %>%
  mutate(index = c("holes", "", "convex", "", "skinny", "", "stringy", "", "dcor2D", "", "splines2D", "", "MICe", "", "TICe", ""))
# Good resource for doing tables
# https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_pdf.pdf
kable(res_all, format = "latex",  caption = "Comparison of index values between noise projections and structured projections for sample size 1000, using 5th and 95th percentiles from 100 simulated sets. Most indices have much larger values for the structured projections, except for convex.", 
      col.names=c("Index","", "lower", "upper", "lower", "upper", "lower", "upper"),
      digits=2, booktabs = T) %>% 
      add_header_above(c(" ", " ", "pipe" = 2, "sine" = 2, "spiral"=2)) %>%
  group_rows("", 1, 2, latex_gap_space = "-0.5cm") %>%
  group_rows("", 3, 4, latex_gap_space = "-0.5cm") %>%
  group_rows("", 5, 6, latex_gap_space = "-0.5cm") %>%
  group_rows("", 7, 8, latex_gap_space = "-0.5cm") %>%
  group_rows("", 9, 10, latex_gap_space = "-0.5cm") %>%
  group_rows("", 11, 12, latex_gap_space = "-0.5cm") %>%
  group_rows("", 13, 14, latex_gap_space = "-0.5cm") %>%
  group_rows("", 15, 16, latex_gap_space = "-0.5cm") %>% 
  #row_spec(1:16, font_size = 10) %>%
  kable_styling(position = "center", font_size = 7) 
```

The structured projection is in variables 5 and 6 ($x_5, x_6$). Two samples sizes are used: $n=(100, 1000)$. All variables are standardized to have mean 0 and standard deviation 1. Figure \ref{fig:data} shows samples from each of the families, of the nuisance and structured pairs of variables. Table \ref{tab:indexTable} compares the PPIs for structured projections against those for nuisance variables, based on 100 simulated data sets of each type, using sample size 1000. The lower and upper show the 5th and 95th percentile of values. All indexes, except convex show distinctly higher values for the structured projections. The convex index shows the inverse scale to other indices, thus (1-convex) will be used in the assessment of performance of PPIs. The scale for the holes index is smaller than the others ranging from about 0.7 through 1, so it is re-scaled in the performance assessment so that all indices can be plotted on a common scale of 0-1 (details in Appendix). Similarly, the TIC index is re-scaled. 

### Property assessment

The procedures for assessing PPI properties of smoothness, squintability, flexibility, rotation invariance, and speed are examined for samples from the family of data sets are as follows:

<!-- - Compare the index values for structured vs nuisance variables, by simulating many examples from the model families, and recording the 90% confidence intervals. This checks if the PPI can detect the structure, that it has distinct differences between the sets of values. 
-->
1. Compute the PPI values on the tour path along an interpolation between pairs of nuisance variables, $x_1 - x_2$ view either to the $x_3 - x_4$. The result is ideally a smooth change in low values. This checks the smoothness property. 
2. Change to a tour path between a pair of nuisance variables $x_1 - x_2$ and the structured pair of variables $x_5 - x_6$, and compute the PPI along this. This examines the squintability, and smoothness. If the function is smooth and slowly increases towards the structured projection, then the structure is visible from a distance. 
3. Use the guided tour to examine the ease of optimization. This depends on having a relatively smooth function, with structure visible from a distance. One index is optimized to show how effectively the maximum is attained, and the values for other PPIs is examined along the same path, to examine the similarity between PPIs. 
4. Rotation invariance is checked by computing PPIs on rotations of the structured projection. 
5. Some PPIs have a choice of parameters, and the choice can have an effect on function smoothness, and sensitivity to structure. The parameters for these PPIs is varied and examined along a tour path between noise and structured projections, to examine the effects of parameter choices and how fiddly the index is to tune. 
6. Computational speed for the selected indices is examined on a range of sample sizes.

<!--
\textcolor{red}{\cite{posse95a} uses simulations with "strongly hidden structure", i.e. truncated Gaussian distributions with approximately identity covariance matrix, such that they don't appear in any of the principal component planes. In particular seems that spiral is hardest to identify based on results from Posse, maybe I can add this to my sphere dataset? or instead of it?}
-->


```{r dataPlotsV1V2, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the first two parameters $x_1$ and $x_2$, i.e. showing a typical uninformed view for each dataset.", eval=FALSE}
pL <- list()
i <- 1
pLabels <- c("Spiral 100", "Spiral 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(spiral100, spiral1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V1, V2)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```


```{r dataPlotsV5V6, fig.width=6, fig.height=7, fig.cap="Projection of all data sets considered onto the last two parameters $x_5$ and $x_6$, i.e. showing the special view built into the datasets.", eval=FALSE}
pL <- list()
i <- 1
pLabels <- c("Spiral 100", "Spiral 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(spiral100, spiral1000, pipe100, pipe1000, sin100, sin1000)){
  pC <- ggplot(ds, aes(V5, V6)) +
    geom_point() +
    ggtitle(pLabels[i]) + theme(aspect.ratio=1)
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

<!--
its possible to do a single figure legend - need to look up code for this
-->

```{r dataPlotsDensity, fig.width=6, fig.height=7, fig.cap="One dimensional density distribution for all parameters and all data sets considered. Apart from statistical fluctuations we see that for the Pipe datasets the parameters $x_5$ and $x_6$ are pushed towards more extreme values by vetoing points away from the circle outline, and for the Sine (and Spiral) dataset the parameter $x_6$ (and $x_5$) follows a very distinct distribution.", eval=FALSE}
pL <- list()
i <- 1
pLabels <- c("Spiral 100", "Spiral 1000", "Pipe 100", "Pipe 1000", "Sine 100", "Sine 1000")
for(ds in list(spiral100, spiral1000, pipe100, pipe1000, sin100, sin1000)){
  distData <- gather(ds, variable, value)
  pC <- ggplot(distData, aes(value, color=variable)) +
    geom_density() +
    ggtitle(pLabels[i])
  pL[[i]] <- pC
  i <- i+1
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], pL[[4]], pL[[5]], pL[[6]],
             ncol=2)
```

<!--
We next compare the various index values for three views: $x_1$ vs $x_2$, $x_5$ vs $x_6$ and $x_1$ vs $x_6$ (capturing differences from the different distribution in $x_6$). For the Spiral distribution the scagnostics indices skinny and stringy are large and close to their theoretic maximum for the special view. On the other hand while dcor2d is larger is also increased for the special view, the value is far from the theoretic maximum and larger values may be found in other views. MIC and TIC are also found to be sensitive to the spiral distribution. Considering next the Pipe distribution, we see that skinny, stringy and dcor2d are increased for the special view, but in particular MIC and TIC index are much larger and may be used to detect this type of structure in the dataset. Moreover, minimising the convex index (or maximising 1-convex) should also allow us to detect the special view. Finally the Sine distribution shows that several of the indices are maximised in the special view, in particular also the splines2d index designed for the detection of fictional dependence.
-->


## PPI traces over a tour sequence of interpolated nuisance projections
\label{sec:smooth}

Figure \ref{fig:plotV1V2toV3V4} shows the traces representing the index values calculated across a tour between a pair of nuisance projections. The range of each axis is set to be the limits of the index, as might be expected over many different data sets, 0 to 1. Each projection in the interpolation will also be noise. Two different sample sizes are show, $n=100$ as a dashed line, and $n=1000$ as a solid line. The ideal trace is a smooth function, with relatively low values, and no difference between the sample sizes. A major feature to notice is that the scagnostics produce noisy functions, which is problematic, because small changes in the projection result in big jumps in the value. This will make them difficult to optimize. TIC also is noisy. On the other hand holes, dcor2d, splines2d and MIC are relatively smooth functions. 

Several of the indexes are sensitive to sample size also, the same structured projection with differing numbers of points, produces different values. 

<!-- index values shouldn't be affected by sample size, on average it should be
same value. Maybe some need to be normalised by n. Something needs fixing here.

can we get "theoretical" min/max values, to assess the scale of the index for any particular data set.
-->

```{r getProj}
getProj <- function(df, tPath, nameStr, size){
  sc <- tibble(
    holes=numeric(),
    cmass=numeric(),
    convex=numeric(),
    skinny=numeric(),
    stringy=numeric(),
    dcor2d=numeric(),
    splines2d=numeric(),
    MIC=numeric(),
    TIC1=numeric(),
    t=numeric(),
    name=character(),
    size=numeric())
  n <- length(tPath)
  for (i in 1:n) {
    dprj <- as.matrix(df) %*% tPath[[i]]
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    cmassRes <- cmass(dprj)
    sc <- add_row(sc, holes=holesRes, cmass=cmassRes,
                  convex=scagRes[,"Convex"], skinny=scagRes[,"Skinny"],
                  stringy=scagRes[,"Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC1=mineRes$TIC, t=i, name=nameStr, size=size)
  }
  if (size == 100) {maxTIC <- 16}
  if (size == 1000) {maxTIC <- 148}
  sc <- sc %>%
    mutate(TIC = TIC1/maxTIC) %>%
    select(-TIC1)
  return(sc)
}
```

```{r V1V2toV3V4}
if(!file.exists("cache/V1V2toV3V4.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  m2 <- matrix(c(0,0,1,0,0,0,0,0,0,1,0,0),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t1 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,m2)))
  t1full <- as.list(interpolate(t1))
  fullRes <- getProj(sin100, t1full, "sin", 100) %>%
    rbind(getProj(sin1000, t1full, "sin", 1000)) %>%
    rbind(getProj(spiral100, t1full, "spiral", 100)) %>%
    rbind(getProj(spiral1000, t1full, "spiral", 1000)) %>%
    rbind(getProj(pipe100, t1full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t1full, "pipe", 1000))
  save(t1, t1full, fullRes, file = "cache/V1V2toV3V4.rda")
} else {
  load("cache/V1V2toV3V4.rda")
}
```

```{r plotV1V2toV3V4, fig.width=6, fig.height=7, fig.cap="PPIs for projections along an interpolation between two nuisance projections. All projections would be nuisance so the PPI are ideally low and smooth, with little difference between sample sizes (solid lines: $n=1000$; dashed: $n=100$). The scagnostic PPIs are noisy. Some indexes have distinct differences in values between sample sizes."}
fullResMelt <- fullRes %>%
  mutate(convex = 1-convex) %>%
  rowwise() %>%
  mutate(holes = rescaleHoles(holes)) %>%
  gather(PPI, value, -t, -name, -size) %>%
  filter(PPI != "cmass") %>%
  mutate(PPI = fct_relevel(PPI, "holes", "cmass", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
fullResMelt %>% 
  ggplot(aes(x=t, y=value)) + 
  geom_line(aes(group=size, linetype=factor(size, levels=c("1000", "100")))) + 
  ylim(c(0,1)) +
  facet_grid(PPI~name) + 
  theme(legend.position="none") + xlab("Sequence of projections (t)") + ylab("PPI value")
```


## PPI traces over a tour sequence between nuisance and structured projections
\label{sec:squintability}

Figure \ref{fig:wIntermediate} shows the PPIs for a tour sequence between a nuisance and structured projection. A long sequence is generated where the path runs from $x_1$-$x_2$, $x_1$-$x_5$, $x_5$-$x_6$, in order to see some of the intricacies of holes index. Sample size is indicated by line type: dashed being $n=100$ and solid is $n=1000$. The beginning of the sequence is the nuisance projection and the end is the structured projection. The index values for most PPIs increases substantially nearing the structured projection, indicating that they "see" the structure. Some indexes see all three structures: scagnostics, MIC and TIC,  which means that they are flexible indexes capable of detecting a range of structure. [@Grimm2016]'s indexes, cor2d and splines2d, are excellent for detecting the sine, and they can see it from far away, indicated by the long slow increase in index value. The holes index easily detects the pipe, and can see it from a distance. The scagnostic index, stringy, can see the structure but is myopic, only when it is very close. Interestingly the scagnostic, skinny, sees the spiral from a distance.  


```{r wIntermediatePath}
if(!file.exists("cache/wIntermediate.rda")){
  m1 <- matrix(c(1,0,0,0,0,0,0,1,0,0,0,0),ncol=2)
  mInt <- matrix(c(1,0,0,0,0,0,0,0,0,0,1,0),ncol=2)
  m2 <- matrix(c(0,0,0,0,1,0,0,0,0,0,0,1),ncol=2)
  #this is silly but seems that first two entries are being ignored, so need some fake entries
  m3 <- matrix(c(1,0,0,0,1,0,0,1,0,0,0,1),ncol=2)
  m4 <- matrix(c(1,1,1,0,0,0,0,1,1,0,0,0),ncol=2)
  t3 <- save_history(sin100,tour_path=planned_tour(list(m3,m4,m1,mInt,m2)))
  t3full <- as.list(interpolate(t3))
  fullResPlanned <- getProj(sin100, t3full, "sin", 100) %>%
    rbind(getProj(sin1000, t3full, "sin", 1000)) %>%
    rbind(getProj(spiral100, t3full, "spiral", 100)) %>%
    rbind(getProj(spiral1000, t3full, "spiral", 1000)) %>%
    rbind(getProj(pipe100, t3full, "pipe", 100)) %>%
    rbind(getProj(pipe1000, t3full, "pipe", 1000))
  save(t3, t3full, fullResPlanned, file = "cache/wIntermediate.rda")
} else {
  load("cache/wIntermediate.rda")
}
```

```{r wIntermediate, fig.width=6, fig.height=7, fig.cap="PPIs for projections along an interpolation between nuisance and structured projections, following $x_1$-$x_2$ to $x_1$-$x_5$ to $x_5$-$x_6$ Solid lines are the larger sample size, $n=1000$, and dashed are smaller sample size, $n=100$. Peaks at the end of the sequence indicate the index sees the structure. The scagnostics, MIC and TIC see all three structures, so are more flexible for general pattern detection. Holes only responds to the pipe, and is a multimodal function for this data with a local maximum at $x_1$-$x_5$ The scagnostics tend to be sensitive to sample size. "}
intermediatePlane <- which(attributes(interpolate(t3))$new_basis)[2]
fullResMeltPlanned <- fullResPlanned %>%
  mutate(convex = 1-convex) %>% 
  rowwise() %>%
  mutate(holes = rescaleHoles(holes)) %>%
  gather(PPI, value, -t, -name, -size) %>%
  filter(PPI != "cmass") %>%
  mutate(PPI = fct_relevel(PPI, "holes", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
fullResMeltPlanned %>% 
  ggplot(aes(x=t, y=value)) + 
  geom_line(aes(group=size, linetype=factor(size, levels=c("1000", "100")))) + 
  ylim(c(0,1)) +
  geom_vline(xintercept=intermediatePlane, color="blue", alpha=0.3) +
  facet_grid(PPI~name) + 
  theme(legend.position="none") + xlab("Sequence of projections (t)") + ylab("PPI value")
```


## Optimisation check with the guided tour
\label{sec:guided}


The guided tour combines optimization with interpolation between pairs of planes. Target planes of the path are chosen to maximize the PPI. There are three derivative-free optimization methods available in the guided tour: \verb# search_better_random# (1), \verb# search_better# (2), and \verb# search_geodesic# (3). Method 1 casts a wide net randomly generating projection planes, computing the PPIs and keeping the best projection, and method 2 conducts a localized maximum search. Method 3 is quite different: a local search is conducted to determine a promising direction, and then this direction is followed until the maximum in that direction is found. For all methods the optimization is iterative, the best projections form target planes in the tour, the tour path is the interpolation to this target, and then a new search for a better projection is made, followed by the interpolation. For each projection during the interpolation steps, the PPI is recorded. 

The stopping rule is that no better projections are found after a fixed number of tries, given a fixed tolerance value measuring difference. For method 1 and 2 two additional parameters control the optimization: the search window $\alpha$, giving the maximum distance from the current plane in which projections are sampled, and the cooling factor, giving the incremental decrease in search window size. Method 3 in principle also has two free parameters, which are however fixed in the current implementation. The first is the small step size used when evaluating the most promising direction, it is fixed to 0.01, and the second parameter being the window over which the line search is performed, fixed to $\pm \pi/4$ away from the current plane.

For distributions and indexes with smooth behavior and good squintability, method 3 is the most effective method for optimization. If these two criteria are not met the method may still be useful, but only given an informed starting projection. In such cases we can follow a method similar to that proposed by Friedman [@f87]: we break the optimization in two distinct steps. A first step ("scouting") uses method 2 with large search window and no cooling as a way of stepping over fluctuations and local maxima and yielding an approximation of the global maximum. Note that this likely requires large number of tries, especially as dimension increases, since most randomly picked planes will not be interesting. The second step is uses method 3 starting from the approximate maximum, which will take small steps to refine the result to be closer to the global maximum.


### Looking down the pipe

Despite the simple structure, the pipe is relatively difficult for the PPIs to find. For the TIC index, there is a fairly small squint angle. For the holes index, there are several local maxima, that divert the optimizer. There is a hint of this from Figure \ref{fig:wIntermediate} because the initial projection (left side of trace) of purely noise variables has a higher index value than the linear combinations of noise and structured variables along the path. The uniform distribution was used to generate the noise variables, which has a higher PPI value than a normal distribution, yielding the higher initial value. In addition, a local maximum is observed whenever the pair of variables is one structured variable and one noise variable, because there is a lighter density in the center of the projection.

The optimization is done in two stages, a scouting phase using method 2, and a refinement stage using method 3.  For the scouting we use $\alpha = 0.5$ and stopping condition of maximum 5000 tries, and we optimist the TIC index. The results are shown in Figures \ref{fig:pipeFirstRun}.

Figure \ref{fig:pipeFirstRun} shows the target projections (points)  selected during the scouting with method 2 on the TIC index. The focus is on the target projections rather than the interpolation between them, because the optimization is done off-line, and only the targets are used for the next step. The horizontal distance between the points in the plot reflects the relative geodesic distance between the planes in the 6D space. All of the other indexes are shown for interest. The TIC index value is generally low for this data, although it successfully detects the pipe. The holes, convex, skinny, and to some extent MIC, mirror the TIC performance. The holes differs in that it has some intermediate high values which are likely the indication of multi-modality of this index on this data.

The final views obtained in each of the two stages are compared in the Appendix, see Fig. \ref{fig:testpipe}.



```{r findpipe, results="hide"}
if(!file.exists("cache/findpipe.rda")){
  set.seed(1984)
  pipeResc <- as.matrix(pipe1000) 
  pipeTour <- save_history(pipeResc,
                          guided_tour(mineIndex("TIC"), search_f = tourr:::search_better,
                                     cooling=1, alpha = 0.5, max.tries = 5000))
  pipeTourFull <- as.list(interpolate(pipeTour))
  save(pipeResc, pipeTour, pipeTourFull, file = "cache/findpipe.rda")
} else {
  load("cache/findpipe.rda")
}
```

```{r eval=FALSE}
# Quick glimpse of optimisations
myholes <- function(mat) {
    n <- nrow(mat)
    d <- ncol(mat)
    num <- 1 - 1/n * sum(exp(-0.5 * rowSums(mat^2)))
    den <- 1 - exp(-d/2)
    (num/den - 0.7) * 1.43
}
quartz()
set.seed(sample(10000:50000, 1))
animate_xy(pipeResc, guided_tour(scagIndex("Skinny"), search_f = tourr:::search_better, cooling=0.9, alpha = pi, max.tries = 500))
animate_xy(pipeResc, guided_tour(mineIndex("MIC"), search_f = tourr:::search_better, cooling=1, alpha = pi, max.tries = 500))
animate_xy(pipeResc, guided_tour(mineIndex("TIC"), search_f = tourr:::search_better, cooling=1, alpha = pi, max.tries = 5000))
animate_xy(pipeResc, guided_tour(holes, search_f = tourr:::search_geodesic,
      cooling=0.8, alpha = pi, max.tries = 500))
```

```{r pipeFirstRun, fig.width=7, fig.height=7, out.width = "0.8\\textwidth", fig.cap="Scouting for the pipe using optimization method 2 on the TIC index. Other PPI values also shown for interest. Only the values for the target plane are shown because the optimization is done off-line. The TIC maximum value for this data is small,  even though it identifies the pipe. The holes, convex, skinny, and to some extent MIC, mirror the performance of TIC for this structure."}
if(!file.exists("cache/pipeFirstRun.rda")){
  pipeTourRes <- getProj(pipe1000, pipeTourFull, "Pipe", 1000)
  #pipeTourResMelt <- melt(pipeTourRes, id=c("t", "name", "size"))
  #never used?
  #save(pipeTourRes, pipeTourResMelt, file = "cache/pipeFirstRun.rda")
  save(pipeTourRes, file = "cache/pipeFirstRun.rda")
} else {
  load("cache/pipeFirstRun.rda")
}
newBasisPipeTour <- pipeTourRes %>%
  mutate(convex = 1-convex) %>% 
  rowwise() %>%
  mutate(holes = rescaleHoles(holes)) %>%
  select(-cmass) %>%
  gather(variable, value, -t, -name, -size) %>%
  filter(t %in% which(attributes(interpolate(pipeTour))$new_basis)) %>%
  mutate(variable = fct_relevel(variable, "holes", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))

ggplot(newBasisPipeTour, aes(x=t, y=value)) +
  geom_point(mapping=aes(color=(variable=="TIC"))) + 
  geom_line(mapping=aes(color=(variable=="TIC"))) + 
  scale_colour_manual(values = c("black", "red")) +
  facet_wrap(~variable, ncol=1, strip.position = "right") +
  guides(color=FALSE) +
  ylim(c(0,1)) +
  xlab("Number of projections") +
  ylab("PPI value") 
```


```{r refinepipe, results="hide"}
if(!file.exists("cache/refinepipe.rda")){
  iLast <- length(pipeTourFull)
  fProj <- pipeTourFull[[iLast]]
  pipeTour2 <- save_history(pipeResc, guided_tour(mineIndex("TIC")), start = fProj)
  pipeTourFull2 <- as.list(interpolate(pipeTour2))
  pipeTourRes2 <- getProj(pipe1000, pipeTourFull2, "Pipe", 1000)
  #pipeTourResMelt2 <- melt(pipeTourRes2, id=c("t", "name", "size"))
  #save(pipeTour2, pipeTourFull2, pipeTourRes2, pipeTourResMelt2, file = "cache/refinepipe.rda")
  save(pipeTour2, pipeTourFull2, pipeTourRes2, file = "cache/refinepipe.rda")
} else {
  load("cache/refinepipe.rda")
}
```



### Finding sine waves

Given the patterns in Figure\ref{fig:wIntermediate} it would be expected that the sine could be found easily, using only optimization method 3 using the splines2d, dcor2d, MIC or TIC indexes. This is examined in Figure \ref{fig:findsine}. Optimization is conducted using the splines2d index, and the trace of the PPI over the optimisation is shown, along with the PPI values for the other indexes over that path. The vertical blue lines indicate anchor bases, where the optimizer stops, and does a new search. The distance between anchor planes is smaller as the maximum is neared.

The only complications arise from a lack of rotation invariance of the splines2d index. It is not easily visible here, but it is possible that the best projection will have a lower PPI. The index changes depending on the basis used to define the plane, but the geodesic interpolation conducted by the tour uses any suitable basis to describe the plane, ignoring that which optimizes the PPI. This is discussed in section \ref{sec:rot}.

<!--, this will be discussed in some more details below. Here we only note that even for anchor planes highlighted by blue vertical lines the index sometimes decreases because of this dependence. These are however not dramatic effects because this search method is closely related to how interpolation is done (a different picture would be obtained using method 2 which often leads to large drops in index values after the interpolation step).-->


<!-- Here there are four index functions that appear to be suitable for the optimization: splines2d, dcor2d, MIC and TIC. On the other hand, while some of the scagnostics indices are reaching their maximum/minimum in the final projection, they do not show a smooth increase/decrease thus preventing efficient optimization when considering them as index functions. The final view identified by the guided tour is shown in Figure \ref{fig:findsine}.

\textcolor{red}{This isn't actually the "ideal" view, could be because of rotation dependence??}-->

```{r findsine, fig.width=6, fig.height=7, out.width = "0.8\\textwidth", fig.cap="Guided tour optimising  the splines2d index, using method 3, for the sine data, with $n=1000$. Anchor planes are marked by the blue vertical lines, and are closer to each other approaching the maxima. The sine is found relatively easily, by splines2D, and it si indicated that MIC, TIC, dcor2D and convex would also likely find this structure.", results="hide"}
set.seed(2018)
if(!file.exists("cache/findsine.rda")){
  sineResc <- as.matrix(sin1000)
  sineTour <- save_history(sineResc,
                          guided_tour(splineIndex()))
  sineTourFull <- as.list(interpolate(sineTour))
  sineTourRes <- getProj(sin1000, sineTourFull, "Sine", 1000)
  #sineTourResMelt <- melt(sineTourRes, id=c("t", "name", "size"))
  #save(sineResc, sineTour, sineTourFull, sineTourRes, sineTourResMelt, file = "cache/findsine.rda")
  save(sineResc, sineTour, sineTourFull, sineTourRes, file = "cache/findsine.rda")
} else {
  load("cache/findsine.rda")
}
anchorIdx <- as.tibble(which(attributes(interpolate(sineTour))$new_basis))
sineTourResMelt <-sineTourRes %>%
  mutate(convex = 1-convex) %>%
  rowwise() %>%
  mutate(holes = rescaleHoles(holes)) %>%
  select(-cmass) %>%
  gather(variable, value, -t, -name, -size) %>%
  mutate(variable = fct_relevel(variable, "holes", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
ggplot(sineTourResMelt, aes(x=t, y=value)) +
  geom_line(mapping=aes(color=(variable=="splines2d"))) + 
  facet_wrap(~variable, ncol=1, scales = "free_y") +
  scale_colour_manual(values = c("black", "red")) +
  facet_wrap(~variable, ncol=1, strip.position = "right") +
  geom_vline(data=anchorIdx, mapping=aes(xintercept=value), color="blue", alpha=0.3) +
  guides(color=FALSE) +
  ylim(c(0,1)) + xlab("Number of projections") + ylab("PPI value")
```

<!--
Di says probably we can skip this plot
```{r testsine, out.width=".5\\textwidth", fig.cap="Final view identified by the optimisation."}
iLast <- length(sineTourFull)
fProj <- sineTourFull[[iLast]]
dProj <- as.tibble(sineResc %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point()
```
-->

### Spiral detection

The spiral is the most challenging structure to detect because it has a small squint angle [@posse95a], especially as the ratio of noise to structure dimensions increases. This is explored in this section, using optimisation method 2 to scout the space for approximate maxima first. The skinny scagnostic index is used because it was observed (Figure \ref{fig:wIntermediate}) to be sensitive to this structure, although the noisiness of the index might be problematic. The stringy appears to be more sensitive to the spiral, but it has a much small squint angle. 

<!-- Here we explore the role of dimensionality in the index optimization, using the skinny index that was found to be sensitive to this structure.
We use a setup similar to that introduced for detecting the pipe structure, i.e.\ we use the search\_better optimization of the guided tour, disabling the cooling and with a large search window, while sampling a large number of random projections (up to 5000) for each step.
-->

The search is conducted for $p=4,5,6$ which would correspond to 2, 3 and 4 noise dimensions respectively. In addition we examine the distance between planes, using a Frobenius norm, as defined by Equation 2 of @BCAH05, and available in the ```proj_dist``` function in the tourr package, to compare searches with squint angle across dimensions. The distance between planes is related to squint angle, how far away from the ideal projection, can the structure be glimpsed. In order for the optimizer to find the spiral, the distance between planes would need to be smaller than the squint angle. Figure \ref{fig:findspiral} summarizes the results. When $p=4$ the scouting method effectively finds the spiral. Plot (a) shows the side-by-side boxplots of distance between ideal plane and planes examined during the optimization, for $p=4,5,6$. These are on average smaller for the lower dimension, and gradually increase as dimension increases. This is an indication of the extra computation needed to brute force find the spiral as noise dimensions increase. Plot (b) shows these as a function of the optimization procedure, where it can be seen that only when $p=4$ does it converge to the ideal. Its likely that expanding the search space should result in uncovering the spiral in higher dimensions.

<!-- We consider distributions in d=4,5,6 dimensions (subsets of the spiral dataset) and compare features found in the tour.
In d=4 dimensions the optimization identifies a view showing the spiral after 39 steps. For larger values of d the optimization does not uncover the spiral view, and we limit the optimizer to 100 steps to compare the so obtained bases to the results found in four dimensions.
Results are shown in the top panel Figure \ref{fig:findspiral}. The top left is showing the pairwise distances between all planes found by the optimization, note that while d=5,6 contains 100 planes, d=4 only contains 39 planes. The top right is then comparing the evolution of the distance from the plane containing the spiral view with progression of the optimization. We see that the pairwise distances grow with d (what do we learn from that?). Moreover we find that the distance to the special view is far from monotonically decreasing towards zero, and large jumps are possible as a consequence of rotation dependence. Note that the spiral itself is approximately rotation invariant, but partial views are not. At medium distances from the special view we often observe a jump to higher distances as consequence of in-plane rotation dependence for such partial views. This stabilizes closer to the special view, however it relies on randomly identifying a projection within this angle. As a consequence increasing dimensionality dramatically reduces the chance of identifying structures with high index values.-->


```{r findspiral, out.width="\\textwidth", fig.cap="Guided tour optimising the skinny index for the Sprial dataset with 1000 datapoints, with p = 4, 5, 6. The top left plot shows the distribution of pairwise distances between planes obtained via the guided tour, the top right shows the evolution of distance to the plane containing the spiral as the index is being optimised. The bottom row shows the final view obtained for p = 4 (left), p = 5 (middle) and p = 6 (right).", results="hide"}

distanceDist <- function(planes, nn=F){
  #nn could be used to turn on only distance to nearest neighbour?
  planes <- as.list(planes)
  maxI <- length(planes)
  distL <- numeric(choose(maxI, 2))
  i <- 1
  idx <- 1
  while(i<maxI+1){
    j <- i+1
    while(j<maxI+1){
      cDist <- proj_dist(planes[[i]], planes[[j]])
      distL[idx] <- cDist
      j <- j+1
      idx <- idx+1
    }
    i <- i+1
  }
  return(distL)
}

distanceToSp <- function(planes, specialPlane){
  planes <- as.list(planes)
  maxI <- length(planes)
  distL <- numeric(maxI)
  i <- 1
  while(i<maxI+1){
    cDist <- proj_dist(planes[[i]], specialPlane)
    distL[i] <- cDist
    i <- i+1
  }
  return(distL)
  }

if(!file.exists("cache/findspiral.rda")){
  distDf <- tibble(d=numeric(), distance=numeric())
  distToSp <- tibble(d=numeric(), distance=numeric(), t=numeric())

  maxBases <- 100
#  maxBases <- 1000
  fViewL <- list()
  allPlanes <- list()
  i <- 1
  for(d in c(4,5,6)){
  #d = 6
    set.seed(58958)
    specialPlane <- matrix(c(rep(0,d-2),1,0,rep(0,d-1),1),ncol=2)
    spiralD <- spiral1000 %>%
      purrr::when(d<6 ~ select(., -V4), ~ .) %>%
      purrr::when(d<5 ~ select(., -V3), ~ .)
    spiralTour <- save_history(as.matrix(spiralD),
                               guided_tour(scagIndex("Skinny"), 
                                  search_f = tourr:::search_better,
                                  max.tries = 5000, 
                                  #max.tries = 50000, 
                                  cooling = 1, alpha = pi),
                              max_bases = maxBases)
    #if(d==4){maxBases <- min(maxBases, length(spiralTour))}
    nBases <- length(spiralTour)
    distDf <- rbind(distDf, tibble(d=d, distance=distanceDist(spiralTour)))
    distToSp <- rbind(distToSp, tibble(d=d, distance=distanceToSp(spiralTour, specialPlane), t=1:nBases))
    fView <- as.tibble(as.matrix(spiralD) %*% as.list(spiralTour)[[nBases]])
    fViewL[[i]] <- ggplot(fView, aes(V1, V2)) + geom_point()
    allPlanes[[i]] <- spiralTour
    i <- i+1
  }
  save(distDf, distToSp, fViewL, allPlanes, file = "cache/findspiral.rda")
} else {
  load("cache/findspiral.rda")
}

distDf <- distDf %>%
  mutate(p=d) %>%
  select(-d)

distToSp <- distToSp %>%
  mutate(p=d) %>%
  select(-d)

p1 <- ggplot(distDf, aes(group=p, y=distance, x=p)) +
  geom_boxplot() +
  scale_x_continuous(breaks = c(4, 5, 6)) + ggtitle("(a)")
p2 <- ggplot(distToSp, aes(color=as.factor(p),x =t, y=distance)) +
  geom_point() +
  labs(y = "Distance to special view", color = "p") + ggtitle("(b)")


fViewL[[1]] <- fViewL[[1]] + xlab("PP1") + ylab("PP2")
fViewL[[2]] <- fViewL[[2]] + xlab("PP1") + ylab("PP2")
fViewL[[3]] <- fViewL[[3]] + xlab("PP1") + ylab("PP2")

grid.arrange(p1, p2, fViewL[[1]], fViewL[[2]], fViewL[[3]], layout_matrix = rbind(c(1,2,2),c(3,4,5)))

```


```{r squintAngle, out.width="0.5\\textwidth", fig.cap="Estimated squint angles for the Spiral dataset with 1000 datapoints, with p = 4, 5, 6, containing estimates evaluated for 100 randomly selected directions each.", results="hide"}

squintAngleEstimat <- function(data, indexF, cutoff, structurePlane, n = 100, stepSize = 0.01){
  # data = numerical input with p > 2 dimensions
  # indexF = the index function used to estimate the squint angle
  # cutoff = lower bound on the index value to be considered structured (i.e. all lower index values are outside the squint angle)
  # structuredPlane = projection matrix onto plane containing the 2-d structure
  # n = number of random starting directions over which the squint angle estimate is averaged
  # stepSize = accuracy, step size determines where the index is evaluated to find the first plane above cutoff
  data <- as.matrix(data) # make sure data is in matrix format
  angles <- numeric(length = n) # collecting all individual estimates
  i <- 1
  p <- ncol(data)
  while(i <= n){
    # first generate random direction, make sure it is not too close to structure plane
    dist <- 0.
    while(dist < 0.1){
      rBasis <- tourr::basis_random(p)
      dist <- tourr::proj_dist(rBasis, structurePlane)
    }
    # now interpolate from rBasis to structure plane with selected step size
    # since planned tour ignores first two entries, generate some random planes to be ignored
    notUsed1 <- tourr::basis_random(p)
    notUsed2 <- tourr::basis_random(p)
    tourHist <- save_history(data,tour_path=planned_tour(list(notUsed1, notUsed2, rBasis, structurePlane)))
    allBases <- as.list(interpolate(tourHist, angle = stepSize))
    cIndex <- 0.
    j <- 1
    while(cIndex < cutoff){
      cProj <- data %*% allBases[[j]]
      cIndex <- indexF(cProj)
      j <- j+1
    }
    cDist <- tourr::proj_dist(allBases[[j]], structurePlane)
    angles[i] <- cDist
    i <- i+1
  }
  return(angles)
}

if(!file.exists("cache/squintAngle.rda")){
  allAngles <- tibble(d=numeric(), angle=numeric())
  for(d in c(4,5,6)){
    set.seed(58958)
    specialPlane <- matrix(c(rep(0,d-2),1,0,rep(0,d-1),1),ncol=2)
    spiralD <- spiral1000 %>%
      purrr::when(d<6 ~ select(., -V4), ~ .) %>%
      purrr::when(d<5 ~ select(., -V3), ~ .)
    cAngles <- squintAngleEstimat(spiralD, scagIndex("Skinny"), 0.6, specialPlane)
    allAngles <- bind_rows(allAngles, tibble(d=d, angle=cAngles))
  }
  save(allAngles, file = "cache/squintAngle.rda")
} else {
  load("cache/squintAngle.rda")
}

allAngles <- rename(allAngles, p=d)

ggplot(allAngles, aes(p, angle, group=p)) + geom_boxplot()
```

## Rotational invariance or not
\label{sec:rot}

Rotational invariance is examined using the sine data ($x_5$-$x_6$), computing PPI for different rotations within the 2D plane, parametrized by angle. Results are shown in Figure \ref{fig:rotationDep}. Several indexes are invariant, holes, convex and MIC, because their value is constant around rotations. The dcor2d, splines2d and TIC index are clearly not rotationally invariant because the value changes depending on the rotation. The scagnostics indexes are approximately rotationally invariant, but particularly the skinny index has some random variation depending on rotation. 

```{r rotationDep, out.width=".6\\textwidth", fig.cap="PPI for rotations of the sine 1000 data, to examine rotation invariance. Most are close to rotation invariant, except for dcor2d, splines2d and TIC."}
if(!file.exists("cache/rotationDep.rda")){
  sineM <- as.matrix(select(sin1000, V5, V6))
  sc <- tibble(
      holes=numeric(),
      cmass=numeric(),
      convex=numeric(),
      skinny=numeric(),
      stringy=numeric(),
      dcor2d=numeric(),
      splines2d=numeric(),
      MIC=numeric(),
      TIC=numeric(),
      alpha=numeric()
  )
  for (a in seq(0,2*pi, pi/100)){
    rotM <- matrix(c(cos(a), sin(a), -sin(a), cos(a)), ncol = 2)
    dprj <- sineM %*% rotM
    scagRes <- scagnostics(dprj)
    dcorRes <- dcor2d(dprj[,1], dprj[,2])
    splineRes <- splines2d(dprj[,1], dprj[,2])
    mineRes <- mine(dprj[,1], dprj[,2])
    holesRes <- holes(dprj)
    cmassRes <- cmass(dprj)
    sc <- add_row(sc, holes=holesRes, cmass=cmassRes,
                  convex=scagRes[,"Convex"], skinny=scagRes[,"Skinny"],
                  stringy=scagRes[,"Stringy"], dcor2d=dcorRes,
                  splines2d=splineRes, MIC=mineRes$MIC,
                  TIC=mineRes$TIC/148, alpha=a)
  }
  save(sc, file = "cache/rotationDep.rda")
} else {
  load("cache/rotationDep.rda")
}

scMelt <- sc %>% 
  mutate(convex = 1-convex) %>%
  rowwise() %>%
  mutate(holes = rescaleHoles(holes)) %>%
  rowwise() %>%
  mutate(cmass = rescaleCmass(cmass)) %>%
  mutate(angle=alpha*360/(2*pi), TIC = TIC) %>%
  select(-alpha) %>%
  gather(PPI, value, -angle) %>%
  mutate(PPI = fct_relevel(PPI, "holes", "cmass", "convex", "skinny", "stringy", "dcor2d", "splines2d", "MIC", "TIC"))
ggplot(scMelt, aes(x=angle, y=value)) +
  geom_line() + 
  ylim(c(-1, 1.1)) +
  facet_wrap(~PPI, ncol = 3) +
  #geom_blank(aes(y = 0)) +
  #geom_blank(aes(y = 1)) +
  coord_polar() +
  xlab("") + ylab("")
```



## Computational performance

\label{sec:timer}
Computational time is important for using the PPIs with the guided tour, online. Figure \ref{fig:timer} summarizes performance for each PPI. For simplicity, data with sample sizes ranging from 100-10000 are drawn from a 6-d solid sphere, using the geozoo package [@geozoo].  The time to compute the PPIs over 100 interpolated grand tour projections is recorded. The scagnostics PPI are computed as a bundle, since this is the code base, and that major computational constraint is common to all the scagnostics. There are two versions of the MIC and TIC algorithm, labelled MINE and MINE E, the second being a newer algorithm which improves their computational performance. 

The results are interesting. The scagnostic indexes and splines2D are very fast regardless of sample size. MIC, TIC (both versions) and dcor2D slow rapidly as sample size increases.


```{r getTimer}

mineE <- function(x,y){
  return(mine(x=x, y=y, est = "mic_e"))
}


timeThis <- function(d, t, idx, pmax, n, idxName){ #d=data matrix, t=interpolated tour path, idx=index function, pmax=max number of projections, n=sample size, idxName=str name of idx funciton
    i <- 1
    dfTimer = data.frame( t= numeric(), i=numeric(), n = numeric(), name=character())
    for(pMatrix in t){
      if(i>pmax) break
      tic.clearlog()
      tic() #start timer
      dProj <- d %*% pMatrix
      sgnst <- idx(dProj[,1],dProj[,2])
      toc(log=TRUE,quiet=TRUE)
      scTd <- unlist(tic.log(format=FALSE))["toc.elapsed"]-unlist(tic.log(format=FALSE))["tic.elapsed"]
      dfTimer <- add_row(dfTimer, t=scTd, i=i, n= n, name=idxName)
      i = i+1
    }
    return(dfTimer)
}

```


```{r timer, fig.cap="Computational perfoamce for PPIs, using sample sizes 100-10000. Colour indicates the PPI. Because the scagnostics calculation is bundled together, the values are the same for all these indexes, and they are really fast to compute. MINE includes the MIC and TIC indexes, and MINE E are computationally more efficient algorithms for these. These, along with dcor2D, are slower with larger sample sizes.", out.width=".6\\textwidth"}
if(!file.exists("cache/timer.rda")){
  set.seed(2018)
  grandTour100 <- save_history(sphereData(6,100), grand_tour(2), max=4) %>%
    interpolate() %>%
    as.list()

  sizeL <- c(10, 100, 500, 1000, 5000, 10000)
  t <- 1
  dfTimer <- NULL
  for(sampleSize in sizeL){
    set.seed(sampleSize + 11) # new seed for each sample size
    if(sampleSize == 0) sampleSize = 10 # smallest considered sample has 10 points
    thisMatrix <-  sphereData(6, sampleSize) %>% rescale()
    scagT <- timeThis(thisMatrix, grandTour100, scagnostics.default, 100, sampleSize, "scagnostics")
    dcorT <- timeThis(thisMatrix, grandTour100, dcor2d, 100, sampleSize, "dcor2D")
    splineT <- timeThis(thisMatrix, grandTour100, splines2d, 100, sampleSize, "splines2D")
    mineT <- timeThis(thisMatrix, grandTour100, mine, 100, sampleSize, "MINE")
    mineeT <- timeThis(thisMatrix, grandTour100, mineE, 100, sampleSize, "MINE E")
    dfTimerC <- list(scagT, dcorT, splineT, mineT,mineeT) %>%
      reduce(rbind)
    dfTimer <- rbind(dfTimer, dfTimerC)
    t <-  t+1
  }
  save(dfTimer, file = "cache/timer.rda")
} else {
  load("cache/timer.rda")
}

timerMeans <- dfTimer %>%
  group_by(name, n) %>%
  summarise(t=mean(t))

ggplot(dfTimer, aes(x = n, y = t)) + 
  geom_point(alpha=0.1, aes(color=name, shape=name)) +
  geom_point(data=timerMeans, aes(color=name, shape=name)) +
  geom_line(data=timerMeans, aes(linetype=name, color=name)) +
  scale_x_log10(limits=c(90,10100)) +
#  scale_y_log10(limits=c(0.00001,1)) + 
  labs(x = "Sample Size", y = "Time [s]", color = "Index family", 
       linetype = "Index family", shape = "Index family") + 
  scale_colour_brewer(palette="Dark2")
```


## Effect of parameter choice in index value
\label{sec:params}
Some parameters must be provided for some PPIs. This can be advantageous, allowing the index to more flexibly work for different types of structure, controlling trade-offs between noise and fine structure detection, and affecting computing time and precision.

- Binning:
    - Scagnostics: the number of bins can be controlled by the user, note however that internally the implementation will reduce the number of bins if too many non-empty bins are found (more than 250).
    - MINE: the maximum number of bins considered is fixed by the user as a function of the number of data points, see Equation \ref{eq:MIC}. The default is chosen such as a trade-off between resolution and noise dependence, but it may be tuned based on requirements dictated by specific datasets. Apart from sensitivity to noise computing time may also be a consideration here.
- Spline knots: for the splines2D measure we need to fix the number of knots. By default it is fixed to be 10 (or lower if appropriate based on the data values). In our examples we find the number to be appropriate to identify functional dependence while rejecting noise, but some distributions may require tuning of this parameter.

<!--### Effect of binning in scagnostics -->

The bins argument for the scagnostics might be reasonably expected to affect the smoothness of the index: a small number of bins should provide a smooth index function, but may affect its ability to detect fine structure. Figure \ref{fig:sprialScagBinning} examines this. Scagnostics PPIs are computed for the spiral1000 data on a tour path between $x_1$-$x_2$ to $x_5$-$x_6$, for number of bins equal to 10, 20, 50. The interesting observation to make is that even with small bin size the functions are all relatively noisy. The problem with the small bin size is that the spiral becomes invisible to the PPI. 

XXX from caption, remove?:  For too rough binning (bins=10) we observe random fluctuations from the binning, with structure detedction being obscured. With bins=20 we see somewhat smoother behaviour for the stringy index, especially when moving into the maximum, while the skinny index shows a dip in the final view, and larger bins=50 is preferred. For the convex index we also find bins=50 to be the better choice.

```{r spiralScagBinning, fig.cap="Comparing the traces of the three scagnostics indices when changing the binning via the bins parameter set to 10, 20 and 50 in this example.", out.width=".7\\textwidth"}
getScagComp <- function(df, tPath){
  sc <- tibble(
    convex10=numeric(),
    skinny10=numeric(),
    stringy10=numeric(),
    convex20=numeric(),
    skinny20=numeric(),
    stringy20=numeric(),
    convex50=numeric(),
    skinny50=numeric(),
    stringy50=numeric(),
    t=numeric())
  n <- length(tPath)
  for (i in 1:n) {
    dprj <- as.matrix(df) %*% tPath[[i]]
    scagRes10 <- scagnostics.default(dprj[,1], dprj[,2], bins=10)$s
    scagRes20 <- scagnostics.default(dprj[,1], dprj[,2], bins=20)$s
    scagRes50 <- scagnostics.default(dprj[,1], dprj[,2], bins=50)$s
    sc <- add_row(sc,
                  convex10=scagRes10["Convex"], skinny10=scagRes10["Skinny"],
                  stringy10=scagRes10["Stringy"],
                  convex20=scagRes20["Convex"], skinny20=scagRes20["Skinny"],
                  stringy20=scagRes20["Stringy"],
                  convex50=scagRes50["Convex"], skinny50=scagRes50["Skinny"],
                  stringy50=scagRes50["Stringy"],
                  t=i)
  }
  
  return(sc)
}
if(!file.exists("cache/spiralScagBinning.rda")){
  scagBinningDf <- getScagComp(spiral1000, t2full)
  scagMelt <- gather(scagBinningDf, variable, value, -t) %>%
    mutate(nbin = as.numeric(str_sub(variable, start = -2))) %>%
    mutate(idx = str_sub(variable, end = -3))
  save(scagBinningDf, scagMelt, file = "cache/spiralScagBinning.rda")
} else {
  load("cache/spiralScagBinning.rda")
}
ggplot(scagMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(nbin))) + 
  facet_wrap(~idx, ncol=1, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  scale_colour_discrete("Num bins") + xlab("Number of projections") + 
  ylab("PPI value")

```

The sensitivity to the binning for calculating the MIC index is examined for one example in the Appendix XXX (add cross reference)


## Ways to refine the PPIs

The biggest issues revealed by the investigation into the new PPIs are a lack of smoothness, particularly for the scagnostics indexes, and the rotation invariance of Grimm's indexes. To fix the smoothness of an index function, it is possible to calculate the PPI for a small neighbourhood of projections and average the value, or alternatively average the PPI for several jittered projections. This is investigated in Fig. \ref{fig:spiralScagSmoothing}. Rotation invariance is more difficult to fix, but an alternative tour interpolation method could be useful. The geodesic interpolation transitions between planes, and it ignores the basis defining the plane, creating a problem with rotation invariant indices. Alternative interpolations based on Givens or Householder rotations could be implemented to transition between bases, which should alleviate the need for rotationally invariant indices. 

<!-- ### Smoothing-->

Two different methods are considered for smoothing the index values:

* Jittering points: using the jitter function we move each points by a random amount drawn from a uniform distribution between $\pm \beta$.
* Jittering angles: using the tourr implementation we can draw a random plane and move some small amount $\epsilon$ in that direction.

<!--
* Turning axes: given the rotation non-invariance, maybe rotation of basis could be useful also??
-->

The mean value from a sample of projections is recorded as the PPI value. This could be robsitufied by dropping the most extreme values. 

This is particularly interesting for the scagnostics indexes skinny and stringy which we found to be most noisy among the indexes considered. Figure \ref{fig:spiralScagSmoothing} studies the potential of these two smoothing approaches, using a the tour path between noise variables of the spiral1000 dataset using different $\epsilon$ and $\beta$ values. Both methods appear to be promising in smoothing the function. Because the scagnistics are fast to compute, either of these methods is feasible. For this example we have smoothed over 10 randomly selected jittered views, computing time increases linearly with the number of randomly jittered views, as this is mostly determined by the time needed to evaluate the scagnostics indexes which is done separately for each view.

<!--\textcolor{red}{I fixed the y axis to be between 0 and 1, which highlights that indeed we get much smoother behaviour, on the other hand theres still large relative jumps observed in all traces, and a smaller y axis scale may be preferred to show this more clearely.}-->

```{r spiralScagSmoothing, fig.cap="Comparing the traces of the scagnostics indices skinny and stringy when smoothing the index values, either by averaging over the index value after jittering the projection by some angle $\\epsilon$ (full line) or after jittering the projected datapoints with some amount $\\beta$ (dashed line). While the behaviour can be somewhat improved, sharp jumps can be observed in all settings considered. For comparison the red line in the background shows the trace without any smoothing applied."}

jitterScagnostics <-function(proj, d, alpha){
  newProj <- tourr:::basis_nearby(proj, alpha = alpha, method = "geodesic")
  newD <- d %*% newProj
  return(as.vector(scagnostics.default(newD[,1],newD[,2])$s))
}

jitterPointsScagnostics <-function(projData, alpha){
  newD <- jitter(projData, amount=alpha)
  return(as.vector(scagnostics.default(newD[,1],newD[,2])$s))
}


getSgnMean <- function(proj, d, alpha, method){
  dProj <- d %*% proj
  orig <- as.vector(scagnostics.default(dProj[,1],dProj[,2])$s)
  if(method == "jitterAngle"){
    sgnVec <- replicate(10, jitterScagnostics(proj, d, alpha))
  } else if (method=="jitterPoints"){
    sgnVec <- replicate(10, jitterPointsScagnostics(dProj, alpha))
  }
  return(rowMeans(cbind(orig, sgnVec)))
}

getScagSmooth <- function(df, tPath){
  sc <- tibble(
    convex=numeric(),
    skinny=numeric(),
    stringy=numeric(),
    t=numeric(),
    method=character(),
    alpha=numeric())
  n <- length(tPath)
  for (method in c("jitterAngle", "jitterPoints")){
    for (alpha in c(0.01, 0.05, 0.1)){
      for (i in 1:n) {
        scagMean <- getSgnMean(tPath[[i]], as.matrix(df), alpha, method)
        convex <- scagMean[6]
        skinny <- scagMean[7]
        stringy <- scagMean[8]
        sc <- sc %>% add_row(convex=convex, skinny=skinny, stringy=stringy,
                             t=i, method=method, alpha=alpha)
      }
    }
  }
  
  return(sc)
}

if(!file.exists("cache/spiralScagSmoothing.rda")){
  scagSmooth <- getScagSmooth(spiral1000, as.list(t1full))
  scagSmoothMelt <- gather(scagSmooth, variable, value, -t, -method, -alpha)
  save(scagSmooth, scagSmoothMelt, file = "cache/spiralScagSmoothing.rda")
} else {
  load("cache/spiralScagSmoothing.rda")
}

#getting non-smoothed result
load("cache/V1V2toV3V4.rda")
noSmoothing <- fullRes %>%
  filter(size==1000) %>%
  filter(name=="spiral") %>%
  select(skinny, stringy, t) %>%
  gather(variable, value, -t)

scagSmoothMelt <- scagSmoothMelt %>%
  filter(variable != "convex") %>%
  mutate(method = factor(method, levels = c("jitterAngle", "jitterPoints")))
ggplot(scagSmoothMelt, aes(x=t, y=value)) +
  geom_line(data=noSmoothing, color="red", alpha=0.5) +
  geom_line(aes(linetype=factor(method))) + 
  theme(legend.position="none") +
  ylim(0,1) +
  facet_grid(alpha~variable) +
  xlab("Sequence of projections (t)") +
  ylab("PPI value")

```


## Summary

Our results can be summarized by evaluating and comparing the advantages and disadvantages of each index function according to the criteria presented above. An overview is given in Table \ref{tab:summary}. The first three entries are from the scagnostics family. The advantages are good scaling of computing time with sample size and that the set of index functions covers general signatures, making the set flexible in applications (the same is however not true for each single index viewed on its own). On the other hand we have found that they are typically too noisy making optimization of the function challenging, which also results in poor squint ability. In particular the stringy index cannot be used with current optimizers. The index functions from @mbgraphic share similar behavior as they are both found to evolve fairly smoothly between interpolated projections, but have been shown to be strongly rotation dependent. The splines2D index is fast to calculate and found to be the most efficient method for identifying functional dependence. We have only considered the non-binned implementation of dcor2D which is slow to calculate on large datasets. While this index is in principle general, we have found it to be too sensitive to noise to be applicable in most realistic examples, therefore we consider flexibility limited. Finally the MINE indexes are found to be smooth only for large datasets, with some rotation dependence observed for the TIC index. While these indexes are slow to compute especially on large datasets, we have found them to be the most general, allowing good discrimination even with noisy structures. In addition we observed good squintability for all three examples considered, given large datasets. Results in Section \ref{sec:params} suggest that tuning of the MINE parameters may result in improved behavior for smaller datasets.

\begin{table}
\begin{center}
\caption{Summary of findings, showing to what extend the considered index functions pass the criteria for a good PPI. "\checkmark" symbols good behavior, "$\cdot$" symbols some issues while "$\times$" symbols failure on the corresponding criteria. Each index has particular strengths and drawbacks and selection must be guided by the considered example, see text for details.}
\label{tab:summary}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Index & smooth & squintability & flexible & rotation invariant & speed \\
\hline
Convex & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & \checkmark \\
Skinny & $\cdot$ & $\cdot$ & $\cdot$ & $\cdot$ & \checkmark \\
Stringy & $\times$ & $\times$ & $\cdot$ & $\cdot$ & \checkmark \\
\hline
splines2D & \checkmark & \checkmark & $\cdot$ & $\times$ & \checkmark \\
dcor2D & \checkmark & \checkmark & $\cdot$ & $\times$ & $\cdot$ \\
\hline
MINE & $\cdot$ & \checkmark  & \checkmark & $\cdot$ & $\cdot$ \\
\hline
\end{tabular}
\end{center}
\end{table}




# Application to physics data
\label{sec:phys}

This section describes the application of these projection pursuit indices to find two-dimensional structure in two multidimensional gravitational waves problems. 

The first example shows 2538 posterior samples obtained by fitting source parameters to the observed gravitational wave signal GW170817 from a neutron star merger [@PhysRevLett119161101]. Data has been downloaded from @ligoData. The fitting procedures are described in detail in @Abbott2018exr. There are six parameters of physical interest (6D) with some known relationships. Projection pursuit is used to find the known relationships. 

The second example shows data from @Smith:2016qas, generated from a simulation study of a binary black hole (BBH) merger event. (XXX More info needed.) There are 12 parameters (12-D), with multiple nuisance parameters. Projection pursuit uncovers several new relationships between parameters. 

## Neutron star merger

Figure \ref{fig:neutronStarSPLOM} in the Appendix shows the scatterplot matrix (with transparency) of the six parameters. (In astrophysics, scatterplot matrices are often called "corner plots" [@corner].) The diagonal shows a univariate density plot of each parameter, and the upper triangle of cells displays the correlation between variables. From this it can be seen that m1 and m2 are strongly, and slightly, nonlinearly associated. Between the other variables, some linear association (R1, R2), some nonlinear association (L1, L2, R1, R2),  heteroskedastic variance in most variables and some bimodality (R1, L1, L2, m1, m2). 

The model describes a neutron star merger and contains 6 free parameters, with each neutron star described by its mass $m$ (m1, m2) and radius $R$ (R1, R2), and a so-called tidal deform ability parameter $\Lambda$ (L1, L2) which is a function of the mass and radius, approximately proportional to $(m/R)^{-5}$. 

### Data pre-pocessing

Because m1 and m2 are very strongly associated, m2 is dropped before doing PP. This relationship is obvious from the scatterplots of pairs of variables and does not need to be re-discovered by PP. 

All variables are scaled to range between 0 and 1. The purpose is that range differences in individual variables should not affect the detection  of relationships between multiple variables. Standardising the range will still leave differences between the standard deviations of the variables, and for this problem this is preferred. Differences in the standard deviations can be important for keeping the non-linear relationships visible to PP.  

### Applying PP

With only five parameters, a reasonable start is to examine the 5D space using a grand tour. This quickly shows that the strong nonlinear relationships between the parameters. PP is then used to extract these relationships. The best index for this sort of problem is the splines2d, and it is fast to compute. 

Figure \ref{fig:nsePlotOrig} shows the optimal projection found by splines2d, a reconstructed view obtained by manually combining parameters, and a plot of the known relationship between parameters. 




```{r neutronStarEq,  results="hide"}
nsD <- read_csv("data/samples.csv") 
nsM <- nsD %>%
  select(-m2) %>% # remove m2 to avoid finding well known correlation
  rescale() # rescaling, this returns a matrix
if(!file.exists("cache/neutronStarEq.rda")){
  set.seed(2018)
  nseTour <- save_history(nsM, guided_tour(splineIndex()))
  nseTourFull <- as.list(interpolate(nseTour))
  save(nseTour, nseTourFull, file = "cache/neutronStarEq.rda")
} else {
  load("cache/neutronStarEq.rda")
}
iLast <- length(nseTourFull)
fProj <- nseTourFull[[iLast]]
dProj <- as.tibble(nsM %*% fProj)
colnames(fProj) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj <- as_tibble(fProj) %>%
  add_column(label=colnames(nsM))
```

```{r axesDrawing}
#select cntrx, cntry for placement
#sclx and scly should be 20% of overall range
getCircle <- function(cntrx, cntry, sclx, scly){
  theta <- seq(0, 2 * pi, length = 50)
  circ <- tibble(x=cos(theta), y=sin(theta))
  circ <- circ %>% 
    mutate(x=x*sclx+cntrx, y=y*scly+cntry)
}
getAxes <- function(fProj, cntrx, cntry, sclx, scly){
  x1 <- rep(cntrx, length(fProj$PP1))
  y1 <- rep(cntry, length(fProj[,1]))
  x2 <- fProj$PP1*sclx+cntrx
  y2 <- fProj$PP2*scly+cntry
  lab <- fProj$label
  axes <- tibble(x1=x1, y1=y1, x2=x2, y2=y2, label=lab)
}
```

```{r nsePlotOrig, fig.cap="Comparison of guided tour final view (left), approximation based on original parameters (middle) and expected relation based on analysis setup (right)."}
p1 <- ggplot(dProj, aes(PP1, PP2)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0,0.6,0.32,0.22), aes(x=x, y=y), color="grey") + 
  xlim(-1.1, 0.5) + ylim(-0.2, 0.9) +
  geom_segment(data=getAxes(fProj, 0,0.6,0.32,0.22), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, 0,0.6,0.32,0.22), aes(x=x2, y=y2, label=label), size=2.5)
p2 <- ggplot(nsD, aes(R1-2*pi*m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
p3 <- ggplot(nsD, aes(R1/m1, L1)) + geom_point(alpha = 0.05) + theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3) 
```

To further investigate relationships between parameters, $L1$ is removed and PP with the splines2D is applied to the remaining four parameters. The dependence of $L2$ on the mass and radius of the lighter neutron star, is revealed (Figure \ref{fig:nseRemL1} left plot). A manual reconstruction shows this is a relationship between L2, R1, R2 and m1 (middle plot), but it is effectively the known relationship between L2, R2 and m2 (right plot) -- m2 is latently in the relationship though m1. 

```{r nseRemL1, fig.cap="Removing L1 and optimise again of the remaining parameters, where m2 remains removed from the set. Because of parameter correlations we can recover clear description of L2 as a function of the other parameters, despite m2 missing."}
nsM2 <- nsD %>%
  select(-m2, -L1) %>% # remove m2 to avoid finding well known correlation
  rescale()
if(!file.exists("cache/neutronRemL1.rda")){
  set.seed(2018)
  nseTour2 <- save_history(nsM2, guided_tour(splineIndex()))
  save(nseTour2, file = "cache/neutronRemL1.rda")
} else{
  load("cache/neutronRemL1.rda")
}
tL <- as.list(nseTour2)
iLast <- length(tL)
fProj <- tL[[iLast]]
dProj <- as.tibble(nsM2 %*% fProj)
colnames(fProj) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj <- as_tibble(fProj) %>%
  add_column(label=colnames(nsM2))
p1 <- ggplot(as.tibble(dProj), aes(PP1, PP2))+geom_point(alpha=0.1)+ theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.5,0.6,0.3,0.18), aes(x=x, y=y), color="grey") + 
  xlim(0, 1.5) + ylim(-0.1, 0.8) +
  geom_segment(data=getAxes(fProj, 0.5,0.6,0.3,0.18), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, 0.5,0.6,0.3,0.18), aes(x=x2, y=y2, label=label), size=2.5)
p2 <- ggplot(as.tibble(nsM2), aes(m1+R1+1.5*R2, L2)) + geom_point(alpha=0.1)+ theme(aspect.ratio=1)
p3 <- ggplot(nsD, aes(R2/m2, L2))+ geom_point(alpha=0.1)+ theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3)
```


## Black hole simulation

This data contains posterior samples for simulation from a model describing a binary black hole (BBH) merger event. There are twelve model parameters. Flat priors are used for most model parameters. 

Figure \ref{fig:bbhSimulation} in the Appendix shows a scatterplot matrix, of nine of the twelve parameters. (Parameter m2 is not shown because it is strongly linearly associated with m1, phi$\_$jl and psi are not shown because they uniform, and not associated with other parameters.) Among the nine plotted parameters, strong nonlinear relationships can be seen between the parameters ra, dec and time. The first two describe the position of the event in the sky, and time is the merging time (in GPS units). Because of the elliptical relationship between dec and time, the TIC index is used for PP, even though it is slow to compute. Between the other parameters, the main structure seen is multimodality and some skewness. These patterns are representative of the likelihood function, since most priors are flat, or built to capture growth with volume rather than distance. 


### Data pre-processing

The analysis is conducted on 11 of the twelve parameters. One variable is removed, m2, because it is so strongly associated with m1. All parameters are scaled into the range 0 to 1. 

### Applying PP

#### Exploring 11D with all PP indexes

All seven PP indexes are applied to the data. Figure \ref{fig:bbhGuided} showing the projections that maximize three of the indexes. TIC and splines2D indexes identify very similar projections, that are based on the three parameters, dec, time and ra. This is to be expected based on the pairwise scatterplots. \textcolor{red}{maybe having tour of those three dimensions would be helpful here}. On the other hand, the 1-convex index finds a very different view, but actually the problem is that the optimization doesn't adequately reach a maximum for this index. 


```{r bbhGuided, results="hide", fig.cap="Projections corresponding to the maxima of three indices: TIC, splines2D and 1-convex. Projections a, b found by TIC and splines3D are very similar, and involve the same three parameters, ra, dec and time. The 1-convex index finds a very different view.", dev = "png", dpi=300}
bbhD <- read_csv("data/posterior_samples.csv")
bbhM <- rescale(select(bbhD,-m2, -chi_eff))
if(!file.exists("cache/bbhGuidedTIC.rda")){
  set.seed(2018)
  bbhTour1 <- save_history(bbhM, guided_tour(mineIndex("TIC")))
  bbhTourFull1 <- as.list(interpolate(bbhTour1))
  save(bbhTour1, bbhTourFull1, file = "cache/bbhGuidedTIC.rda")
} else {
  load("cache/bbhGuidedTIC.rda")
}
if(!file.exists("cache/bbhGuidedSpline.rda")){
  set.seed(2018)
  bbhTour2 <- save_history(bbhM, guided_tour(splineIndex()))
  bbhTourFull2 <- as.list(interpolate(bbhTour2))
  save(bbhTour2, bbhTourFull2, file = "cache/bbhGuidedSpline.rda")
} else {
  load("cache/bbhGuidedSpline.rda")
}
if(!file.exists("cache/bbhGuidedConvex.rda")){
  set.seed(2018)
  bbhTour3 <- save_history(bbhM, guided_tour(invConvexIndex()))
  bbhTourFull3 <- as.list(interpolate(bbhTour3))
  save(bbhTour3, bbhTourFull3, file = "cache/bbhGuidedConvex.rda")
} else {
  load("cache/bbhGuidedConvex.rda")
}

iLast <- length(bbhTourFull1)
fProj1 <- bbhTourFull1[[iLast]]
dProj1 <- as.tibble(bbhM %*% fProj1)
colnames(fProj1) <- c("PP1", "PP2")
colnames(dProj1) <- c("PP1", "PP2")
fProj1 <- as_tibble(fProj1) %>%
  add_column(label=colnames(bbhM))
p1 <- ggplot(dProj1, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.4,0.5,0.26,0.24), aes(x=x, y=y), color="grey") + 
  xlim(0.1, 1.4) + ylim(-0.3, 0.9) +
  geom_segment(data=getAxes(fProj1, 0.4,0.5,0.26,0.24), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj1, 0.4,0.5,0.26,0.24), aes(x=x2, y=y2, label=label), size=2.5) + ggtitle("a. TIC: 779.46")
iLast <- length(bbhTourFull2)
fProj2 <- bbhTourFull2[[iLast]]
dProj2 <- as.tibble(bbhM %*% fProj2)
colnames(fProj2) <- c("PP1", "PP2")
colnames(dProj2) <- c("PP1", "PP2")
fProj2 <- as_tibble(fProj2) %>%
  add_column(label=colnames(bbhM))
p2 <- ggplot(dProj2, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.3,-1,0.24,0.22), aes(x=x, y=y), color="grey") + 
  xlim(-0.1, 1.1) + ylim(-1.3, -0.2) +
  geom_segment(data=getAxes(fProj2,0.3,-1,0.24,0.22), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj2, 0.3,-1,0.24,0.22), aes(x=x2, y=y2, label=label), size=2.5) + ggtitle("b. splines2D: 0.97")
iLast <- length(bbhTourFull3)
fProj3 <- bbhTourFull3[[iLast]]
dProj3 <- as.tibble(bbhM %*% fProj3)
colnames(fProj3) <- c("PP1", "PP2")
colnames(dProj3) <- c("PP1", "PP2")
fProj3 <- as_tibble(fProj3) %>%
  add_column(label=colnames(bbhM))
p3 <- ggplot(dProj3, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(0.75, 1, 0.3, 0.28), aes(x=x, y=y), color="grey") + 
  xlim(-0.2, 1.3) + ylim(-0.1, 1.3) +
  geom_segment(data=getAxes(fProj3,0.75, 1, 0.3, 0.28), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj3, 0.75, 1, 0.3, 0.28), aes(x=x2, y=y2, label=label), size=2.5) + ggtitle("c. 1-convex: 0.50")
grid.arrange(p1, p2, p3, ncol=3) 
```


```{r indexTableBBH, eval=FALSE}
getIdxComp <- function(dProj, n){
  ticI <- mine(dProj$PP1, dProj$PP2)$TIC
  splineI <- splines2d(dProj$PP1, dProj$PP2)
  invConvI <- 1-scagnostics(as.matrix(dProj))[,"Convex"]
  return(tibble(TIC=ticI, splines2D=splineI, invConv=invConvI, nView=n))
}
idxT <- tibble(TIC=numeric(),
               splines2D=numeric(),
               invConv=numeric(),
               nView=numeric())
idxT <- rbind(idxT, getIdxComp(dProj1, 1),
      getIdxComp(dProj2, 2),
      getIdxComp(dProj3, 3))
colnames(idxT) <- c("TIC", "splines2D", "1-convex", "Projection")
knitr::kable(idxT,  caption = "Matrix of index values, for all three indices and all three final projections. \\label{tab:indexTableBBH}", digits=2) 
```


#### Exploring reduced space

The variables time, dec and ra are dropped from the data, and PP is applied to the remaining 8D space. Figure \ref{fig:bbhGuided2} shows the projections which maximize the TIC, splines2D and 1-convex indices. The results provide similar information as already learned from the scatterplot matrix (Figure \ref{bbhSimulation}). The parameters chi$\_$tot and chi$\_$p are linearly related (TIC maxima), and theta$\_$jn has a bimodal distribution yielding the figure 8 shape found by the splines2D index. The 1-convex index finds nothing interesting. 


```{r bbhGuided2, results="hide", fig.cap="Projections of the reduced 8D space corresponding to the maxima of three indices: TIC, splines2D and 1-convex.", dev = "png", dpi=300}
bbhM2 <- rescale(select(bbhD,-m2, -chi_eff, -time, -dec, -ra))
if(!file.exists("cache/bbhGuidedTIC2.rda")){
  set.seed(2018)
  bbhTour12 <- save_history(bbhM2, guided_tour(mineIndex("TIC")))
  bbhTourFull12 <- as.list(interpolate(bbhTour12))
  save(bbhTour12, bbhTourFull12, file = "cache/bbhGuidedTIC2.rda")
} else {
  load("cache/bbhGuidedTIC2.rda")
}
if(!file.exists("cache/bbhGuidedSpline2.rda")){
  set.seed(2018)
  bbhTour22 <- save_history(bbhM2, guided_tour(splineIndex()))
  bbhTourFull22 <- as.list(interpolate(bbhTour22))
  save(bbhTour22, bbhTourFull22, file = "cache/bbhGuidedSpline2.rda")
} else {
  load("cache/bbhGuidedSpline2.rda")
}
if(!file.exists("cache/bbhGuidedConvex2.rda")){
  set.seed(2018)
  bbhTour32 <- save_history(bbhM2, guided_tour(invConvexIndex()))
  bbhTourFull32 <- as.list(interpolate(bbhTour32))
  save(bbhTour32, bbhTourFull32, file = "cache/bbhGuidedConvex2.rda")
} else {
  load("cache/bbhGuidedConvex2.rda")
}

iLast <- length(bbhTourFull12)
fProj1 <- bbhTourFull12[[iLast]]
dProj <- as.tibble(bbhM2 %*% fProj1)
colnames(fProj1) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj1 <- as_tibble(fProj1) %>%
  add_column(label=colnames(bbhM2))
iv <- mine(dProj$PP1,dProj$PP2)["TIC"]
p1 <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1) + 
  geom_path(data=getCircle(1, 0, 0.34, 0.25), aes(x=x, y=y), color="grey") + 
  xlim(-0.2, 1.5) + ylim(-0.25, 1) +
  geom_segment(data=getAxes(fProj1,1, 0, 0.34, 0.25), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj1, 1, 0, 0.34, 0.25), aes(x=x2, y=y2, label=label), size=2.5) +
  ggtitle(paste0("a, TIC: ", toString(format(iv, digits=2))))
iLast <- length(bbhTourFull22)
fProj2 <- bbhTourFull22[[iLast]]
dProj <- as.tibble(bbhM2 %*% fProj2)
colnames(fProj2) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj2 <- as_tibble(fProj2) %>%
  add_column(label=colnames(bbhM2))
iv <- splines2d(dProj$PP1,dProj$PP2)
p2 <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(0.75, 0, 0.25, 0.24), aes(x=x, y=y), color="grey") + 
  xlim(-0.25, 1) + ylim(-0.9, 0.3) +
  geom_segment(data=getAxes(fProj2, 0.75, 0, 0.25, 0.24), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj2, 0.75, 0, 0.25, 0.24), aes(x=x2, y=y2, label=label), size=2.5) +
  ggtitle(paste0("b, splines2D: ", toString(format(iv, digits=2))))
iLast <- length(bbhTourFull32)
fProj3 <- bbhTourFull32[[iLast]]
dProj <- as.tibble(bbhM2 %*% fProj3)
colnames(fProj3) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj3 <- as_tibble(fProj3) %>%
  add_column(label=colnames(bbhM2))
iv <- 1-scagnostics(as.matrix(dProj))[,"Convex"]
p3 <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(0, 1.25, 0.32, 0.32), aes(x=x, y=y), color="grey") + 
  xlim(-0.4, 1.2) + ylim(0, 1.6) +
  geom_segment(data=getAxes(fProj3, 0, 1.25, 0.32, 0.32), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj3, 0, 1.25, 0.32, 0.32), aes(x=x2, y=y2, label=label), size=2.5) +
  ggtitle(paste0("c, 1-convex: ", toString(format(iv, digits=2))))
grid.arrange(p1, p2, p3, ncol=3) 
```

#### Effect of random starts, and subsets used

The initial conditions for the optimization, and the subset of variables used, can have a large effect on the projections returned. We illustrate this using only the splines2D index, and find that there is one more association that can be learned that was masked earlier. 

Figure \ref{fig:bbhGuided3} shows six maxima obtained by different starts, for two types of parameters: first, spin related parameters (i.e. alpha, theta\_jn, chi\_tot and chi\_p), and second position related parameters (i.e. ra, dec and distance). Four of the six (a-d) are almost identical, but not interesting projections. Projection f has the highest PP index value but it is primarily the view seen in the bivariate plot of dec and ra.  

Choosing a different subset of variables reveals something new. The subspace of m1, ra, chi\_tot, alpha, distance, dec produces a more refined view of Figure \ref{fig:bbhGuided3} projection f. When alpha contributes in contrast to dec, the relationship between the points is almost perfectly on a curve. This is shown in Figure \ref{constructedExample}. Manually reconstructing this can be done by differencing the two parameters, in their original units. \textcolor{red}{This result is unexpected, it can be understood from ????? (Does it mean that knowing the precision angle gives more precise information about the location? Knowing ra uniquely determines dec-alpha rather than dec it seems.)}

```{r bbhGuided3, results="hide", fig.cap=paste("Final views identified in the dataset considering the seven dimensional parameter space (alpha, theta\\_jn, chi\\_tot, chi\\_p, ra, dec, distance), differing only by randomly selected starting plane."), dev = "png", dpi=300}
bbhM3 <- rescale(select(bbhD, alpha, theta_jn, chi_tot, chi_p, ra, dec, distance))
getResWithSeed <- function(s){
  set.seed(s)
  startM <- orthonormalise(matrix(runif(14),ncol = 2))
  tourRes <- save_history(bbhM3, guided_tour(splineIndex()))
}

if(!file.exists("cache/bbhGuidedSpline3.rda")){
  set.seed(2018)
  seedVals <- sample(1:10000, 6)
  bbhRes1 <- getResWithSeed(seedVals[1])
  bbhRes2 <- getResWithSeed(seedVals[2])
  bbhRes3 <- getResWithSeed(seedVals[3])
  bbhRes4 <- getResWithSeed(seedVals[4])
  bbhRes5 <- getResWithSeed(seedVals[5])
  bbhRes6 <- getResWithSeed(seedVals[6])
  save(bbhRes1, bbhRes2, bbhRes3, bbhRes4, bbhRes5, bbhRes6, file = "cache/bbhGuidedSpline3.rda")
} else {
  load("cache/bbhGuidedSpline3.rda")
}
resList <- list(as.list(bbhRes1), as.list(bbhRes2), as.list(bbhRes3),
                as.list(bbhRes4), as.list(bbhRes5), as.list(bbhRes6))
pList <- list()
idxV <- NULL
i <- 1
xmins <- c(-0.5,-0.5,-0.4,-0.9,-1,0.25)
xmaxs <- c(0.75,0.75,0.8,0.4,0.25,1.5)
ymins <- c(-0.8,-0.8,-0.7,-0.8,-1.2,-0.6)
ymaxs <- c(0.7,0.6,0.7,0.7,0.,0.9)
xscales <- c(0.25,0.25,0.24,0.26,0.25,0.25)
yscales <- c(0.3,0.28,0.28,0.3,0.24,0.3)
xcents <- c(0.25,-0.25,-0.1,0.1,-0.75,1.25)
ycents <- c(0.4,-0.4,-0.4,-0.4,-0.75,0.5)
titleL <- c("a: ","b: ","c: ","d: ","f: ","e: ")
for (r in resList){
  iLast <- length(r)
  fProj <- r[[iLast]]
  dProj <- as.tibble(bbhM3 %*% fProj)
  colnames(fProj) <- c("PP1", "PP2")
  colnames(dProj) <- c("PP1", "PP2")
  fProj <- as_tibble(fProj) %>%
    add_column(label=colnames(bbhM3))
  idxV <- splines2d(dProj$PP1, dProj$PP2)
  pList[[i]] <- ggplot(dProj, aes(PP1,PP2)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(xcents[i],ycents[i],xscales[i],yscales[i]), aes(x=x, y=y), color="grey") + 
  xlim(xmins[i], xmaxs[i]) + ylim(ymins[i], ymaxs[i]) +
  geom_segment(data=getAxes(fProj, xcents[i],ycents[i],xscales[i],yscales[i]), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, xcents[i],ycents[i],xscales[i],yscales[i]), aes(x=x2, y=y2, label=label), size=2.5) +
    ggtitle(paste0(titleL[i], toString(format(idxV, digits=2))))
  i <- i+1
}
grid.arrange(pList[[1]], pList[[2]], pList[[3]],pList[[4]], pList[[6]], pList[[5]], ncol=3)
```

```{r constructedExample, results="hide", fig.cap="Selecting subset of 6 parameters, for which spline index can be used to improve constraint on parameter combination wrt ra vs dec.", dev = "png", dpi=300}
bbhC <- rescale(select(bbhD, m1, ra, chi_tot, alpha, distance, dec))
if(!file.exists("cache/constructedExample.rda")){
  set.seed(1999)
  tC <- save_history(bbhC, guided_tour(splineIndex()))
  save(tC, file = "cache/constructedExample.rda")
} else {
  load("cache/constructedExample.rda")
}
iL <- length(as.list(tC))
dProj <- bbhC %*% as.list(tC)[[iL]]
fProj <- as.list(tC)[[iL]]
colnames(fProj) <- c("PP1", "PP2")
colnames(dProj) <- c("PP1", "PP2")
fProj <- as_tibble(fProj) %>%
  add_column(label=colnames(bbhC))
p1 <- ggplot(as.tibble(dProj), aes(PP1, PP2))+geom_point(alpha=0.02) + theme(aspect.ratio=1)+ 
  geom_path(data=getCircle(-0.7, 0.6, 0.2, 0.24), aes(x=x, y=y), color="grey") + 
  xlim(-0.9, 0.1) + ylim(-0.1, 1.1) +
  geom_segment(data=getAxes(fProj, -0.7, 0.6, 0.2, 0.24), aes(x=x1, xend=x2, y=y1, yend=y2)) +
  geom_text(data=getAxes(fProj, -0.7, 0.6, 0.2, 0.24), aes(x=x2, y=y2, label=label), size=2.5)
p2 <- ggplot(as.tibble(bbhD),aes(dec-alpha, ra)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)
p3 <- ggplot(as.tibble(bbhD),aes(dec, ra)) + geom_point(alpha = 0.02) + theme(aspect.ratio=1)
grid.arrange(p1, p2, p3, ncol=3)
```

<!--

# Applications to collections of time series 

This could be another example.

```{r eval=FALSE}
music <- read_csv("data/tigs_music.csv")
music <- apply(music, 2, function(x) (x-mean(x))/sd(x))
musicTour <- save_history(music,
                         guided_tour(mineIndex("TIC"), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTour <- save_history(music,
                         guided_tour(dcorIndex(), 
                            search_f = tourr:::search_better,
                            cooling=1, alpha = 0.5, max.tries = 5000))
musicTourFull <- as.list(interpolate(musicTour))
iLast <- length(musicTourFull)
fProj <- musicTourFull[[iLast]]
dProj <- as.tibble(as.matrix(music) %*% fProj)
ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1)
fullRes <- getProj(as.matrix(music), musicTourFull, "TIC", 100)
fullResMelt <- melt(fullRes, id=c("t", "name", "size"))
ggplot(fullResMelt, aes(x=t, y=value)) +
  geom_line(aes(color=factor(size))) + 
  facet_wrap(variable~name, ncol=3, scales = "free_y", labeller = label_wrap_gen(multi_line=FALSE)) +
  guides(color=FALSE)
```

-->

# Discussion

# Appendix {-} 

## Re-scaling of holes index

Holes and cmass indexes are derived from $I_0^N$ of \cite{CBC93}. As noted in proposition 1 of that paper the index takes local maxima for the minimum and maximum of $a_0$ which are achieved by central hole and central mass distributions respectively. \cite{Cook:2007:IDG:1557227} then gives explicit index functions defined for sphered data (zero mean, identity variance-covariance matrix). They are defined such that each one is maximized for central holes or central mass type distributions, with maximum=1, and cmass=1-holes. It follows that for either index both large and small values signal deviation from the normal distribution, and given a normal distribution we expect to find "average" index values rather than values close to zero.

We can estimate the values found for normal distribution by comparing values of $a_{00}$ of Sec 7.1 in \cite{CBC93}. The maximum value is $1/(2\pi)$ found for cmass type distributions, the minimum is $1/(2\pi e)$ found for hole type distributions. Evaluating for normal distributions gives $1/(4\pi)$, rescaling such that the index values range from 0 to 1 then puts the value for normal distributions at approximately $0.2$. This appears consistent with the results we have, i.e. for normal distributions the cmass index is about $0.2$, and the holes index (=1-cmass) is about $0.8$.

Rescaling: first have cutoff at the respective value for the normal distribution, i.e. any value below 0.2 (0.8) is set to zero for cmass (holes) index, rescale remaining range to be between zero and one.

## Binning sensitivity of MIC index
To examine the sensitivity of binning in the MIC PPI, the classic RANDU data [@Marsaglia68], available in R, is used. Binning is defined by $\delta$, where $B(n) = n^{\delta}$. Figure \ref{fig:randuEx} shows the best projection, index value and computing time obtained when optimizing the MIC index with values $\delta = 0.6, 0.7, 0.8$. With small $\delta$, less bins, the structure isn't visible, and with larger $\delta$ the structure is confounded with noise. It does appear that this parameter affects the performace of the MIC index.


```{r randuEx, results="hide", fig.cap="Best projection obtained by optimising the MIC index on the RANDU data, using different number of bins, defined by $\\delta$. The smaller the value the fewer bins.  Above each plot is written the value of $\\delta$, time required to optimize (seconds) and the MIC index value. The best $\\delta = 0.7$, and the result indicates that this parameter does affect MIC performance."}
if(!file.exists("cache/randuEx.rda")){
  pL <- list()
  i <- 1
  for (alpha in c( 0.6, 0.7, 0.8)){
    set.seed(556677)
    tic.clearlog()
    tic() #start timer
    tP <- save_history(randu, guided_tour(mineIndexAlpha("MIC", alpha)),max_bases = 100000)
    toc(log=TRUE,quiet=TRUE)
    optT <- unlist(tic.log(format=FALSE))["toc.elapsed"]-unlist(tic.log(format=FALSE))["tic.elapsed"]
    imax <- length(as.list(tP))
    fV <- as.tibble(as.matrix(randu) %*% as.list(tP)[[imax]])
    colnames(fV) <- c("PP1", "PP2")
    idxV <- mine(fV$PP1, fV$PP2, alpha = alpha)[["MIC"]]
    pL[[i]] <- ggplot(fV, aes(PP1,PP2)) + geom_point() + theme(aspect.ratio=1) + 
      ggtitle(paste(alpha, toString(format(optT, digits=2)), toString(format(idxV, digits=2)), sep = ", "))
    i <- i+1
  }
  save(pL, file = "cache/randuEx.rda")
} else {
  load("cache/randuEx.rda")
}
grid.arrange(pL[[1]], pL[[2]], pL[[3]], ncol=3)
```


## Additional Figures

### Final projection of pipe guided tour
Figure \ref{fig:testpipe} shows the projection returned during the scouting phase (left) and the refinement phase (right). It was important to start the method 3 optimizer at the best projection returned during the scouting phase, to smoothly converge more closely to the ideal projection.

```{r testpipe,  fig.height=4, fig.width=8, out.width = "0.8\\textwidth",  fig.cap="Projections returned by TIC optimisation: by the scouting phase (left) and refined by  optimisation method 3 (right), starting from the scouting phase projection."}
iLast <- length(pipeTourFull)
fProj <- pipeTourFull[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
p1 <- ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1) +
  xlab("PP1") + ylab("PP2")
iLast <- length(pipeTourFull2)
fProj <- pipeTourFull2[[iLast]]
dProj <- as.tibble(pipeResc %*% fProj)
p2 <- ggplot(dProj, aes(V1,V2)) + geom_point() + theme(aspect.ratio=1) +
  xlab("PP1") + ylab("PP2")
grid.arrange(p1, p2, ncol=2)
```

### Dataset overview scatter plot matrices

```{r neutronStarSPLOM, fig.height=8, fig.width=8, fig.cap="Scatter plot matrix of the neutron star dataset, darker regions represent higher marginalised posterior densities."}
ggpairs(nsD, lower=list(continuous = wrap("points", alpha = 0.05)))
```

```{r bbhSimulation, fig.height=8, fig.width=8, dev = "png", dpi=300, fig.cap="Scatter plot matrix showing most of the variables included in the BBH dataset. Strong correlation between the parameters time, dec and ra can be observed."}
bbhDsmall <- select(bbhD, -phi_jl, -m2, -psi, -chi_eff)
ggpairs(bbhDsmall, lower=list(continuous = wrap("points", alpha = 0.02)))
```